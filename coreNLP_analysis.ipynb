{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Data Analysis Project\n",
    "**Team**: ToeStewBrr - Alexander Sternfeld, Marguerite Thery, Antoine Bonnet, Hugo Bordereaux\n",
    "\n",
    "**Dataset**: CMU Movie Summary Corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CoreNLP Analysis\n",
    "\n",
    "We first load data files and download the pre-processed dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import *\n",
    "from coreNLP_analysis import *\n",
    "\n",
    "download_data()\n",
    "plot_df = load_plot_df()\n",
    "movie_df = load_movie_df()\n",
    "char_df = load_char_df()\n",
    "names_df = load_names_df()\n",
    "cluster_df = load_cluster_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Extracting characters\n",
    "\n",
    "For any character, we want to extract related information (from name clusters, character metadata) as well as actions, characteristics and relations (from CoreNLP). We first extract information from the pre-processed dataframes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Harry Potter's character as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_name = 'Harry Potter'\n",
    "movie_ids = list(char_df[char_df['Character name'] == 'Harry Potter']['Wikipedia ID'])\n",
    "char_ids = names_df.loc[char_name].values[0]\n",
    "trope = cluster_df.loc[cluster_df['Character name'] == char_name]\n",
    "# if trop is empty, set trope to None\n",
    "if trope.empty:\n",
    "    trope = None\n",
    "\n",
    "print('Movies with character', char_name, ':')\n",
    "print('\\tMovie IDs:', movie_ids)\n",
    "print('\\tCharacter IDs:', char_ids)\n",
    "print('\\tTrope:', trope)\n",
    "\n",
    "movie_id = movie_ids[3] \n",
    "print('Selecting movie ID as example:', movie_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now extract information from the CoreNLP plot summary analysis. Each xml file has a tree structure, which we can parse as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the tree from xml CoreNLP output\n",
    "def get_tree(movie_id):\n",
    "    xml_filename = os.path.join(XML_DIR, '{}.xml'.format(movie_id))\n",
    "    tree = ET.parse(xml_filename)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract all parsed sentences from the xml file, each of which we can view as a tree structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given an xml file, we return all of its CoreNLP parsed sentences \n",
    "def get_parsed_sentences(tree):\n",
    "    sentences = []\n",
    "    root = tree.getroot()\n",
    "    for child in tree.iter():\n",
    "        if child.tag == \"parse\":\n",
    "            sentences.append(child.text)\n",
    "    return sentences\n",
    "\n",
    "# To print parsed sentences as a pretty tree. \n",
    "def print_tree(parsed_string):\n",
    "    tree = Tree.fromstring(parsed_string)\n",
    "    tree.pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = get_tree(movie_id)\n",
    "parsed_str = get_parsed_sentences(tree)[5]\n",
    "print_tree(parsed_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to extract all character names from the xml file. Note that we aggregate consecutive words tagged as NNP (noun, proper, singular) as the same character name (this assumes that plot summaries never contain two distinct names side by side without delimiting punctuation). This is a reasonable assumption since list of names are almost always separated by commas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_characters(tree):\n",
    "    characters = []\n",
    "    current_word = None\n",
    "    was_person = False\n",
    "    character = ''\n",
    "    for child in tree.iter():\n",
    "        if child.tag == 'word':\n",
    "            current_word = child.text\n",
    "        if child.tag == 'NER' and child.text == 'PERSON':\n",
    "            if was_person:# Continue the character\n",
    "                character += ' ' + current_word\n",
    "            else: # Start the character\n",
    "                character = current_word\n",
    "                was_person = True\n",
    "        if was_person and child.tag == 'NER' and child.text != 'PERSON': # End the character\n",
    "            characters.append(character)\n",
    "            character = ''\n",
    "            was_person = False\n",
    "    return characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = get_characters(tree)\n",
    "characters[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISCARD THIS METHOD?\n",
    "\n",
    "# Given a character in a movie, find all sentences mentioning the character\n",
    "def sentences_with_character(xml_filename, char_name):\n",
    "    char_sentences = []\n",
    "    if os.path.isfile(xml_filename):\n",
    "        sentences = get_parsed_sentences(xml_filename)\n",
    "        char_sentences = [sentence for sentence in sentences if char_name in sentence]\n",
    "    return char_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that some characters are sometimes mentioned by their full name, and sometimes by a partial name (e.g. Harry Potter is most often mentioned as simply Harry). To get a more precise idea of how many times each character is mentioned, we wish to denote each character by their full name, i.e. the longest version of their name that appears in the plot summary. \n",
    "\n",
    "To optimize full name lookup, for each plot summary we construct a dictionary which stores as key every partial name mentioned, and as corresponding values the full name of each character.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_name(string, characters):\n",
    "    ''' \n",
    "    Find the longest name of a given character in a list of character names. \n",
    "    Input: \n",
    "        string: character name (partial or full)\n",
    "        characters: list of character names\n",
    "    Output: \n",
    "        full_name: longest name of character found in characters\n",
    "    '''\n",
    "    names = string.split(' ')\n",
    "    max_length = 0\n",
    "    for character in characters:\n",
    "        char_names = character.split(' ')\n",
    "        if set(names) <= set(char_names): \n",
    "            num_names = len(char_names)\n",
    "            if num_names > max_length:\n",
    "                max_length = num_names\n",
    "                full_name = character\n",
    "    return full_name\n",
    "\n",
    "# Helper function: given a list of characters, make a dictionary with all maps (short name : full name)\n",
    "def full_name_dict(characters): \n",
    "    full_names = {}\n",
    "    for character in characters:\n",
    "        full_names[character] = get_full_name(character, characters)\n",
    "    return full_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy example\n",
    "characters_example = ['Harry Potter', 'Harry', 'Lord Voldemort', 'Harry James Potter', 'Dumbledore', 'Albus Dumbledore']\n",
    "print('Full name:', get_full_name('Harry Potter', characters_example))\n",
    "print('All full name pairs:', full_name_dict(characters_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can now construct a dictionary with keys being the characters' full name and values being the number of times any version of their name is mentioned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of characters names, create a dictionary of characters with \n",
    "# keys being their full name and values being the number of times they appear in the list\n",
    "\n",
    "def aggregate_characters(characters):\n",
    "    ''' \n",
    "    Input: list of characters\n",
    "    Output: dictionary of (full name : number of times name is mentioned in list)\n",
    "    Example: ['Harry Potter', 'Voldemort', 'Harry'] -> {'Harry Potter': 2, 'Voldemort': 1}\n",
    "    '''\n",
    "    character_dict = dict()\n",
    "    for character in characters:\n",
    "        full_character = get_full_name(character, characters)\n",
    "        if full_character in character_dict:\n",
    "            character_dict[full_character] += 1\n",
    "        else:\n",
    "            character_dict[full_character] = 1\n",
    "    return character_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_characters(characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now extract the most mentioned characters in any plot summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a parse tree, extract the most mentioned characters in decreasing order\n",
    "def most_mentioned(tree):\n",
    "    '''\n",
    "    Input: \n",
    "        tree: parse tree of the xml file\n",
    "        N: the number of characters to return\n",
    "    Output:\n",
    "        A dictionary of the N characters most mentioned in the movie\n",
    "    '''\n",
    "    characters = get_characters(tree)\n",
    "    character_dict = aggregate_characters(characters)\n",
    "    sorted_characters = sorted(character_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_mentioned(tree)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.2. Extracting relationships\n",
    "\n",
    " We cannot extract character interactions directly from the CoreNLP output (or can we?). Instead, we use the number of common mentions of two characters in the same sentence as a proxy for the number of interactions. We first define a method that gets the full name of all characters mentioned in each sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of characters and a string sentence, find the sequence of characters appearing in the sentence\n",
    "def characters_in_sentence(characters, sentence):\n",
    "    '''\n",
    "    Input: \n",
    "        characters: list of characters\n",
    "        sentence: string\n",
    "    Output: \n",
    "        characters_mentioned: list of characters that appear in the sentence\n",
    "        characters: list of remaining characters\n",
    "    Example: characters_in_sentence(['Harry Potter', 'Voldemort', 'Dumbledore'], 'Harry Potter fights Voldemort bravely.')\n",
    "    Output: ['Harry Potter', 'Voldemort'], ['Dumbledore']\n",
    "    '''\n",
    "    characters_mentioned = []\n",
    "    if (len(characters) == 0):\n",
    "            return characters_mentioned, characters\n",
    "    character = characters.pop(0)\n",
    "    while character in sentence:\n",
    "        sentence = sentence.split(character, 1)[1]\n",
    "        characters_mentioned.append(character)\n",
    "        if (len(characters) == 0):\n",
    "            break\n",
    "        character = characters.pop(0)\n",
    "    return characters_mentioned, characters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_summary = plot_df.loc[plot_df['Wikipedia ID'] == movie_id]['Summary'].values[0]\n",
    "sentences = re.split(r'(?<=[.!?])\\s+', plot_summary)\n",
    "print('First sentence:', sentences[0])\n",
    "print('Characters:', characters[:10])\n",
    "characters_mentioned, characters_rem = characters_in_sentence(characters[:10], sentences[0])\n",
    "print('Characters mentioned:', characters_mentioned)\n",
    "print('Remaining characters:', characters_rem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a method that takes in a movie ID, and outputs the number of common mentions (i.e. interactions) for each pair of characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def character_pairs(movie_id):\n",
    "    ''' \n",
    "    Find all pairs of characters that appear in the same sentence in a movie plot summary. \n",
    "    Input: \n",
    "        movie_id: integer Movie ID\n",
    "    Output:\n",
    "        A list of all character pairs in the movie in decreasing order of frequency\n",
    "    '''\n",
    "    char_pairs = dict()\n",
    "\n",
    "    # Parse xml file and get all characters from plot summary\n",
    "    xml_filename = os.path.join(XML_DIR, '{}.xml'.format(movie_id))\n",
    "    tree = ET.parse(xml_filename)\n",
    "    characters = get_characters(tree)\n",
    "    full_name_map = full_name_dict(characters) # all maps (partial name : full name)\n",
    "\n",
    "    # Split plot summary into sentences\n",
    "    plot_summary = plot_df.loc[plot_df['Wikipedia ID'] == movie_id]['Summary'].values[0]\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', plot_summary)\n",
    "\n",
    "    # For each sentence, find the characters mentioned and their full names, then add count for each pair between them\n",
    "    for sentence in sentences:\n",
    "        # Only consider sentences with at least 2 mentioned characters\n",
    "        characters_mentioned, characters = characters_in_sentence(characters, sentence)\n",
    "        if len(characters_mentioned) >= 2: \n",
    "            \n",
    "            # Get full names of each character mentioned, remove doubles\n",
    "            full_mentioned = set([full_name_map[c] for c in characters_mentioned])\n",
    "\n",
    "            # Add count for each pair of characters\n",
    "            pairs = list(itertools.combinations(sorted(full_mentioned), 2))\n",
    "            for pair in pairs:\n",
    "                if pair in char_pairs:\n",
    "                    char_pairs[pair] += 1\n",
    "                else:\n",
    "                    char_pairs[pair] = 1\n",
    "    \n",
    "    # Sort character pairs by number of times they appear together\n",
    "    sorted_pairs = sorted(char_pairs.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_pairs(movie_id)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a movie ID, find the character pairs with the highest number of interactions (both mentioned in a single sentence).\n",
    "def most_interactions(movie_id):\n",
    "    # Get all characters in the movie\n",
    "    xml_filename = os.path.join(XML_DIR, '{}.xml'.format(movie_id))\n",
    "    tree = ET.parse(xml_filename)\n",
    "    characters = get_characters(tree)\n",
    "\n",
    "    # Find the plot summary sentences with at least two characters \n",
    "    plot_summary = plot_df.loc[plot_df['Wikipedia ID'] == movie_id]['Summary'].values[0]\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', plot_summary)\n",
    "    \n",
    "    # Find the character pairs with the most interactions\n",
    "    character_pairs = dict()\n",
    "    for sentence in sentences:\n",
    "        characters = get_characters(sentence)\n",
    "        character_pairs = aggregate_characters(characters)\n",
    "    sorted_pairs = sorted(character_pairs.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_filename = os.path.join(XML_DIR, '{}.xml'.format(movie_id))\n",
    "tree = ET.parse(xml_filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_summary = plot_df.loc[plot_df['Wikipedia ID'] == movie_id]['Summary'].values[0]\n",
    "sentences = re.split(r'(?<=[.!?])\\s+', plot_summary)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_interactions(movie_id)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to identify the two main characters in a plot summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: only consider romantic movies\n",
    "#TODO: Aggregate consecutive entity names inton one\n",
    "extracted_dir = 'Data/CoreNLP/corenlp_plot_summaries_xml'\n",
    "\n",
    "def make_pairs (extracted_dir): \n",
    "  pairs = []\n",
    "  for filename in os.listdir(extracted_dir):\n",
    "      f = os.path.join(extracted_dir, filename) \n",
    "      if os.path.isfile(f):\n",
    "          # Create characters list for each file \n",
    "          characters = []\n",
    "          tree = ET.parse(f)\n",
    "          root = tree.getroot()\n",
    "          for child in tree.iter():\n",
    "              if child.tag == \"word\":\n",
    "                current_word = child.text\n",
    "              if child.tag == \"NER\": \n",
    "                if child.text == \"PERSON\":\n",
    "                  characters.append(current_word)\n",
    "          # Select the two characters which appear most often in the file\n",
    "          values, counts = np.unique(characters, return_counts=True)\n",
    "          two_most_frequent_characters = values[counts.argsort()[-2:][::-1]]\n",
    "          # If there exists two characters, create a pair \n",
    "          if len(two_most_frequent_characters) > 1:\n",
    "            pairs.append([f.replace('Data/CoreNLP/corenlp_plot_summaries_xml/', '').replace('.xml', ''), two_most_frequent_characters[0], two_most_frequent_characters[1]])\n",
    "  return pairs\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into an array and as a dataframe\n",
    "pairs = make_pairs(XML_DIR)\n",
    "pairs = np.asarray(pairs).reshape(-1, 3)  \n",
    "pairs_df = pd.DataFrame(pairs, columns=['Wikipedia ID', 'char1', 'char2']) \n",
    "pairs_df        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge pairs dataset with characters \n",
    "char_df['Wikipedia ID'] = char_df['Wikipedia ID'].astype(str)\n",
    "pairs_df['Wikipedia ID'] = pairs_df['Wikipedia ID'].astype(str)\n",
    "pairs_char = pairs_df.merge(char_df, on=\"Wikipedia ID\")\n",
    "\n",
    "# Filter out the nan values\n",
    "pairs_char = pairs_char[~pairs_char['Character name'].isna()]\n",
    "\n",
    "# Create columns which indicates if char1 and char2 are in character name \n",
    "pairs_char['char1_is_in_name'] = pairs_char.apply(lambda x: 1 if x['char1'] in x['Character name'] else 0, axis=1)\n",
    "pairs_char['char2_is_in_name'] = pairs_char.apply(lambda x: 1 if x['char2'] in x['Character name'] else 0, axis=1)\n",
    "pairs_char[pairs_char['char1_is_in_name'] == 1 & pairs_char['char2_is_in_name'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. CoreNLP Analysis\n",
    "\n",
    "To use the powerful CoreNLP model, first [download it](https://stanfordnlp.github.io/CoreNLP/download.html), then cd into the downloaded `stanford-corenlp` directory. If you have Java, you can run the following command to open a CoreNLP shell: \n",
    "\n",
    "\n",
    "`java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer` \n",
    " \n",
    "Now that the shell is running, we can use their models to annotate some sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "nlp.annotate('Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008.')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
