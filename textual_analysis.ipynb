{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Applied Data Analysis Project\n",
    "**Team**: ToeStewBrr - Alexander Sternfeld, Marguerite Thery, Antoine Bonnet, Hugo Bordereaux\n",
    "\n",
    "**Dataset**: CMU Movie Summary Corpus\n",
    "\n",
    "# Part 3: Textual Analysis\n",
    "\n",
    "In this notebook, we analyze the pre-processed output of our custom CoreNLP pipeline. \n",
    "\n",
    "### Table of contents\n",
    "1. [Loading pre-processed coreNLP data](#section1)\n",
    "2. [Persona clusters](#section2)\n",
    "    - 2.1. [Embedding descriptions](#section2-1)\n",
    "    - 2.2. [Weighted average of word vectors](#section2-2)\n",
    "    - 2.3. [Dimensionality reduction](#section2-3)\n",
    "        - 2.3.1. [Principal Component Analysis (PCA)](#section2-3-1)\n",
    "        - 2.3.2. [*t*-distributed Stochastic Neighbor Embedding (t-SNE)](#section2-3-2)\n",
    "    - 2.4. [Clustering personas](#section2-4)\n",
    "    - 2.5. [Visualizing persona clusters](#section2-5)\n",
    "    - 2.6. [Preparing data for website use](#section2-6)\n",
    "\n",
    "### 2.3. Dimensionality reduction <a class=\"anchor\" id=\"section2-3\"></a>\n",
    "\n",
    "#### 2.3.1. Principal Component Analysis (PCA) <a class=\"anchor\" id=\"section2-3-1\"></a>\n",
    "\n",
    "**Prerequisite**: \n",
    "\n",
    "Install [spaCy](https://spacy.io) using the following commands: \n",
    "\n",
    "        pip install spacy\n",
    "        \n",
    "        python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "from extraction import *\n",
    "from coreNLP_analysis import *\n",
    "from load_data import *\n",
    "from textual_analysis import *\n",
    "\n",
    "\n",
    "# NOTE: If you haven't loaded NLTK before, set this to True\n",
    "load_nltk = False\n",
    "\n",
    "if load_nltk: #Load the spaCy model for the semantic analysis\n",
    "    nlp_spacy = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load pre-processed coreNLP data <a class=\"anchor\" id=\"section1\"></a>\n",
    "\n",
    "We first load the pre-processed output from our custom CoreNLP pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_description_path = 'Data/CoreNLP/char_descriptions.csv'\n",
    "full_description_path = 'Data/CoreNLP/full_descriptions.csv'\n",
    "\n",
    "# Load character descriptions\n",
    "char_description_df = pd.read_csv(char_description_path, sep='\\t', index_col=None, low_memory=False)\n",
    "\n",
    "# Convert to lists\n",
    "char_description_df['agent_verbs'] = char_description_df.agent_verbs.apply(\n",
    "    lambda x: literal_eval(x) if type(x) == str else x)\n",
    "char_description_df['patient_verbs'] = char_description_df.patient_verbs.apply(\n",
    "    lambda x: literal_eval(x) if type(x) == str else x)\n",
    "char_description_df['attributes'] = char_description_df.attributes.apply(\n",
    "    lambda x: literal_eval(x) if type(x) == str else x)\n",
    "char_description_df['descriptions'] = char_description_df.descriptions.apply(\n",
    "    lambda x: literal_eval(x) if type(x) == str else x)\n",
    "char_description_df['title'] = char_description_df.title.apply(\n",
    "    lambda x: literal_eval(x) if type(x) == str else x)\n",
    "\n",
    "full_description_df = pd.read_csv(full_description_path, sep='\\t', index_col=None, low_memory=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Persona clusters <a class=\"anchor\" id=\"section2\"></a>\n",
    "\n",
    "### 2.1. Embedding descriptions <a class=\"anchor\" id=\"section2-1\"></a>\n",
    "\n",
    "We embed all descriptive words (actions, attributes, titles) of all characters into a high-dimensional vector space using spaCy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = 'Data/CoreNLP/char_description_embeddings.pickle'\n",
    "\n",
    "# If we have already embedded the descriptions, load them from the pickle file\n",
    "if os.path.exists(embedding_file):\n",
    "    char_description_df = pd.read_pickle(embedding_file)\n",
    "\n",
    "else:\n",
    "    # Embed descriptions (Get a comfy chair, this takes a while) \n",
    "    char_description_df = construct_descriptions_embeddings(char_description_df, nlp_spacy)\n",
    "\n",
    "    # Split embeddings by category\n",
    "    char_description_df = embeddings_categorical(char_description_df)\n",
    "\n",
    "    # Save the embeddings to a pickle file\n",
    "    with open(embedding_file, 'wb') as f:\n",
    "        pickle.dump(char_description_df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Weighted average of word vectors <a class=\"anchor\" id=\"section2-2\"></a>\n",
    "\n",
    "We then weigh the word embedding of each word for each character by their cosine distance to the average semantic vector of words with the sam type used for all characters in the dataset. The *cosine distance* is defined as:\n",
    "\n",
    "$$\\text{cosine distance}(x_1, x_2) = 1-\\frac{x_1 \\cdot x_2}{||x_1||\\cdot||x_2||}$$\n",
    "\n",
    "where $x_1$ and $x_2$ are the vector representations of two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_df = weight_embeddings(char_description_df, column='title', percentile=0)\n",
    "\n",
    "weight_df = weight_embeddings(weight_df, column='attributes', percentile=60)\n",
    "\n",
    "weight_df = weight_embeddings(weight_df, column='agent_verbs', percentile=75)\n",
    "\n",
    "weight_df = weight_embeddings(weight_df, column='patient_verbs', percentile=85)\n",
    "\n",
    "weight_df = weight_embeddings(weight_df, column='descriptions', title_weight=0.35)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Dimensionality reduction <a class=\"anchor\" id=\"section2-3\"></a>\n",
    "\n",
    "#### 2.3.1. Principal Component Analysis (PCA) <a class=\"anchor\" id=\"section2-3-1\"></a>\n",
    "\n",
    "To visualize our clusters, we then map these high-dimensional descriptive vectors to 50-dimensional space using PCA to prepare the ground for a second dimensionality reduction technique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows in char_description_df that have less than X descriptions\n",
    "min_words = 5\n",
    "df = weight_df.copy(deep=True)\n",
    "df = df[df.descriptions.apply(lambda x: type(x) != float)]\n",
    "df = df[df.descriptions.apply(lambda x: len(x) >= min_words)]\n",
    "print('Percentage of characters with at least {} descriptions: {:.2f}%'.format(min_words, 100*len(df)/len(weight_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction: PCA to 50 dimensions -> t-SNE to 3 dimensions\n",
    "column = 'weighted_descriptions_embeddings'\n",
    "n_total = df[column].apply(lambda x: 1 if type(x) == np.ndarray else 0).sum()\n",
    "pca_df = descriptions_PCA(df, column=column, n_components=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2. *t*-distributed Stochastic Neighbor Embedding (t-SNE) <a class=\"anchor\" id=\"section2-3-2\"></a>\n",
    "\n",
    "We now perform [t-SNE dimensionality reduction](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) on the pre-reduced weighted embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE reduction (this takes a few minutes to run)\n",
    "tsne_df = descriptions_tSNE(pca_df, column=column, n_components=3, learning_rate='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the embeddings to a pickle file\n",
    "pickle_file = 'Data/CoreNLP/char_description_embeddings_tsne.pickle'\n",
    "with open(pickle_file, 'wb') as f:\n",
    "    pickle.dump(tsne_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'weighted_descriptions_embeddings'\n",
    "# If loaded, load the embeddings from the pickle file\n",
    "pickle_file = 'Data/CoreNLP/char_description_embeddings_tsne.pickle'\n",
    "if os.path.exists(pickle_file):\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        tsne_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Clustering personas <a class=\"anchor\" id=\"section2-4\"></a>\n",
    "\n",
    "The persona point cloud is clustered into several categories using [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html). This clustering method is mainly parameterized by $\\varepsilon$ (`eps`), corresponding to the \"maximum distance between two samples for one to be considered as in the neighborhood of the other\", and `min_samples`, which is \"the number of samples in a neighborhood for a point to be considered as a core point.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN parameters:\n",
    "eps = 6.7\n",
    "min_samples = 108\n",
    "\n",
    "# Run DBSCAN clustering\n",
    "cluster_df, n_clusters, n_removed = DBSCAN_cluster(tsne_df, column, method='tsne', eps=eps, min_samples=min_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Visualizing persona clusters <a class=\"anchor\" id=\"section2-5\"></a>\n",
    "\n",
    "The clustered persona point cloud is shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 't-SNE + DBSCAN with {} clusters, \\nRemoved {}/{} noisy data points\\nDBSCAN: eps = {}, min_samples = {}\\nFilter: desc = {}, min_words={}'.format(\n",
    "    n_clusters, n_removed, n_total, eps, min_samples, 'descriptions', min_words)\n",
    "plot_clusters_3d(cluster_df, title, column=column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Preparing data for website use <a class=\"anchor\" id=\"section2-6\"></a>\n",
    "\n",
    "We now aggregate all of our data into a single `.csv`file that will be used as the basis of our point cloud on the website. This includes movie metadata, character metadata, actor metadata and embedded character descriptions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cluster_df.copy(deep=True)\n",
    "#From column 'descriptions', keep the three with the highest cosine similarity\n",
    "df = filter_descriptions(df)\n",
    "#Delete columns from cluster_df\n",
    "df = df.drop(columns=['agent_verbs', 'patient_verbs', 'attributes', 'descriptions_embeddings', 'attributes_embeddings', 'title_embeddings',\n",
    "                                      'agent_verbs_embeddings', 'patient_verbs_embeddings', 'weighted_title_embeddings',\n",
    "                                      'weighted_attributes_embeddings', 'weighted_agent_verbs_embeddings', 'weighted_patient_verbs_embeddings',\n",
    "                                      'weighted_descriptions_embeddings', 'descriptions'])\n",
    "\n",
    "df.rename(columns={\n",
    "    'tsne_1_weighted_descriptions_embeddings': 'X',\n",
    "    'tsne_2_weighted_descriptions_embeddings': 'Y',\n",
    "    'tsne_3_weighted_descriptions_embeddings': 'Z'},\n",
    "    inplace=True)\n",
    "    \n",
    "# Delete columns from full_description_df\n",
    "full_descr = full_description_df.copy(deep=True)\n",
    "full_descr = full_descr.drop(columns=['Character name', 'agent_verbs', 'patient_verbs', 'attributes',\n",
    "                                                        'title', 'religion', 'children', 'all_descriptions',\n",
    "                                                        'Freebase ID', 'Date of birth', 'Freebase character/map ID', 'Freebase actor ID'])\n",
    "\n",
    "# Remove duplicates, based on Freebase character ID\n",
    "final_df = df.drop_duplicates(subset=['Freebase character ID'])\n",
    "\n",
    "# Merge on Freebase character ID\n",
    "final_df = df.merge(\n",
    "    full_descr, on='Freebase character ID', how='left')\n",
    "\n",
    "# Convert release date to year\n",
    "final_df['Release date'] = final_df['Release date'].apply(\n",
    "    lambda x: int(x.split('-')[0]) if type(x) == str else x)\n",
    "\n",
    "#Load tsv file from 'Data/CoreNLP/MovieSummaries/movie.metadata.tsv'\n",
    "metadata_df = load_movie_df()\n",
    "\n",
    "# Merge with final_df on Wikipedia ID, keep from metadata_df only the column 'Name'\n",
    "final_df = final_df.merge(\n",
    "    metadata_df[['Box office revenue', 'Genres', 'Wikipedia ID', 'Name']], on='Wikipedia ID', how='left')\n",
    "\n",
    "# Remove columns\n",
    "final_df = final_df.drop(\n",
    "    columns=['age', 'plot_name'])\n",
    "\n",
    "# Get a dictionary of each genre and the number of times it appears in metadata_df\n",
    "genre_dict = {}\n",
    "for genres in metadata_df['Genres']:\n",
    "   # type is list of strings\n",
    "   if type(genres) == list:\n",
    "    for genre in genres:\n",
    "        if genre in genre_dict:\n",
    "            genre_dict[genre] += 1\n",
    "        else:\n",
    "            genre_dict[genre] = 1\n",
    "\n",
    "# For each row in final_df, from column 'Genres', keep the 3 genres that have the highest value in genre_dict\n",
    "final_df['Genres'] = final_df['Genres'].apply(\n",
    "    lambda x: sorted(x, key=lambda genre: genre_dict[genre], reverse=True)[:3] if type(x) == list else x)\n",
    "\n",
    "\n",
    "# Save final_df to a csv file\n",
    "final_df.to_csv(\n",
    "    'Data/CoreNLP/final_df.csv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interactive point cloud\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cda0ac541ab6c535dcb4ffe1de6394d0d0ba460ea4bcec2c3250fd08f595b9ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
