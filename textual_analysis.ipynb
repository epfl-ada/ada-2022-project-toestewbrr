{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Applied Data Analysis Project\n",
    "**Team**: ToeStewBrr - Alexander Sternfeld, Marguerite Thery, Antoine Bonnet, Hugo Bordereaux\n",
    "\n",
    "**Dataset**: CMU Movie Summary Corpus\n",
    "\n",
    "# Part 3: Textual Analysis\n",
    "\n",
    "In this notebook, we analyze the pre-processed output of our custom CoreNLP pipeline. \n",
    "\n",
    "### Table of contents\n",
    "1. [Loading pre-processed coreNLP data](#section1)\n",
    "2. [Persona clusters](#section2)\n",
    "    - 2.1. [Embedding descriptions](#section2-1)\n",
    "    - 2.2. [Weighted average of word vectors](#section2-2)\n",
    "    - 2.3. [Dimensionality reduction](#section2-3)\n",
    "        - 2.3.1. [Principal Component Analysis (PCA)](#section2-3-1)\n",
    "        - 2.3.2. [*t*-distributed Stochastic Neighbor Embedding (t-SNE)](#section2-3-2)\n",
    "    - 2.4. [Clustering personas](#section2-4)\n",
    "    - 2.5. [Visualizing persona clusters](#section2-5)\n",
    "    - 2.6. [Preparing data for website use](#section2-6)\n",
    "\n",
    "### 2.3. Dimensionality reduction <a class=\"anchor\" id=\"section2-3\"></a>\n",
    "\n",
    "#### 2.3.1. Principal Component Analysis (PCA) <a class=\"anchor\" id=\"section2-3-1\"></a>\n",
    "\n",
    "**Prerequisite**: \n",
    "\n",
    "Install [spaCy](https://spacy.io) using the following commands: \n",
    "\n",
    "        pip install spacy\n",
    "        \n",
    "        python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "import random\n",
    "\n",
    "from extraction import *\n",
    "from coreNLP_analysis import *\n",
    "from load_data import *\n",
    "from textual_analysis import *\n",
    "\n",
    "\n",
    "# NOTE: If you haven't loaded NLTK before, set this to True\n",
    "load_nltk = False\n",
    "\n",
    "if load_nltk: #Load the spaCy model for the semantic analysis\n",
    "    nlp_spacy = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load pre-processed coreNLP data <a class=\"anchor\" id=\"section1\"></a>\n",
    "\n",
    "We first load the pre-processed output from our custom CoreNLP pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_description_path = 'Data/CoreNLP/char_descriptions.csv'\n",
    "full_description_path = 'Data/CoreNLP/full_descriptions.csv'\n",
    "\n",
    "# Load character descriptions\n",
    "char_description_df = pd.read_csv(char_description_path, sep='\\t', index_col=None, low_memory=False)\n",
    "\n",
    "# Convert to lists\n",
    "char_description_df['agent_verbs'] = char_description_df.agent_verbs.apply(\n",
    "    lambda x: literal_eval(x) if type(x) == str else x)\n",
    "char_description_df['patient_verbs'] = char_description_df.patient_verbs.apply(\n",
    "    lambda x: literal_eval(x) if type(x) == str else x)\n",
    "char_description_df['attributes'] = char_description_df.attributes.apply(\n",
    "    lambda x: literal_eval(x) if type(x) == str else x)\n",
    "char_description_df['descriptions'] = char_description_df.descriptions.apply(\n",
    "    lambda x: literal_eval(x) if type(x) == str else x)\n",
    "char_description_df['title'] = char_description_df.title.apply(\n",
    "    lambda x: literal_eval(x) if type(x) == str else x)\n",
    "\n",
    "full_description_df = pd.read_csv(full_description_path, sep='\\t', index_col=None, low_memory=False)\n",
    "relations_df = pd.read_csv('Data/CoreNLP/char_relations.csv', sep='\\t', index_col=None, low_memory=False)\n",
    "char_df = load_char_df()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Persona clusters <a class=\"anchor\" id=\"section2\"></a>\n",
    "\n",
    "### 2.1. Embedding descriptions <a class=\"anchor\" id=\"section2-1\"></a>\n",
    "\n",
    "We embed all descriptive words (actions, attributes, titles) of all characters into a high-dimensional vector space using spaCy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = 'Data/CoreNLP/char_description_embeddings.pickle'\n",
    "\n",
    "# If we have already embedded the descriptions, load them from the pickle file\n",
    "if os.path.exists(embedding_file):\n",
    "    char_description_df = pd.read_pickle(embedding_file)\n",
    "\n",
    "else:\n",
    "    # Embed descriptions (Get a comfy chair, this takes a while) \n",
    "    char_description_df = construct_descriptions_embeddings(char_description_df, nlp_spacy)\n",
    "\n",
    "    # Split embeddings by category\n",
    "    char_description_df = embeddings_categorical(char_description_df)\n",
    "\n",
    "    # Save the embeddings to a pickle file\n",
    "    with open(embedding_file, 'wb') as f:\n",
    "        pickle.dump(char_description_df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Weighted average of word vectors <a class=\"anchor\" id=\"section2-2\"></a>\n",
    "\n",
    "We then weigh the word embedding of each word for each character by their cosine distance to the average semantic vector of words with the sam type used for all characters in the dataset. The *cosine distance* is defined as:\n",
    "\n",
    "$$\\text{cosine distance}(x_1, x_2) = 1-\\frac{x_1 \\cdot x_2}{||x_1||\\cdot||x_2||}$$\n",
    "\n",
    "where $x_1$ and $x_2$ are the vector representations of two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_df = weight_embeddings(char_description_df, column='title', percentile=0)\n",
    "\n",
    "weight_df = weight_embeddings(weight_df, column='attributes', percentile=60)\n",
    "\n",
    "weight_df = weight_embeddings(weight_df, column='agent_verbs', percentile=75)\n",
    "\n",
    "weight_df = weight_embeddings(weight_df, column='patient_verbs', percentile=85)\n",
    "\n",
    "weight_df = weight_embeddings(weight_df, column='descriptions', title_weight=0.35)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Dimensionality reduction <a class=\"anchor\" id=\"section2-3\"></a>\n",
    "\n",
    "#### 2.3.1. Principal Component Analysis (PCA) <a class=\"anchor\" id=\"section2-3-1\"></a>\n",
    "\n",
    "To visualize our clusters, we then map these high-dimensional descriptive vectors to 50-dimensional space using PCA to prepare the ground for a second dimensionality reduction technique. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only the relations where the wikipedia ID is the same for both characters (they are in the same movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_df['subject wikipedia ID'] = relations_df['Subject freebase character ID'].apply(\n",
    "    lambda x: char_df[char_df['Freebase character ID'] == x]['Wikipedia ID'].values[0] if type(x) == str else x)\n",
    "relations_df['object wikipedia ID'] = relations_df['Object freebase character ID'].apply(\n",
    "    lambda x: char_df[char_df['Freebase character ID'] == x]['Wikipedia ID'].values[0] if type(x) == str else x)\n",
    "# find the indices of rows in relations_df where both subject wikipedia ID and object wikipedia ID are not NaN and they are not equal\n",
    "indices = relations_df[(relations_df['subject wikipedia ID'].notnull()) & (relations_df['object wikipedia ID'].notnull()) & (\n",
    "    relations_df['subject wikipedia ID'] != relations_df['object wikipedia ID'])].index\n",
    "# remove these rows from relations_df\n",
    "relations_df = relations_df.drop(indices)\n",
    "# remove the columns 'subject wikipedia ID' and 'object wikipedia ID'\n",
    "relations_df = relations_df.drop(\n",
    "    columns=['subject wikipedia ID', 'object wikipedia ID'])\n",
    "# Keep only the rows in relations_df where the Subject freebase character ID and Object freebase character ID are both not NaN\n",
    "relations_df = relations_df[(relations_df['Subject freebase character ID'].notnull()) & (\n",
    "    relations_df['Object freebase character ID'].notnull())]\n",
    "# Get a set of the freebase character IDs in relations_df\n",
    "freebase_ids = set(relations_df['Subject freebase character ID'].unique()).union(\n",
    "    set(relations_df['Object freebase character ID'].unique()))\n",
    "\n",
    "# Add a dummy column to df called 'in_relation' which is True if the freebase character ID is in freebase_ids and False otherwise\n",
    "weight_df['in_relation'] = weight_df['Freebase character ID'].apply(\n",
    "    lambda x: x in freebase_ids if type(x) == str else False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows in char_description_df that have less than X descriptions\n",
    "min_words_non_rel = 5\n",
    "min_words_rel = 1\n",
    "df = weight_df.copy(deep=True)\n",
    "df = df[df.descriptions.apply(lambda x: type(x) != float)]\n",
    "\n",
    "# For characters that are not in a relation, keep only rows with at least min_words_non_rel descriptions\n",
    "df = df[(df.in_relation == False) | (df.descriptions.apply(lambda x: len(x) >= min_words_rel))]\n",
    "\n",
    "# For characters that are in a relation, keep only rows with at least min_words_rel descriptions\n",
    "df = df[(df.in_relation == True) | (df.descriptions.apply(lambda x: len(x) >= min_words_non_rel))]\n",
    "\n",
    "# Print percentage of characters kept\n",
    "print('Percentage of characters kept: {:.2f}%'.format(100*len(df)/len(weight_df)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction: PCA to 50 dimensions -> t-SNE to 3 dimensions\n",
    "column = 'weighted_descriptions_embeddings'\n",
    "n_total = df[column].apply(lambda x: 1 if type(x) == np.ndarray else 0).sum()\n",
    "pca_df = descriptions_PCA(df, column=column, n_components=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2. *t*-distributed Stochastic Neighbor Embedding (t-SNE) <a class=\"anchor\" id=\"section2-3-2\"></a>\n",
    "\n",
    "We now perform [t-SNE dimensionality reduction](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) on the pre-reduced weighted embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'weighted_descriptions_embeddings'\n",
    "\n",
    "# If loaded, load the embeddings from the pickle file\n",
    "pickle_file = 'Data/CoreNLP/char_description_embeddings_tsne.pickle'\n",
    "if os.path.exists(pickle_file):\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        tsne_df = pickle.load(f)\n",
    "\n",
    "else:\n",
    "    # t-SNE reduction (this takes a few minutes to run)\n",
    "    tsne_df = descriptions_tSNE(\n",
    "        pca_df, column=column, n_components=3, learning_rate='auto')\n",
    "\n",
    "    # Save the embeddings to a pickle file\n",
    "    pickle_file = 'Data/CoreNLP/char_description_embeddings_tsne.pickle'\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump(tsne_df, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Clustering personas <a class=\"anchor\" id=\"section2-4\"></a>\n",
    "\n",
    "The persona point cloud is clustered into several categories using [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html). This clustering method is mainly parameterized by $\\varepsilon$ (`eps`), corresponding to the \"maximum distance between two samples for one to be considered as in the neighborhood of the other\", and `min_samples`, which is \"the number of samples in a neighborhood for a point to be considered as a core point.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN parameters:\n",
    "eps = 6.07\n",
    "min_samples = 107\n",
    "\n",
    "# Run DBSCAN clustering\n",
    "cluster_df, n_clusters, n_removed = DBSCAN_cluster(tsne_df, column, method='tsne', eps=eps, min_samples=min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of rows in relations_df where the Subject freebase character ID and Object freebase character ID are both in Freebase character ID of cluster_df\n",
    "n_relations = relations_df[(relations_df['Subject freebase character ID'].isin(cluster_df['Freebase character ID'])) & (relations_df['Object freebase character ID'].isin(cluster_df['Freebase character ID']))].shape[0]\n",
    "print('Number of relations kept: {}'.format(n_relations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Visualizing persona clusters <a class=\"anchor\" id=\"section2-5\"></a>\n",
    "\n",
    "The clustered persona point cloud is shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 't-SNE + DBSCAN with {} clusters, \\nRemoved {}/{} noisy data points\\nDBSCAN: eps = {}, min_samples = {}\\nFilter: desc = {}, min_words={}'.format(\n",
    "    n_clusters, n_removed, n_total, eps, min_samples, 'descriptions', min_words_non_rel)\n",
    "plot_clusters_3d(cluster_df, title, column=column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Preparing data for website use <a class=\"anchor\" id=\"section2-6\"></a>\n",
    "\n",
    "We now aggregate all of our data into a single `.csv`file that will be used as the basis of our point cloud on the website. This includes movie metadata, character metadata, actor metadata and embedded character descriptions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cluster_df.copy(deep=True)\n",
    "#From column 'descriptions', keep the three with the highest cosine similarity\n",
    "df = filter_descriptions(df)\n",
    "#Delete columns from cluster_df\n",
    "df = df.drop(columns=['agent_verbs', 'patient_verbs', 'attributes', 'descriptions_embeddings', 'attributes_embeddings', 'title_embeddings',\n",
    "                                      'agent_verbs_embeddings', 'patient_verbs_embeddings', 'weighted_title_embeddings',\n",
    "                                      'weighted_attributes_embeddings', 'weighted_agent_verbs_embeddings', 'weighted_patient_verbs_embeddings',\n",
    "                                      'weighted_descriptions_embeddings', 'descriptions'])\n",
    "\n",
    "df.rename(columns={\n",
    "    'tsne_1_weighted_descriptions_embeddings': 'X',\n",
    "    'tsne_2_weighted_descriptions_embeddings': 'Y',\n",
    "    'tsne_3_weighted_descriptions_embeddings': 'Z'},\n",
    "    inplace=True)\n",
    "    \n",
    "# Delete columns from full_description_df\n",
    "full_descr = full_description_df.copy(deep=True)\n",
    "full_descr = full_descr.drop(columns=['Character name', 'agent_verbs', 'patient_verbs', 'attributes',\n",
    "                                                        'title', 'religion', 'children', 'all_descriptions',\n",
    "                                                        'Freebase ID', 'Date of birth', 'Freebase character/map ID', 'Freebase actor ID'])\n",
    "\n",
    "# Remove duplicates, based on Freebase character ID\n",
    "final_df = df.drop_duplicates(subset=['Freebase character ID'])\n",
    "\n",
    "# Merge on Freebase character ID\n",
    "final_df = df.merge(\n",
    "    full_descr, on='Freebase character ID', how='left')\n",
    "\n",
    "# Convert release date to year\n",
    "final_df['Release date'] = final_df['Release date'].apply(\n",
    "    lambda x: int(x.split('-')[0]) if type(x) == str else x)\n",
    "\n",
    "#Load tsv file from 'Data/CoreNLP/MovieSummaries/movie.metadata.tsv'\n",
    "metadata_df = load_movie_df()\n",
    "\n",
    "# Merge with final_df on Wikipedia ID, keep from metadata_df only the column 'Name'\n",
    "final_df = final_df.merge(\n",
    "    metadata_df[['Box office revenue', 'Genres', 'Wikipedia ID', 'Name']], on='Wikipedia ID', how='left')\n",
    "\n",
    "# Remove columns\n",
    "final_df = final_df.drop(\n",
    "    columns=['age', 'plot_name'])\n",
    "\n",
    "# Get a dictionary of each genre and the number of times it appears in metadata_df\n",
    "genre_dict = {}\n",
    "for genres in metadata_df['Genres']:\n",
    "   # type is list of strings\n",
    "   if type(genres) == list:\n",
    "    for genre in genres:\n",
    "        if genre in genre_dict:\n",
    "            genre_dict[genre] += 1\n",
    "        else:\n",
    "            genre_dict[genre] = 1\n",
    "\n",
    "# For each row in final_df, from column 'Genres', keep the 3 genres that have the highest value in genre_dict\n",
    "final_df['Genres'] = final_df['Genres'].apply(\n",
    "    lambda x: sorted(x, key=lambda genre: genre_dict[genre], reverse=True)[:3] if type(x) == list else x)\n",
    "\n",
    "\n",
    "# Save final_df to a csv file\n",
    "final_df.to_csv(\n",
    "    'Data/CoreNLP/final_df_test.csv', sep='\\t', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clustering analysis\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load final df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load final_df from csv file\n",
    "final_df = pd.read_csv('Data/CoreNLP/final_df_test.csv', sep='\\t')\n",
    "# Load relations.csv\n",
    "relations_df = pd.read_csv('Data/CoreNLP/char_relations.csv', sep='\\t')\n",
    "# Load movie metadata\n",
    "metadata_df = load_movie_df()\n",
    "# Rename movie_id to Wikipedia ID\n",
    "relations_df.rename(columns={'movie_id': 'Wikipedia ID'}, inplace=True)\n",
    "#load char_df\n",
    "char_df = load_char_df()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_df_final = relations_df.copy()\n",
    "# Get a subset of relations_df where the Subject freebase character ID and Object freebase character ID are in final_df\n",
    "relations_df_final = relations_df[relations_df['Subject freebase character ID'].isin(final_df['Freebase character ID'])]\n",
    "relations_df_final = relations_df_final[relations_df_final['Object freebase character ID'].isin(final_df['Freebase character ID'])]\n",
    "\n",
    "\n",
    "final_df['partner'] = [[] for _ in range(len(final_df))]\n",
    "for index2, row2 in relations_df_final.iterrows():\n",
    "    id_subject = row2['Subject freebase character ID']\n",
    "    wiki_id = row2['Wikipedia ID']\n",
    "    # Find row where Freebase character ID is the same as the Subject freebase character ID and Wikipedia ID as well\n",
    "    row = final_df[(final_df['Freebase character ID'] == id_subject) & (final_df['Wikipedia ID'] == wiki_id)]\n",
    "    # if row is empty, continue\n",
    "    if row.empty:\n",
    "        continue\n",
    "    # Get the index of the row\n",
    "    index = row.index[0]\n",
    "    id_object = row2['Object freebase character ID'] \n",
    "    if final_df[(final_df['Freebase character ID'] == id_object) & (final_df['Wikipedia ID'] == wiki_id)].empty:\n",
    "        continue\n",
    "    # Add the Object freebase character ID to the list in the 'partner' column\n",
    "    final_df.at[index, 'partner'] = final_df.at[index, 'partner'] + [row2['Object freebase character ID']]\n",
    "    \n",
    "# Set empty lists to nan\n",
    "final_df['partner'] = final_df['partner'].apply(lambda x: np.nan if len(x) == 0 else x)\n",
    "\n",
    "# Add column 'Has_partner' with 1 if the character has a partner and 0 if not\n",
    "final_df['Has_partner'] = final_df['partner'].apply(lambda x: 1 if type(x) == list else 0)\n",
    "\n",
    "# Save final_df to a csv file\n",
    "final_df.to_csv(\n",
    "    'Data/CoreNLP/final_df_test.csv', sep='\\t', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some code for labeling - REMOVE FOR FINAL SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "df_new = final_df.copy(deep=True)\n",
    "# import CountVectorizer\n",
    "\n",
    "# Get a dictionary of each cluster and the top 3 characters with the highest box office revenue\n",
    "cluster_dict = {}\n",
    "for cluster in df_new['labels'].unique():\n",
    "    cluster_df = df_new[df_new['labels'] == cluster]\n",
    "    cluster_df = cluster_df.sort_values(\n",
    "        by='Box office revenue', ascending=False)\n",
    "    cluster_dict[cluster] = cluster_df[['Character name',\n",
    "                                        'Name']].head(10).values.tolist()\n",
    "\n",
    "\n",
    "df_new = pd.DataFrame.from_dict(cluster_dict, orient='index', columns=[\n",
    "                                'character 1', 'character 2', 'character 3', 'character 4', 'character 5', 'character 6', 'character 7', 'character 8', 'character 9', 'character 10'])\n",
    "\n",
    "\n",
    "def get_top_n_titles(corpus, n=None):\n",
    "    # Corpus may contain NaN values, so we need to remove them\n",
    "    corpus = corpus.dropna()\n",
    "    # Corpus is a pandas series, so we need to convert it to a list\n",
    "    corpus = corpus.tolist()\n",
    "    # Now it is a list of lists of strings, convert it to a list of strings\n",
    "    corpus = [item for sublist in corpus for item in sublist]\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx])\n",
    "                  for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    # Corpus may contain NaN values, so we need to remove them\n",
    "    corpus = corpus.dropna()\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx])\n",
    "                  for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "# Make a dictionary with as key the cluster number and as value a list with the 10 most frequent words in the column \"descriptions\" for that cluster, use final_df\n",
    "cluster_dict = {}\n",
    "for cluster in final_df['labels'].unique():\n",
    "    cluster_df = final_df[final_df['labels'] == cluster]\n",
    "    cluster_dict[cluster] = get_top_n_titles(\n",
    "        cluster_df['title'], 10)\n",
    "\n",
    "# Add column 'top 10 words' to df_new, which contains the 10 most frequent words in the column \"descriptions\" for each cluster, but not the count\n",
    "df_new['top 10 title'] = df_new.index.map(cluster_dict)\n",
    "# Keep only the top 10 words, not the count\n",
    "df_new['top 10 title'] = df_new['top 10 title'].apply(\n",
    "    lambda x: [word[0] for word in x])\n",
    "\n",
    "\n",
    "# Do the same for attributes\n",
    "cluster_dict = {}\n",
    "for cluster in final_df['labels'].unique():\n",
    "    cluster_df = final_df[final_df['labels'] == cluster]\n",
    "    cluster_dict[cluster] = get_top_n_words(\n",
    "        cluster_df['attributes'], 10)\n",
    "\n",
    "df_new['top 10 attributes'] = df_new.index.map(cluster_dict)\n",
    "df_new['top 10 attributes'] = df_new['top 10 attributes'].apply(\n",
    "    lambda x: [word[0] for word in x])\n",
    "\n",
    "# Do the same for agent_verbs\n",
    "cluster_dict = {}\n",
    "for cluster in final_df['labels'].unique():\n",
    "    cluster_df = final_df[final_df['labels'] == cluster]\n",
    "    cluster_dict[cluster] = get_top_n_words(\n",
    "        cluster_df['agent_verbs'], 10)\n",
    "\n",
    "df_new['top 10 agent verbs'] = df_new.index.map(cluster_dict)\n",
    "df_new['top 10 agent verbs'] = df_new['top 10 agent verbs'].apply(\n",
    "    lambda x: [word[0] for word in x])\n",
    "# save df_new to a csv file\n",
    "df_new.to_csv('Data/CoreNLP/clusters.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cluster_df.copy(deep=True)\n",
    "#From column 'descriptions', keep the three with the highest cosine similarity\n",
    "df = filter_descriptions(df)\n",
    "#Delete columns from cluster_df\n",
    "df = df.drop(columns=['agent_verbs', 'patient_verbs', 'attributes', 'descriptions_embeddings', 'attributes_embeddings', 'title_embeddings',\n",
    "                      'agent_verbs_embeddings', 'patient_verbs_embeddings', 'weighted_title_embeddings',\n",
    "                      'weighted_attributes_embeddings', 'weighted_agent_verbs_embeddings', 'weighted_patient_verbs_embeddings',\n",
    "                      'weighted_descriptions_embeddings', 'descriptions'])\n",
    "\n",
    "df.rename(columns={\n",
    "    'tsne_1_weighted_descriptions_embeddings': 'X',\n",
    "    'tsne_2_weighted_descriptions_embeddings': 'Y',\n",
    "    'tsne_3_weighted_descriptions_embeddings': 'Z'},\n",
    "    inplace=True)\n",
    "\n",
    "# Delete columns from full_description_df\n",
    "full_descr = full_description_df.copy(deep=True)\n",
    "full_descr = full_descr.drop(columns=['Character name', 'title', 'religion', 'children', 'all_descriptions',\n",
    "                                                        'Freebase ID', 'Date of birth', 'Freebase character/map ID', 'Freebase actor ID'])\n",
    "\n",
    "# Remove duplicates, based on Freebase character ID\n",
    "final_df = df.drop_duplicates(subset=['Freebase character ID'])\n",
    "\n",
    "# Merge on Freebase character ID\n",
    "final_df = df.merge(\n",
    "    full_descr, on='Freebase character ID', how='left')\n",
    "\n",
    "# Convert release date to year\n",
    "final_df['Release date'] = final_df['Release date'].apply(\n",
    "    lambda x: int(x.split('-')[0]) if type(x) == str else x)\n",
    "\n",
    "#Load tsv file from 'Data/CoreNLP/MovieSummaries/movie.metadata.tsv'\n",
    "metadata_df = load_movie_df()\n",
    "\n",
    "# Merge with final_df on Wikipedia ID, keep from metadata_df only the column 'Name'\n",
    "final_df = final_df.merge(\n",
    "    metadata_df[['Box office revenue', 'Genres', 'Wikipedia ID', 'Name']], on='Wikipedia ID', how='left')\n",
    "\n",
    "# Remove columns\n",
    "final_df = final_df.drop(\n",
    "    columns=['age', 'plot_name'])\n",
    "\n",
    "# Get a dictionary of each genre and the number of times it appears in metadata_df\n",
    "genre_dict = {}\n",
    "for genres in metadata_df['Genres']:\n",
    "   # type is list of strings\n",
    "   if type(genres) == list:\n",
    "    for genre in genres:\n",
    "        if genre in genre_dict:\n",
    "            genre_dict[genre] += 1\n",
    "        else:\n",
    "            genre_dict[genre] = 1\n",
    "\n",
    "# For each row in final_df, from column 'Genres', keep the 3 genres that have the highest value in genre_dict\n",
    "final_df['Genres'] = final_df['Genres'].apply(\n",
    "    lambda x: sorted(x, key=lambda genre: genre_dict[genre], reverse=True)[:3] if type(x) == list else x)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INCORRECT CODE FOR ADDING RELATIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all Subjects where the Freebase Character ID is not nan\n",
    "subjects_known = relations_df[relations_df['Subject freebase character ID'].notna()]['Subject'].unique()\n",
    "objects_known = relations_df[relations_df['Object freebase character ID'].notna()]['Object'].unique()\n",
    "subjects_unknown = relations_df[relations_df['Subject freebase character ID'].isna()]\n",
    "objects_unknown = relations_df[relations_df['Object freebase character ID'].isna()]\n",
    "\n",
    "# Map subjects_unknown to Character name\n",
    "subjects_unknown_dict = {}\n",
    "for index_rel, row_rel in subjects_unknown.iterrows():\n",
    "    # Find the movie in final_df\n",
    "    wiki_id = row_rel['Wikipedia ID']\n",
    "    # Get rows from final_df where Wikipedia ID is wiki_id, but it might not be in the dataframe\n",
    "    df = final_df[final_df['Wikipedia ID'] == wiki_id]\n",
    "    # If the dataframe is not empty\n",
    "    if not df.empty:\n",
    "        for index, row in df.iterrows():\n",
    "            # Get the character name\n",
    "            character = row['Character name']\n",
    "            # Get the subject\n",
    "            subject = row_rel['Subject']\n",
    "            # Check if Wikipedia ID is in subject\n",
    "            if subject in character:\n",
    "                # Store mapping, use subject and wiki_id as key\n",
    "                print('Yay')\n",
    "                subjects_unknown_dict[subject] = character\n",
    "            elif character in subject:\n",
    "                subjects_unknown_dict[subject] = character\n",
    "                print('Yay')\n",
    "\n",
    "# Do the same for objects_unknown\n",
    "objects_unknown_dict = {}\n",
    "for index_rel, row_rel in objects_unknown.iterrows():\n",
    "    # Find the movie in final_df\n",
    "    wiki_id = row_rel['Wikipedia ID']\n",
    "    # Get rows from final_df where Wikipedia ID is wiki_id, but it might not be in the dataframe\n",
    "    df = final_df[final_df['Wikipedia ID'] == wiki_id]\n",
    "    # If the dataframe is not empty\n",
    "    if not df.empty:\n",
    "        for index, row in df.iterrows():\n",
    "            # Get the character name\n",
    "            character = row['Character name']\n",
    "            # Get the subject\n",
    "            object = row_rel['Object']\n",
    "\n",
    "            # Check if Wikipedia ID is in subject\n",
    "            if object in character:\n",
    "                print('yay')\n",
    "                # Store mapping, use subject and wiki_id as key\n",
    "                objects_unknown_dict[[object, wiki_id]] = character\n",
    "            elif character in object:\n",
    "                print('yay')\n",
    "                objects_unknown_dict[[object, wiki_id]] = character\n",
    "\n",
    "# For each row in relations_df, if the Subject is in subjects_unknown_dict, replace it with the value from subjects_unknown_dict and add the Freebase character ID\n",
    "for index, row in relations_df.iterrows():\n",
    "    wiki_id = row['Wikipedia ID']\n",
    "    subject = row['Subject']\n",
    "    if [subject, wiki_id] in subjects_unknown_dict:\n",
    "        relations_df.at[index,\n",
    "                        'Subject'] = subjects_unknown_dict[[subject, wiki_id]]\n",
    "        relations_df.at[index, 'Subject freebase character ID'] = final_df[final_df['Character name']\n",
    "                                                                           == subjects_unknown_dict[[subject, wiki_id]]]['Freebase character ID'].values[0]\n",
    "        # print if the Wikipedia ID of the row in relations_df and the Wikipedia ID of the row in final_df are not the same\n",
    "        if row['Wikipedia ID'] != final_df[final_df['Character name'] == subjects_unknown_dict[[subject, wiki_id]]]['Wikipedia ID'].values[0]:\n",
    "            print('Wikipedia ID not the same')\n",
    "\n",
    "# For each row in relations_df, if the Object is in objects_unknown_dict, replace it with the value from objects_unknown_dict and add the Freebase character ID\n",
    "for index, row in relations_df.iterrows():\n",
    "    wiki_id = row['Wikipedia ID']\n",
    "    object = row['Object']\n",
    "    if [object, wiki_id] in objects_unknown_dict:\n",
    "        relations_df.at[index,\n",
    "                        'Object'] = objects_unknown_dict[[object, wiki_id]]\n",
    "        relations_df.at[index, 'Object freebase character ID'] = final_df[final_df['Character name']\n",
    "                                                                          == objects_unknown_dict[[object, wiki_id]]]['Freebase character ID'].values[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "645a45bafa7a70bf663118869c5a8c62ee426a479322e8202ebe39ad8b921733"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
