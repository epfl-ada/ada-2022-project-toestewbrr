{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "AYC13i4R39dN"
   },
   "source": [
    "# Applied Data Analysis Project\n",
    "**Team**: ToeStewBrr - Alexander Sternfeld, Marguerite Thery, Antoine Bonnet, Hugo Bordereaux\n",
    "\n",
    "**Dataset**: CMU Movie Summary Corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MlSSpJAG39dP"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import tarfile\n",
    "import urllib\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import re\n",
    "import gzip\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TadIrDF39dR"
   },
   "source": [
    "## 1. Loading data\n",
    "\n",
    "We first extract all files from the [MoviesSummaries dataset](http://www.cs.cmu.edu/~ark/personas/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('Data/MovieSummaries'):\n",
    "    filename = 'http://www.cs.cmu.edu/~ark/personas/data/MovieSummaries.tar.gz'\n",
    "    my_tar = tarfile.open(fileobj=urllib.request.urlopen(filename), mode=\"r:gz\") \n",
    "    my_tar.extractall('./Data') # specify which folder to extract to\n",
    "    my_tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`corenlp_plot_summaries.tar.gz [628 M, separate download]`: The plot summary of each movie, run through the Stanford CoreNLP pipeline (tagging, parsing, NER and coref). Each filename begins with the Wikipedia movie ID (which indexes into movie.metadata.tsv).\n",
    "\n",
    "We now extract all coreNLP files, then uncompress them to the XML format. \n",
    "\n",
    "Note: Extraction of CoreNLP files takes 15 minutes, while conversion takes 30 seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all coreNLP files to Data/CoreNLP\n",
    "if not os.path.exists('Data/CoreNLP'):\n",
    "    coreNLPfilename = 'http://www.cs.cmu.edu/~ark/personas/data/corenlp_plot_summaries.tar'\n",
    "    my_tar = tarfile.open(fileobj=urllib.request.urlopen(coreNLPfilename), mode=\"r|\") \n",
    "    my_tar.extractall(path='./Data/CoreNLP') # specify which folder to extract to\n",
    "    my_tar.close()\n",
    "\n",
    "# Convert every file in directory Data/CoreNLP to xml format\n",
    "raw_dir = 'Data/CoreNLP/corenlp_plot_summaries'\n",
    "extracted_dir = 'Data/CoreNLP/corenlp_plot_summaries_xml'\n",
    "if not os.path.exists(extracted_dir):\n",
    "    os.mkdir(extracted_dir)\n",
    "    for filename in os.listdir(raw_dir):\n",
    "        f = os.path.join(raw_dir, filename) \n",
    "        if os.path.isfile(f):\n",
    "            # Open and store file as xml \n",
    "            with gzip.open(f, 'rb') as f_in:\n",
    "                gz_file = os.path.join(extracted_dir, filename)\n",
    "                with open(gz_file[:-3], 'wb') as f_out:\n",
    "                    f_out.write(f_in.read())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoIsNdG539dR"
   },
   "source": [
    "## 2. Pre-processing data\n",
    "\n",
    "### 2.1. Plot summaries\n",
    "\n",
    "`plot_summaries.txt [29 M]`: Plot summaries of 42,306 movies extracted from the November 2, 2012 dump of English-language Wikipedia.  Each line contains the Wikipedia movie ID (which indexes into movie.metadata.tsv) followed by the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6AUVYUc39dS",
    "outputId": "f909218b-7eaa-40d6-faee-e6937677b4c9"
   },
   "outputs": [],
   "source": [
    "plot_path = 'Data/MovieSummaries/plot_summaries.txt'\n",
    "plot_cols = ['Wikipedia ID', 'Summary']\n",
    "plot_df = pd.read_csv(plot_path, sep='\\t', header=None, names=plot_cols, index_col=0)\n",
    "plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Hugo: this method stems the words to their lexical root. \n",
    "# Implement Stemming using out of the box Porter algorithm\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "plot_stemmed = [[stemmer.stem(word) for word in sentence.split(\" \")] for sentence in plot_df.iloc[:5].Summary]\n",
    "plot_stemmed = [\" \".join(sentence) for sentence in plot_stemmed]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The word count conversion and tf-idf weighting produce sparse matrices which are destined to be used by NNs. We need something different. \n",
    "# Word count conversion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(strip_accents='ascii',stop_words='english')\n",
    "plot_counts = count_vect.fit_transform(plot_stemmed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF weighting\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "plot_data = tfidf_transformer.fit_transform(plot_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "# from sklearn.preprocessing import normalize\n",
    "# normalize(newsgroups_trainData, norm='l1', axis=0, copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8vh3IEW39dS"
   },
   "source": [
    "### 2.2. Movie metadata\n",
    "\n",
    "`movie.metadata.tsv.gz [3.4 M]`: Metadata for 81,741 movies, extracted from the Noverber 4, 2012 dump of Freebase.  Tab-separated; columns:\n",
    "\n",
    "1. Wikipedia movie ID\n",
    "2. Freebase movie ID\n",
    "3. Movie name\n",
    "4. Movie release date\n",
    "5. Movie box office revenue\n",
    "6. Movie runtime\n",
    "7. Movie languages (Freebase ID:name tuples)\n",
    "8. Movie countries (Freebase ID:name tuples)\n",
    "9. Movie genres (Freebase ID:name tuples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 898
    },
    "id": "p5NKaQHm39dT",
    "outputId": "3b973e13-6efd-4c48-95a7-0346d5ec753c"
   },
   "outputs": [],
   "source": [
    "strip_encoding = lambda x: np.nan if x == '{}' else \\\n",
    "    [w.replace(' Language', '').replace(' language', '') for w in re.findall(r'\"(.*?)\"', x)[1::2]]\n",
    "\n",
    "movie_path = 'Data/MovieSummaries/movie.metadata.tsv'\n",
    "movie_cols = ['Wikipedia ID', 'Freebase ID', 'Name', 'Release date', \n",
    "              'Box office revenue', 'Runtime', 'Languages', 'Countries', 'Genres']\n",
    "movie_df = pd.read_csv(movie_path, sep='\\t', header=None, names=movie_cols, index_col=0, dtype = {'Freebase ID': str})\n",
    "movie_df['Languages'] = movie_df['Languages'].apply(strip_encoding)\n",
    "movie_df['Countries'] = movie_df['Countries'].apply(strip_encoding)\n",
    "movie_df['Genres'] = movie_df['Genres'].apply(strip_encoding)\n",
    "movie_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMGsQrxV39dT"
   },
   "source": [
    "### 2.3. Character metadata\n",
    "\n",
    "`character.metadata.tsv.gz [14 M]`: Metadata for 450,669 characters aligned to the movies above, extracted from the November 4, 2012 dump of Freebase.  Tab-separated; columns:\n",
    "\n",
    "1. Wikipedia movie ID\n",
    "2. Freebase movie ID\n",
    "3. Movie release date\n",
    "4. Character name\n",
    "5. Actor date of birth\n",
    "6. Actor gender\n",
    "7. Actor height (in meters)\n",
    "8. Actor ethnicity (Freebase ID)\n",
    "9. Actor name\n",
    "10. Actor age at movie release\n",
    "11. Freebase character/actor map ID\n",
    "12. Freebase character ID\n",
    "13. Freebase actor ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 762
    },
    "id": "7xdjzblS39dT",
    "outputId": "9c609dff-a8d6-495a-f3c5-7e12ffb2639d"
   },
   "outputs": [],
   "source": [
    "char_path = 'Data/MovieSummaries/character.metadata.tsv'\n",
    "char_cols = ['Wikipedia ID', 'Freebase ID', 'Release date', 'Character name', 'Date of birth', \n",
    "             'Gender', 'Height', 'Ethnicity', 'Actor name', 'Actor age at release', \n",
    "             'Freebase character/map ID', 'Freebase character ID', 'Freebase actor ID']\n",
    "char_df = pd.read_csv(char_path, sep='\\t', header=None, names=char_cols, index_col=0)\n",
    "char_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Name clusters\n",
    "\n",
    "`name.clusters.txt`: 970 unique character names used in at least two different movies, along with 2,666 instances of those types.  The ID field indexes into the Freebase character/actor map ID in character.metadata.tsv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Data/MovieSummaries/'\n",
    "names_path = path+'name.clusters.txt'\n",
    "names_cols = ['Character name', 'Cluster']\n",
    "names_df = pd.read_csv(names_path, sep='\\t', header=None, names=names_cols, dtype = {'Freebase ID': str})\n",
    "names_df = names_df.groupby('Character name').aggregate(list)\n",
    "names_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. TV Tropes Clusters\n",
    "\n",
    "`tvtropes.clusters.txt`: 72 character types drawn from tvtropes.com, along with 501 instances of those types.  The ID field indexes into the Freebase character/actor map ID in character.metadata.tsv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_path = path+'tvtropes.clusters.txt'\n",
    "cluster_cols = ['Cluster', 'Character data']\n",
    "cluster_df = pd.read_csv(cluster_path, sep='\\t', header=None, names=cluster_cols, dtype = {'Freebase ID': str})\n",
    "cluster_df['Character data'] = cluster_df['Character data'].apply(lambda x: ast.literal_eval(x))\n",
    "cluster_df['Character name'] = cluster_df['Character data'].apply(lambda x: x['char'])\n",
    "cluster_df['Movie'] = cluster_df['Character data'].apply(lambda x: x['movie'])\n",
    "cluster_df['Freebase character/map ID'] = cluster_df['Character data'].apply(lambda x: x['id'])\n",
    "cluster_df['Actor'] = cluster_df['Character data'].apply(lambda x: x['actor'])\n",
    "cluster_df.drop('Character data', axis=1, inplace=True)\n",
    "cluster_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now join the TV tropes clusters with movie.metadata so we are able to access movie genre and filter on romance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fix this\n",
    "cluster_char = cluster_df.merge(char_df, on='Freebase character/map ID')\n",
    "cluster_char_movie = cluster_char.merge(movie_df, on='Freebase ID')\n",
    "romance_cluster = cluster_char_movie[cluster_char_movie['Genres'].apply(lambda x: 'Roman' in x)]\n",
    "romance_cluster.groupby(romance_cluster['Cluster']).size().sort_values(ascending=False)\n",
    "romance_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.1. Analysing romantic genres\n",
    "\n",
    "One notices that there are several types of romantic movies: romantic comedy, romance film, romantic drama. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "romance_genres = ['Romantic comedy', 'Romance Film', 'Romantic drama', 'Romantic fantasy', 'Romantic thriller']\n",
    "is_romantic = lambda i: lambda x: any(y in romance_genres[i] for y in x) if type(x) == list else False\n",
    "romance_movies = movie_df[movie_df['Genres'].apply(is_romantic(slice(0, 5)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Organize by category\n",
    "romantic_comedy = romance_movies.loc[movie_df['Genres'].apply(is_romantic(0))]\n",
    "romantic_film = romance_movies.loc[movie_df['Genres'].apply(is_romantic(1))]\n",
    "romantic_drama = romance_movies.loc[movie_df['Genres'].apply(is_romantic(2))]\n",
    "romantic_fantasy = romance_movies.loc[movie_df['Genres'].apply(is_romantic(3))]\n",
    "romantic_thriller = romance_movies.loc[movie_df['Genres'].apply(is_romantic(4))]\n",
    "\n",
    "print('Roman' , romance_movies.shape[0])\n",
    "print('Romantic comedies: ', romantic_comedy.shape[0], '\\nRomantic films: ', romantic_film.shape[0], '\\nRomantic drama: ', romantic_drama.shape[0], '\\nRomantic fantasy: ', romantic_fantasy.shape[0], '\\nRomantic thriller: ', romantic_thriller.shape[0])\n",
    "print('Total number of films: ', movie_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.2. Romantic movies runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Should correct outliers\n",
    "#combined_runtime = pd.DataFrame({'Romantic comedy': romantic_comedy['Runtime'], 'Romance Film': romantic_film['Runtime'], 'Romantic drama': romantic_drama['Runtime'], 'Romantic fantasy': romantic_fantasy['Runtime']})\n",
    "#sns.boxplot(combined_runtime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.kdeplot(romantic_comedy['Runtime'], color='blue')\n",
    "ax = sns.kdeplot(romantic_drama['Runtime'], color='green')\n",
    "ax = sns.kdeplot(romantic_film['Runtime'], color='red')\n",
    "ax = sns.kdeplot(romantic_fantasy['Runtime'], color='orange')\n",
    "ax.set_xlim(0,250)\n",
    "ax.legend(['Romantic comedy', 'Romantic drama', 'Romance Film', 'Romantic fantasy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.3. Romantic movies box office revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Does not give a good view\n",
    "#combined_box_office = pd.DataFrame({'Romantic comedy': romantic_comedy['Box office revenue'], 'Romance Film': romantic_film['Box office revenue'], 'Romantic drama': romantic_drama['Box office revenue'], 'Romantic fantasy': romantic_fantasy['Box office revenue']})\n",
    "#sns.boxplot(combined_box_office)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = sns.kdeplot(romantic_comedy['Box office revenue'], log_scale=True, color='blue')\n",
    "ax = sns.kdeplot(romantic_drama['Box office revenue'], log_scale=True, color='green')\n",
    "ax = sns.kdeplot(romantic_film['Box office revenue'], log_scale=True, color='red')\n",
    "ax = sns.kdeplot(romantic_fantasy['Box office revenue'], log_scale=True, color='orange')\n",
    "ax.legend(['Romantic comedy', 'Romantic drama', 'Romance Film', 'Romantic fantasy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.4. Romantic movies countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "romantic_comedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_countries = lambda x: len(x) if type(x) == list else x\n",
    "romantic_comedy['number_countries'] = romantic_comedy['Countries'].apply(get_countries)\n",
    "romantic_fantasy['number_countries'] = romantic_fantasy['Countries'].apply(get_countries)\n",
    "romantic_film['number_countries'] = romantic_film['Countries'].apply(get_countries)\n",
    "romantic_drama['number_countries'] = romantic_drama['Countries'].apply(get_countries)\n",
    "\n",
    "combined_numb_countries = pd.DataFrame({\n",
    "    'Romantic comedy': romantic_comedy['number_countries'], \n",
    "    'Romance Film': romantic_film['number_countries'], \n",
    "    'Romantic drama': romantic_drama['number_countries'], \n",
    "    'Romantic fantasy': romantic_fantasy['number_countries']})\n",
    "\n",
    "print('Percentage romantic comedy movie countries > 1: ', round(romantic_comedy[romantic_comedy['number_countries']> 1].shape[0]/romantic_comedy.shape[0], 2), '%')\n",
    "print('Other countries can be added in code...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.5. Movie languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get languages whole movie set\n",
    "movies_language = movie_df[movie_df['Languages'].notnull()]\n",
    "languages=movies_language['Languages'].sum()\n",
    "values, counts = np.unique(languages, return_counts=True)\n",
    "print('5 most common languages in movies are: ')\n",
    "print(values[counts.argsort()[-5:][::-1]])\n",
    "\n",
    "#Get languages romantic movies overall\n",
    "romance_movies_lang = romance_movies[romance_movies['Languages'].notnull()]\n",
    "languages_romance = romance_movies_lang.Languages.sum()\n",
    "values, counts = np.unique(languages_romance, return_counts=True)\n",
    "print('5 most common languages in romantic movies: ')\n",
    "print(values[counts.argsort()[-5:][::-1]])\n",
    "\n",
    "rom_com_known = romantic_comedy[romantic_comedy['Languages'].notnull()]\n",
    "languages_romcom = rom_com_known.Languages.sum()\n",
    "values, counts = np.unique(languages_romcom, return_counts=True)\n",
    "print('\\n5 most common languages in romantic comedies: ')\n",
    "print(values[counts.argsort()[-5:][::-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifnIh1uE39dU"
   },
   "source": [
    "### 3.5. CoreNLP Plot summaries\n",
    "\n",
    "We would like to identify the two main characters in a plot summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gfYMmpwSGxMD",
    "outputId": "c45a3a93-e2ae-4785-f362-814836b0be1a"
   },
   "outputs": [],
   "source": [
    "#TODO: only consider romantic movies\n",
    "#TODO: Aggregate consecutive entity names inton one\n",
    "import xml.etree.ElementTree as ET\n",
    "extracted_dir = 'Data/CoreNLP/corenlp_plot_summaries_xml'\n",
    "\n",
    "def make_pairs (extracted_dir): \n",
    "  pairs = []\n",
    "  for filename in os.listdir(extracted_dir):\n",
    "      f = os.path.join(extracted_dir, filename) \n",
    "      if os.path.isfile(f):\n",
    "          # Create characters list for each file \n",
    "          characters = []\n",
    "          tree = ET.parse(f)\n",
    "          root = tree.getroot()\n",
    "          for child in tree.iter():\n",
    "              if child.tag == \"word\":\n",
    "                current_word = child.text\n",
    "              if child.tag == \"NER\": \n",
    "                if child.text == \"PERSON\":\n",
    "                  characters.append(current_word)\n",
    "          # Select the two characters which appear most often in the file\n",
    "          values, counts = np.unique(characters, return_counts=True)\n",
    "          two_most_frequent_characters = values[counts.argsort()[-2:][::-1]]\n",
    "          # If there exists two characters, create a pair \n",
    "          if len(two_most_frequent_characters) > 1:\n",
    "            pairs.append([f.replace('Data/CoreNLP/corenlp_plot_summaries_xml/', '').replace('.xml', ''), two_most_frequent_characters[0], two_most_frequent_characters[1]])\n",
    "  return pairs\n",
    "\n",
    "# Convert into an array and as a dataframe\n",
    "pairs = make_pairs(extracted_dir)\n",
    "pairs = np.asarray(pairs).reshape(-1, 3)  \n",
    "pairs_df = pd.DataFrame(pairs, columns=['Wikipedia ID', 'char1', 'char2']) \n",
    "pairs_df                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge pairs dataset with characters \n",
    "char_df['Wikipedia ID'] = char_df['Wikipedia ID'].astype(str)\n",
    "pairs_df['Wikipedia ID'] = pairs_df['Wikipedia ID'].astype(str)\n",
    "pairs_char = pairs_df.merge(char_df, on=\"Wikipedia ID\")\n",
    "\n",
    "# Filter out the nan values\n",
    "pairs_char = pairs_char[~pairs_char['Character name'].isna()]\n",
    "\n",
    "# Create columns which indicates if char1 and char2 are in character name \n",
    "pairs_char['char1_is_in_name'] = pairs_char.apply(lambda x: 1 if x['char1'] in x['Character name'] else 0, axis=1)\n",
    "pairs_char['char2_is_in_name'] = pairs_char.apply(lambda x: 1 if x['char2'] in x['Character name'] else 0, axis=1)\n",
    "pairs_char[pairs_char['char1_is_in_name'] == 1 & pairs_char['char2_is_in_name'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any character, we want to extract related information (from name clusters, character metadata) as well as actions, characteristics and relations (from CoreNLP). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def characterize(char_name):\n",
    "    characteristics = dict()\n",
    "    characteristics['Movie ID'] = list(char_df[char_df['Character name'] == 'Harry Potter'].index)\n",
    "    characteristics['Freebase character/actor ID'] = names_df.loc[char_name].values[0]\n",
    "    characteristics['Trope'] = cluster_df.loc[cluster_df['Character name'] == char_name]\n",
    "    if characteristics['Trope'].empty:\n",
    "        characteristics['Trope'] = None \n",
    "    return characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_name = 'Harry Potter'\n",
    "hp = characterize(char_name)\n",
    "print(hp)\n",
    "\n",
    "movie_id = hp['Movie ID'][0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now would like to extract love relationships between characters from plot summaries. We will use extract this information from the output coreNLP files at our disposition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To print xml files as a pretty tree\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def print_xml_tree(parsed_string):\n",
    "    tree = Tree.fromstring(parsed_string)\n",
    "    tree.pretty_print()\n",
    "\n",
    "parsed_str = \"(ROOT (S (NP (NP (NNP Harry) (POS 's)) (NN copy)) (VP (VBZ is) (VP (VBN inscribed) (PP (IN on) (NP (DT the) (NN fly) (NN page))) (PP (IN as) (NP (DT the) (`` ``) (NNP Half-Blood) (NNP Prince))))) (. .) ('' '')))\"\n",
    "print_xml_tree(parsed_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given an xml file, we return all of its parsed sentences \n",
    "def get_parsed_sentences(filename):\n",
    "    sentences = []\n",
    "    tree = ET.parse(filename)\n",
    "    root = tree.getroot()\n",
    "    for child in tree.iter():\n",
    "        if child.tag == \"parse\":\n",
    "            sentences.append(child.text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a character in a movie, find all sentences mentioning the character\n",
    "def characterize_by_movie(movie_id, char_name):\n",
    "    char_sentences = []\n",
    "    extracted_dir = 'Data/CoreNLP/corenlp_plot_summaries_xml'\n",
    "    xml_filename = os.path.join(extracted_dir, str(movie_id) + '.xml')\n",
    "    if os.path.isfile(xml_filename):\n",
    "        sentences = get_parsed_sentences(xml_filename)\n",
    "        char_sentences = [sentence for sentence in sentences if char_name in sentence]\n",
    "    return char_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = characterize_by_movie(movie_id, 'Harry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the powerful CoreNLP model, first [download it](https://stanfordnlp.github.io/CoreNLP/download.html), then cd into the downloaded `stanford-corenlp` directory. If you have Java, you can run the following command to open a CoreNLP shell: \n",
    "\n",
    "\n",
    "`java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer` \n",
    " \n",
    "Now that the shell is running, we can use their models to annotate some sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "nlp.annotate('Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. An Analysis of Romance\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
