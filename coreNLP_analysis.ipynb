{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Data Analysis Project\n",
    "**Team**: ToeStewBrr - Alexander Sternfeld, Marguerite Thery, Antoine Bonnet, Hugo Bordereaux\n",
    "\n",
    "**Dataset**: CMU Movie Summary Corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoreNLP Analysis\n",
    "\n",
    "[**CoreNLP**](https://nlp.stanford.edu/software/) is an incredible natural language processing toolkit created at Stanford University. CoreNLP is applied through a **pipeline** of sequential analysis steps called annotators. The full list of available annotators is available [here](https://stanfordnlp.github.io/CoreNLP/annotators.html). \n",
    "\n",
    "As described by its creators: \n",
    "\n",
    "*\"CoreNLP is your one stop shop for natural language processing in Java! CoreNLP enables users to derive linguistic annotations for text, including token and sentence boundaries, parts of speech, named entities, numeric and time values, dependency and constituency parses, coreference, sentiment, quote attributions, and relations. CoreNLP currently supports 8 languages: Arabic, Chinese, English, French, German, Hungarian, Italian, and Spanish.\"* \n",
    "\n",
    "You can create your own pipeline to extract the desired information. You can try it out for yourself in this [online shell](https://corenlp.run).\n",
    "\n",
    "### Loading data\n",
    "We first load data files and download the pre-processed dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from load_data import *\n",
    "from coreNLP_analysis import *\n",
    "from extraction import *\n",
    "\n",
    "download_data(coreNLP=False)\n",
    "plot_df = load_plot_df()\n",
    "movie_df = load_movie_df()\n",
    "char_df = load_char_df()\n",
    "names_df = load_names_df()\n",
    "cluster_df = load_cluster_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Exploring pre-processed CoreNLP data\n",
    "\n",
    "The authors of the Movie CMU dataset used CoreNLP to parse each plot summary to extract various linguistic insights. In this section, we explore how much information we can gather from these pre-processed files. \n",
    "\n",
    "We will use *Harry Potter*'s character throughout this section.\n",
    "\n",
    "#### 1.1. Character data\n",
    "\n",
    "For any character, we first extract related information from the provided name clusters and character metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given character, extract all pre-processed dataframe data\n",
    "char_name = 'Harry Potter'\n",
    "movie_ids = list(char_df[char_df['Character name'] == char_name]['Wikipedia ID'])\n",
    "\n",
    "print('Movies with character', char_name, ':')\n",
    "print('\\tMovie IDs:', movie_ids)\n",
    "\n",
    "movie_id = movie_ids[3]\n",
    "movie_name = movie_df.loc[movie_df['Wikipedia ID'] == movie_id]['Name'].iloc[0]\n",
    "\n",
    "print('Selecting as example: \\n\\tMovie ID:', movie_id, '\\n\\tMovie title:', movie_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Extracting sentences\n",
    "\n",
    "We now extract information from the CoreNLP plot summary analysis. The authors of the dataset stored the analysis output of each movie into a `.xml` file. Each file has a tree structure detailing each word of each sentence as well as the parsed sentence in tree form. \n",
    "\n",
    "We now extract all parsed sentences from the `.xml` files. \n",
    "\n",
    "A **parsed sentence** is a syntactic analysis tree, where each word is a leaf tagged by its lexical function (e.g. *VBZ* for verbs or *DT* for determinants). Semantic interactions between different words are also indicated within the structure of the tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the tree of xml file and all parsed sentences\n",
    "tree = get_tree(movie_id)\n",
    "sentences = get_parsed_sentences(tree)\n",
    "\n",
    "# Picking the fifth sentence as example\n",
    "parsed_str = sentences[5]\n",
    "print(parsed_str)\n",
    "print_tree(parsed_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Extracting characters\n",
    "\n",
    "We also want to extract all character names directly from the xml file. Note that we aggregate consecutive words tagged as NNP (noun, proper, singular) as the same character name (this assumes that plot summaries never contain two distinct names side by side without delimiting punctuation). This is a reasonable assumption since list of names are almost always separated by commas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = get_characters(tree)\n",
    "print(characters[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that some characters are sometimes mentioned by their full name, and sometimes by a partial name (e.g. Harry Potter is most often mentioned as simply Harry). To get a more precise idea of how many times each character is mentioned, we wish to denote each character by their full name, i.e. the longest version of their name that appears in the plot summary. \n",
    "\n",
    "*NOTE*: The dataset has the character metadata of only a third of the movies, so we need to extract full names from the plot summary itself and not the provided dataframes. \n",
    "\n",
    "To optimize full name lookup, for each plot summary we construct a dictionary which stores as key every partial name mentioned, and as corresponding values the full name of each character.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_name = 'Albus'\n",
    "full_name = get_full_name(char_name, characters)\n",
    "print('Example: the full name of \"{}\" is \"{}\".'.format(char_name,full_name))\n",
    "print('Full name dictionary:', full_name_dict(characters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now extract the most mentioned characters in any plot summary, in descending order of frequency. We can then see that Harry Potter is indeed the main character of the movie, as he is mentioned 26 times, more than any other character in the summary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_mentions = most_mentioned(movie_id)\n",
    "print(char_mentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 1.4. Extracting interactions\n",
    "\n",
    "We are also interested in character interactions. We can use the number of common mentions of two characters in the same sentence as a proxy for the number of interactions. For any movie, we find the number of common mentions (i.e. interactions) for each pair of characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_pairs = character_pairs(movie_id, plot_df)\n",
    "print(char_pairs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_interaction = character_pairs(movie_id, plot_df)[0][0]\n",
    "print('Main interaction in the movie:', main_interaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5. Extracting characters and interactions of all movies\n",
    "\n",
    "We will now use the above code to obtain the main character and main interaction for every plot summary. \n",
    "\n",
    "*NOTE*: This code takes a while to run, so you can load the analysis from a pre-processed file instead.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If we've already run this code, we can load the dataframe from a file\n",
    "plot_char_filename = 'Data/MovieSummaries/plot_characters.csv'\n",
    "pairs_df = pd.read_csv(plot_char_filename, sep='\\t', index_col=0)\n",
    "pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otherwise: get main character and number of mentions for each movie and store it into a file (takes a while to run)\n",
    "if not os.path.exists(plot_char_filename):\n",
    "    pairs_df = plot_df.copy(deep=True)\n",
    "    pairs_df['Main character'] = pairs_df['Wikipedia ID'].apply(most_mentioned)\n",
    "    pairs_df['Number of mentions'] = pairs_df['Main character'].apply(lambda x: np.nan if x is None else x[0][1])\n",
    "    pairs_df['Main character'] = pairs_df['Main character'].apply(lambda x: np.nan if x is None else x[0][0])\n",
    "\n",
    "    # Get main pairs of characters for each movie and number of interactions \n",
    "    pairs_df['Main interaction'] = pairs_df['Wikipedia ID'].apply(lambda x: character_pairs(x, plot_df))\n",
    "    pairs_df['Number of interactions'] = pairs_df['Main interaction'].apply(lambda x: np.nan if x is None else x[0][1])\n",
    "    pairs_df['Main interaction'] = pairs_df['Main interaction'].apply(lambda x: np.nan if x is None else x[0][0])\n",
    "\n",
    "    # Store data into csv file\n",
    "    pairs_df.to_csv(plot_char_filename, sep='\\t')\n",
    "    pairs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, the coreNLP files provided with the datasets are useful to extract the characters mentioned. \n",
    "\n",
    " However, our goal is to extract love relationships as well as the persona of characters in love. Using common mentions as a proxy for love relationships is a vulgar approximation and so we must run our own NLP analysis on the plot summaries to extract useful information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Custom CoreNLP Analysis\n",
    "\n",
    "We now use a **custom CoreNLP pipeline** to analyze the plot summaries. For now, due to the weakness of our available computing power, we only analyze romantic comedy movies. \n",
    "\n",
    "\n",
    "#### 2.1. Data preparation\n",
    "\n",
    "We extract the romantic comedy plot summaries that we will pass through our pipeline and store them as `.txt` files to be able to run them through the new coreNLP pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For later use: romance_genres = ['Romantic comedy', 'Romance Film', 'Romantic drama', 'Romantic fantasy', 'Romantic thriller']\n",
    "\n",
    "# Get a dataframe with romantic movies and their corresponding plots\n",
    "romance_genres = ['Romantic comedy'] \n",
    "rom_com_plots = get_plots(romance_genres, movie_df, plot_df)\n",
    "#display(rom_com_plots)\n",
    "\n",
    "# Store each plot summary as .txt file\n",
    "for index, row in rom_com_plots.iterrows():\n",
    "    # If directory doesn't exist, create it\n",
    "    if not os.path.exists('Data/MovieSummaries/RomancePlots'):\n",
    "        os.makedirs('Data/MovieSummaries/RomancePlots')\n",
    "    with open(\"Data/MovieSummaries/RomancePlots/{}.txt\".format(row['Wikipedia ID']), 'w', encoding='utf8') as f:\n",
    "        if type(row['Summary']) == str:\n",
    "            f.write(row['Summary'])\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Custom CoreNLP pipeline\n",
    "\n",
    "Our custom pipeline consists of the following annotators: \n",
    "\n",
    "1. [Tokenization (tokenize)](https://stanfordnlp.github.io/CoreNLP/tokenize.html): Turns the whole text into tokens. \n",
    "\n",
    "2. [Parts Of Speech (POS)](https://stanfordnlp.github.io/CoreNLP/pos.html): Tags each token with part of speech labels (e.g. determinants, verbs and nouns). \n",
    "\n",
    "3. [Lemmatization (lemma)](https://stanfordnlp.github.io/CoreNLP/lemma.html): Reduces each word to its lemma (e.g. *was* becomes *be*). \n",
    "\n",
    "4. [Named Entity Recognition (NER)](https://stanfordnlp.github.io/CoreNLP/ner.html): Identifies named entities from the text, including characters, locations and organizations. \n",
    "\n",
    "5. [Constituency parsing (parse)](https://stanfordnlp.github.io/CoreNLP/parse.html): Performs a syntactic analysis of each sentence in the form of a tree. \n",
    "\n",
    "6. [Coreference resolution (coref)](https://stanfordnlp.github.io/CoreNLP/coref.html): Aggregates mentions of the same entities in a text (e.g. when 'Harry' and 'he' refer to the same person). \n",
    "\n",
    "7. [Dependency parsing (depparse)](https://stanfordnlp.github.io/CoreNLP/depparse.html): Syntactic dependency parser. \n",
    "\n",
    "8. [Natural Logic (natlog)](https://stanfordnlp.github.io/CoreNLP/natlog.html): Identifies quantifier scope and token polarity. Required as preliminary for OpenIE. \n",
    "\n",
    "9. [Open Information Extraction (OpenIE)](https://stanfordnlp.github.io/CoreNLP/openie.html): Identifies relation between words as triples *(subject, relation, object of relation)*. We use this to extract relationships between characters, as well as character traits. \n",
    "\n",
    "10. [Knowledge Base Population (KBP)](https://stanfordnlp.github.io/CoreNLP/kbp.html): Identifies meaningful relation triples. \n",
    "\n",
    "\n",
    "#### 2.2. Running our pipeline\n",
    "\n",
    "We now run our own CoreNLP analysis on the plot summaries. This allows us to extract love relationships from the plot summaries much more accurately.\n",
    "\n",
    "**Goal**: Run our custom CoreNLP pipeline. \n",
    "\n",
    "**Recommendation**: Be careful about memory storage (takes a lot of memory to run!)\n",
    "\n",
    "**Prerequisite**: [java](https://www.java.com). \n",
    "\n",
    "**Installation steps**:\n",
    "1. Download the CoreNLP toolkit [here](https://stanfordnlp.github.io/CoreNLP/download.html).\n",
    "\n",
    "2. Data preparation: Extract plot summaries for romantic comedies into `.txt` files. Create a filelist containing the name of all the files which need to be processed using the following command: \n",
    "\n",
    "        find RomancePlots/*.txt > filelist.txt\n",
    "\n",
    "3. Change directory (`cd`) into the downloaded `stanford-corenlp` directory. \n",
    "        \n",
    "4. Run the custom CoreNLP pipeline via your terminal using the following command:\n",
    "\n",
    "        java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,pos,lemma,ner,parse,coref,depparse,natlog,openie,kbp -coref.md.type RULE -filelist filelist.txt -outputDirectory RomancePlotsOutputs/ -outputFormat xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis outputs are now stored as `.xml` files in the `RomancePlotsOutputs` directory. We now unzip them. RomancePlotsOutputs has 1491 readable files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all the romance plots xml files\n",
    "with ZipFile('CoreNLP/RomanceOutputs.zip', 'r') as zipObj:\n",
    "   zipObj.extractall('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extracting information\n",
    "\n",
    "Now that we have run the coreNLP pipeline and that the analysis of each movie has been a stored into a .xml output file, we can extract the information from these files. \n",
    "\n",
    "We will first extract the attributes and actions related to entities in the plot summaries. We will extract verbs and attributes independently. \n",
    "Agent verb: character does the action\n",
    "Patient verb: character is the object of the action\n",
    "Attributes: character attributes\n",
    "\n",
    "**Dependency parsing extraction**\n",
    "| Relation | Description |  Type  |  Example |\n",
    "|---|---|---|---|\n",
    "| obl:agent | Agent | Agent verb | 'They were rescued by Dumbledore' -> obl:agent(rescued, Dumbledore) |\n",
    "| nsubj  | Nominal subject | Agent verb | 'Harry confronts Snape' -> nsubj(confronts, Harry) |\n",
    "| nsubj:pass | Passive nominal subject | Patient verb | 'Goyle casts a curse and is burned to death' -> nsubj:pass(burned, Goyle)|\n",
    "| nsubj:xsubj | Indirect nominal subject | Patient verb | 'Goyle casts a curse and is unable to control it' -> nsubj:xsubj(control, Goyle)|\n",
    "| obj |  Direct object | Patient verb | 'To protect Harry' -> obj(protect, Harry) |\n",
    "| appos | Appositional modifier | Attribute | 'Harry's mother, Lily' -> appos(mother, Lily) |\n",
    "| amod | Adjectival modifier | Attribute | 'After burrying Dobby' -> amod(Dobby, burrying) |\n",
    "| nmod:poss | Possessive nominal modifier | Attribute | 'Snape's memories' -> nmod:poss(memories, Snape) |\n",
    "| nmod:of | 'Of' nominal modifier | Attribute |'With the help of Griphook' -> nmod:of(help, Griphook) |\n",
    "\n",
    "We will also extract KBP outputs, which stores data including the main role, spouse, age and religion for each character if specified. \n",
    "\n",
    "**KBP Extraction**\n",
    "| Attributes | Relation name | \n",
    "|---|---|\n",
    "| Main role | per:title |\n",
    "| Marital relationship | per:spouse  |  \n",
    "| Age  | per:age | \n",
    "| Religion  | per:religion | \n",
    "\n",
    "[KBP documentation](https://stanfordnlp.github.io/CoreNLP/kbp.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now extract the description of each character in the Harry Potter movie, which is composed of all agent verbs, patient verbs and attributes present in the plot summary. We also extract the love relationships in there, if present. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_filename = f'Data/CoreNLP/PlotsOutputs/667372.xml'\n",
    "\n",
    "tree = ET.parse(example_filename)\n",
    "descriptions_df, relations_df = get_descriptions_relations(tree)\n",
    "descriptions_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now extract the character descriptions of all characters in each movie and store the results in a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_path = 'Data/CoreNLP/descriptions.csv'\n",
    "relations_path = 'Data/CoreNLP/relations.csv'\n",
    "\n",
    "if not os.path.exists(description_path) and not os.path.exists(relations_path):\n",
    "\n",
    "    # Extract descriptions and relations from all xml files\n",
    "    output_dir = 'Data/CoreNLP/PlotsOutputs'\n",
    "    descriptions, relations = extract_descriptions_relations(output_dir)\n",
    "\n",
    "    # Save descriptions and relations into csv files\n",
    "    descriptions.to_csv(description_path, sep='\\t')\n",
    "    relations.to_csv(relations_path, sep='\\t')\n",
    "\n",
    "# If we've already run the extraction, we can load the dataframe from a file\n",
    "else: \n",
    "    descriptions = pd.read_csv(description_path, sep='\\t', index_col=0)\n",
    "    relations = pd.read_csv(relations_path, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same thing for the romance movies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "romance_description_path = 'Data/CoreNLP/romance_descriptions.csv'\n",
    "romance_relations_path = 'Data/CoreNLP/romance_relations.csv'\n",
    "\n",
    "if not os.path.exists(romance_description_path) and not os.path.exists(romance_relations_path):\n",
    "\n",
    "    # Extract descriptions and relations from all romance xml files\n",
    "    romance_output_dir = 'Data/CoreNLP/RomancePlotsOutputs'\n",
    "\n",
    "    # Remove file '43849.xml' from the directory, as it is not a valid xml file\n",
    "    if os.path.exists(f'{romance_output_dir}/43849.xml'):\n",
    "        os.remove(f'{romance_output_dir}/43849.xml')\n",
    "    \n",
    "    romance_descriptions, romance_relations = extract_descriptions_relations(romance_output_dir, log_interval=1)\n",
    "\n",
    "    # Save descriptions and relations into csv files\n",
    "    romance_descriptions.to_csv(romance_description_path, sep='\\t')\n",
    "    romance_relations.to_csv(romance_relations_path, sep='\\t')\n",
    "\n",
    "# If we've already run the extraction, we can load the dataframe from a file\n",
    "else: \n",
    "    romance_descriptions = pd.read_csv(romance_description_path, sep='\\t', index_col=0)\n",
    "    romance_relations = pd.read_csv(romance_relations_path, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Extracting relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Once everyone run their parts. From google colab, run relations_df on all the zip file and concatenate the results into a single dataframe. Export to csv. \n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "for whoami in ['romance', 'alex', 'hugo', 'antoine', 'marg']: \n",
    "    path = path + whoami \n",
    "    rel = get_relations(path, ['per:spouse', 'per:title', 'per:age', 'per:religion'])\n",
    "rel.to_csv((whoami+'.csv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframes with the different relations\n",
    "title_df = get_per('title')\n",
    "\n",
    "title_df_grouped = title_df.groupby(['Wikipedia ID', 'Subject'])['Relation'].apply(', '.join).reset_index()\n",
    "title_df_grouped.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed the titles using spacy and nltk\n",
    "loading = True\n",
    "if loading:\n",
    "    nlp_spacy = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Create pandas series of unique Relation string values\n",
    "relations = title_df['Relation'].unique()\n",
    "\n",
    "# Create dataframe with titles and their embeddings using concatenate\n",
    "title_embeddings = pd.concat([pd.Series(relations), pd.Series(relations).apply(lambda x: nlp_spacy(x).vector)], axis=1)\n",
    "title_embeddings.columns = ['Title', 'Embedding']\n",
    "title_embeddings.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster the titles using kmeans\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Create a list of silhouette scores for different k values\n",
    "silhouette_scores = []\n",
    "for k in range(2, 30):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(title_embeddings['Embedding'].tolist())\n",
    "    silhouette_scores.append(silhouette_score(title_embeddings['Embedding'].tolist(), kmeans.labels_))\n",
    "\n",
    "# Plot the silhouette scores\n",
    "plt.plot(range(2, 30), silhouette_scores)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans with K = 24, only assign title to a cluster if the silhouette score is above 0.1\n",
    "kmeans = KMeans(n_clusters=24, random_state=0).fit(title_embeddings['Embedding'].tolist())\n",
    "title_embeddings['Cluster'] = kmeans.labels_\n",
    "title_embeddings = title_embeddings[title_embeddings['Cluster'].apply(lambda x: silhouette_score(title_embeddings['Embedding'].tolist(), kmeans.labels_) > 0.05)]\n",
    "title_embeddings.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing our analysis\n",
    "\n",
    "Now that we have extracted useful information about characters in our movie database, we now visualize the data to extract useful insights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the clusters in a 3d diagram\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Reduce the dimensionality of the embeddings to 3, store each coordinate in a column\n",
    "tsne = TSNE(n_components=3, random_state=0)\n",
    "title_embeddings['X'] = tsne.fit_transform(title_embeddings['Embedding'].tolist())[:,0]\n",
    "title_embeddings['Y'] = tsne.fit_transform(title_embeddings['Embedding'].tolist())[:,1]\n",
    "title_embeddings['Z'] = tsne.fit_transform(title_embeddings['Embedding'].tolist())[:,2]\n",
    "\n",
    "# Plot the clusters in a 3d diagram\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(title_embeddings['X'], title_embeddings['Y'], title_embeddings['Z'], c=title_embeddings['Cluster'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print 5 titles from each cluster\n",
    "for i in range(24):\n",
    "    print('Cluster {}:'.format(i))\n",
    "    print(title_embeddings[title_embeddings['Cluster'] == i]['Title'].sample(5).values)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 10 most common character roles \n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "sns.countplot(x='Relation', data=title_df, order=title_df.groupby(['Relation']).count().sort_values(by = 'Wikipedia ID', ascending=False).head(10).index, ax=ax)\n",
    "\n",
    "ax.set_title('Most common character role in romance movies')\n",
    "ax.set_ylabel('Number of characters')\n",
    "xlabels = ['{}'.format(x) for x in title_df.groupby(['Relation']).count().sort_values(by = 'Wikipedia ID', ascending=False).head(10).index]\n",
    "ax.set_xticklabels(xlabels)\n",
    "ax.set_xlabel('')\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('flare')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis love relations (per:love)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "love_df = get_per('spouse')\n",
    "print(\"Number of unique movies from which romantic relationships have been identified:\", len(love_df['Wikipedia ID'].unique()))\n",
    "love_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up dataframe love_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove self loving relationships\n",
    "love_df = love_df[love_df['Relation'] != love_df['Subject']]\n",
    "\n",
    "# Only keep relations where the relationship is both ways\n",
    "love_df = love_df[love_df['Relation'] < love_df['Subject']]\n",
    "love_df.head(20)\n",
    "\n",
    "love_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also notice some pronouns are identified as subjects which mislead the number of relationships in a movie. We want to obtain a dataframe where the subjects and objects of the relationship are characters in the movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dataframe containing the list of all the characters appearing in a movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_characters = []\n",
    "for id in love_df['Wikipedia ID'].unique():\n",
    "    tree = get_tree_romance(id)\n",
    "    characters = get_characters(tree)\n",
    "    movie_characters.append((id, characters))\n",
    "\n",
    "movie_characters_df = pd.DataFrame(movie_characters, columns=[\n",
    "                                   'Wikipedia ID', 'Characters'])\n",
    "\n",
    "love_cast_df = love_df.merge(movie_characters_df, on='Wikipedia ID')\n",
    "love_cast_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check if the subject and object are part of the characters' list of the movie and filter out relations which does not involve movie's characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out relations that are not between movie's characters\n",
    "love_cast_df = love_cast_df.copy(deep=True)\n",
    "love_cast_df['Subject in characters'] = love_cast_df.apply(lambda x: x['Subject'] in x['Characters'], axis=1)\n",
    "love_cast_df['Relation in characters'] = love_cast_df.apply(lambda x: x['Relation'] in x['Characters'], axis=1)\n",
    "love_cast_df = love_cast_df[love_cast_df['Subject in characters'] & love_cast_df['Relation in characters']]\n",
    "love_cast_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to analyze the distribution of the number of relations per movie. We expect to see a lot of movie with 2 love relationships. Indeed, since we look at romantic comedies, we assume there are two characters in love in the movie and that their love is reciprocal (Hally loves Sally and Sally loves Harry). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the number of relations per movie\n",
    "love_cast_df['Number of relations'] = love_cast_df.groupby(['Wikipedia ID'])['Relation'].transform('count')\n",
    "# Obtain a dataframe with the number of relations per movie\n",
    "relations_per_movie = love_cast_df[['Wikipedia ID', 'Number of relations']].drop_duplicates()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.countplot(x='Number of relations', data=relations_per_movie, ax=ax)\n",
    "ax.set_title('Number of love relations per movie')\n",
    "ax.set_ylabel('Number of movies')\n",
    "ax.set_xlabel('Number of directed love relations in the movie')\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('flare')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we observe a high number of movies with 2 relationships. The number of movie with one relationship is quite high as well. We can interpret it as non-reciprocal love relations. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
