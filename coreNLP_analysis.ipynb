{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied Data Analysis Project\n",
    "\n",
    "**Team**: ToeStewBrr - Alexander Sternfeld, Marguerite Thery, Antoine Bonnet & Hugo Bordereaux.\n",
    "\n",
    "**Dataset**: CMU Movie Summary Corpus\n",
    "\n",
    "# Part 2: CoreNLP Analysis\n",
    "\n",
    "In this notebook, we use coreNLP to analyze the movie plots. \n",
    "\n",
    "[**CoreNLP**](https://nlp.stanford.edu/software/) is an incredible natural language processing toolkit created at Stanford University. CoreNLP is applied through a **pipeline** of sequential analysis steps called annotators. The full list of available annotators is available [here](https://stanfordnlp.github.io/CoreNLP/annotators.html). \n",
    "\n",
    "As described by its creators: \n",
    "\n",
    "*\"CoreNLP is your one stop shop for natural language processing in Java! CoreNLP enables users to derive linguistic annotations for text, including token and sentence boundaries, parts of speech, named entities, numeric and time values, dependency and constituency parses, coreference, sentiment, quote attributions, and relations. CoreNLP currently supports 8 languages: Arabic, Chinese, English, French, German, Hungarian, Italian, and Spanish.\"* \n",
    "\n",
    "You can create your own pipeline to extract the desired information. You can try it out for yourself in this [online shell](https://corenlp.run).\n",
    "\n",
    "### Table of contents\n",
    "1. [Loading data](#section1)\n",
    "2. [Exploring pre-processed CoreNLP data](#section2)\n",
    "    - 2.1. [Character metadata](#section2-1)\n",
    "    - 2.2. [Parsing sentences](#section2-2)\n",
    "    - 2.3. [Characters](#section2-3)\n",
    "    - 2.4. [Character interactions](#section2-4)\n",
    "    - 2.5. [Extracting characters and interactions](#section2-5)\n",
    "3. [Custom CoreNLP Analysis](#section3)\n",
    "    - 3.1. [Custom CoreNLP pipeline](#section3-1)\n",
    "    - 3.2. [Running our pipeline](#section3-2)\n",
    "4. [Extracting data](#section4)\n",
    "    - 4.1. [What to look for?](#section4-1)\n",
    "    - 4.2. [Running extraction](#section4-2)\n",
    "5. [Processing extracted data](#section5)\n",
    "    - 5.1. [Processing descriptions](#section5-1)\n",
    "    - 5.2. [Processing relations](#section5-2)\n",
    "\n",
    "\n",
    "## 1. Loading data <a class=\"anchor\" id=\"section1\"></a>\n",
    "We first load data files and download the pre-processed dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from ast import literal_eval\n",
    "\n",
    "from load_data import *\n",
    "from coreNLP_analysis import *\n",
    "from extraction import *\n",
    "\n",
    "download_data(coreNLP=False)\n",
    "plot_df = load_plot_df()\n",
    "movie_df = load_movie_df()\n",
    "char_df = load_char_df()\n",
    "names_df = load_names_df()\n",
    "cluster_df = load_cluster_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring pre-processed CoreNLP data <a class=\"anchor\" id=\"section2\"></a>\n",
    "\n",
    "The authors of the Movie CMU dataset used CoreNLP to parse each plot summary to extract various linguistic insights. In this section, we explore how much information we can gather from these pre-processed files. \n",
    "\n",
    "We will use *Harry Potter*'s character throughout this section.\n",
    "\n",
    "### 2.1. Character metadata <a class=\"anchor\" id=\"section2-1\"></a>\n",
    "\n",
    "For any character, we first extract related information from the provided name clusters and character metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies with character Harry Potter :\n",
      "\tMovie IDs: [858575, 667372, 670407, 31941988, 9834441, 667368, 667371, 667361, 667361]\n",
      "Selecting as example: \n",
      "\tMovie ID: 31941988 \n",
      "\tMovie title: Harry Potter and the Deathly Hallows – Part 2\n"
     ]
    }
   ],
   "source": [
    "# Given character, extract all pre-processed dataframe data\n",
    "char_name = 'Harry Potter'\n",
    "movie_ids = list(char_df[char_df['Character name'] == char_name]['Wikipedia ID'])\n",
    "\n",
    "print('Movies with character', char_name, ':')\n",
    "print('\\tMovie IDs:', movie_ids)\n",
    "\n",
    "movie_id = movie_ids[3]\n",
    "movie_name = movie_df.loc[movie_df['Wikipedia ID'] == movie_id]['Name'].iloc[0]\n",
    "\n",
    "print('Selecting as example: \\n\\tMovie ID:', movie_id, '\\n\\tMovie title:', movie_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Parsing sentences <a class=\"anchor\" id=\"section2-2\"></a>\n",
    "\n",
    "We now extract information from the CoreNLP plot summary analysis. The authors of the dataset stored the analysis output of each movie into a `.xml` file. Each file has a tree structure detailing each word of each sentence as well as the parsed sentence in tree form. \n",
    "\n",
    "We now extract all parsed sentences from the `.xml` files. \n",
    "\n",
    "A **parsed sentence** is a syntactic analysis tree, where each word is a leaf tagged by its lexical function (e.g. *VBZ* for verbs or *DT* for determinants). Semantic interactions between different words are also indicated within the structure of the tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT (S (PP (IN In) (NP (NP (NNP Bellatrix) (POS 's)) (NN vault))) (, ,) (NP (NNP Harry)) (VP (VBZ discovers) (SBAR (S (NP (DT the) (NNP Horcrux)) (VP (VBZ is) (NP (NP (NNP Helga) (NNP Hufflepuff) (POS 's)) (NN cup)))))) (. .))) \n",
      "                                                ROOT                                                 \n",
      "                                                 |                                                    \n",
      "                                                 S                                                   \n",
      "                _________________________________|_________________________________________________   \n",
      "               |             |    |                               VP                               | \n",
      "               |             |    |        _______________________|____                            |  \n",
      "               |             |    |       |                           SBAR                         | \n",
      "               |             |    |       |                            |                           |  \n",
      "               |             |    |       |                            S                           | \n",
      "               |             |    |       |            ________________|_______                    |  \n",
      "               PP            |    |       |           |                        VP                  | \n",
      "  _____________|___          |    |       |           |            ____________|_______            |  \n",
      " |                 NP        |    |       |           |           |                    NP          | \n",
      " |              ___|____     |    |       |           |           |             _______|_______    |  \n",
      " |             NP       |    |    NP      |           NP          |            NP              |   | \n",
      " |       ______|___     |    |    |       |       ____|_____      |     _______|___________    |   |  \n",
      " IN    NNP        POS   NN   ,   NNP     VBZ     DT        NNP   VBZ  NNP     NNP         POS  NN  . \n",
      " |      |          |    |    |    |       |      |          |     |    |       |           |   |   |  \n",
      " In Bellatrix      's vault  ,  Harry discovers the      Horcrux  is Helga Hufflepuff      's cup  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the tree of xml file and all parsed sentences\n",
    "tree = get_tree(movie_id)\n",
    "sentences = get_parsed_sentences(tree)\n",
    "\n",
    "# Picking the fifth sentence as example\n",
    "parsed_str = sentences[5]\n",
    "print(parsed_str)\n",
    "print_tree(parsed_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Characters <a class=\"anchor\" id=\"section2-3\"></a>\n",
    "\n",
    "We also want to extract all character names directly from the xml file. Note that we aggregate consecutive words tagged as NNP (noun, proper, singular) as the same character name (this assumes that plot summaries never contain two distinct names side by side without delimiting punctuation). This is a reasonable assumption since list of names are almost always separated by commas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Voldemort', 'Albus Dumbledore', 'Severus Snape', 'Dobby', 'Harry Potter', 'Ron', 'Hermione', 'Griphook', 'Harry', 'Ollivander', 'Ollivander', 'Draco Malfoy', 'Malfoy', 'Harry', 'Harry', 'Helga Hufflepuff', 'Griphook', 'Harry', 'Voldemort', 'Griphook']\n"
     ]
    }
   ],
   "source": [
    "characters = get_characters(tree)\n",
    "print(characters[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that some characters are sometimes mentioned by their full name, and sometimes by a partial name (e.g. Harry Potter is most often mentioned as simply Harry). To get a more precise idea of how many times each character is mentioned, we wish to denote each character by their full name, i.e. the longest version of their name that appears in the plot summary. \n",
    "\n",
    "*NOTE*: The dataset has the character metadata of only a third of the movies, so we need to extract full names from the plot summary itself and not the provided dataframes. \n",
    "\n",
    "To optimize full name lookup, for each plot summary we construct a dictionary which stores as key every partial name mentioned, and as corresponding values the full name of each character.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: the full name of \"Albus\" is \"Albus Dumbledore\".\n",
      "Full name dictionary: {'Voldemort': 'Voldemort', 'Albus Dumbledore': 'Albus Dumbledore', 'Severus Snape': 'Severus Snape', 'Dobby': 'Dobby', 'Harry Potter': 'Harry Potter', 'Ron': 'Ron', 'Hermione': 'Hermione Weasley', 'Griphook': 'Griphook', 'Harry': 'Harry Potter', 'Ollivander': 'Ollivander', 'Draco Malfoy': 'Draco Malfoy', 'Malfoy': 'Draco Malfoy', 'Helga Hufflepuff': 'Helga Hufflepuff', 'Rowena Ravenclaw': 'Rowena Ravenclaw', 'Hogsmeade': 'Hogsmeade', 'Aberforth Dumbledore': 'Aberforth Dumbledore', 'Ariana': 'Ariana', 'Neville Longbottom': 'Neville Longbottom', 'Snape': 'Severus Snape', 'Minerva McGonagall': 'Minerva McGonagall', 'Luna Lovegood': 'Luna Lovegood', 'Helena Ravenclaw': 'Helena Ravenclaw', 'Gregory Goyle': 'Gregory Goyle', 'Blaise Zabini': 'Blaise Zabini', 'Nagini': 'Nagini', 'Fred': 'Fred', 'Lily': 'Lily', 'James': 'James', 'Dumbledore': 'Albus Dumbledore', 'Neville': 'Neville Longbottom', 'Molly Weasley': 'Molly Weasley', 'Ginny Potter': 'Ginny Potter', 'Hermione Weasley': 'Hermione Weasley'}\n"
     ]
    }
   ],
   "source": [
    "char_name = 'Albus'\n",
    "full_name = get_full_name(char_name, characters)\n",
    "print('Example: the full name of \"{}\" is \"{}\".'.format(char_name,full_name))\n",
    "print('Full name dictionary:', full_name_dict(characters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now extract the most mentioned characters in any plot summary, in descending order of frequency. We can then see that Harry Potter is indeed the main character of the movie, as he is mentioned 26 times, more than any other character in the summary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Harry Potter', 26), ('Voldemort', 21), ('Severus Snape', 11), ('Ron', 6), ('Hermione Weasley', 6), ('Albus Dumbledore', 5), ('Griphook', 3), ('Draco Malfoy', 3), ('Neville Longbottom', 3), ('Nagini', 3), ('Ollivander', 2), ('Lily', 2), ('Dobby', 1), ('Helga Hufflepuff', 1), ('Rowena Ravenclaw', 1), ('Hogsmeade', 1), ('Aberforth Dumbledore', 1), ('Ariana', 1), ('Minerva McGonagall', 1), ('Luna Lovegood', 1), ('Helena Ravenclaw', 1), ('Gregory Goyle', 1), ('Blaise Zabini', 1), ('Fred', 1), ('James', 1), ('Molly Weasley', 1), ('Ginny Potter', 1)]\n"
     ]
    }
   ],
   "source": [
    "char_mentions = most_mentioned(movie_id)\n",
    "print(char_mentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.4. Character interactions <a class=\"anchor\" id=\"section2-4\"></a>\n",
    "\n",
    "We are also interested in character interactions. We can use the number of common mentions of two characters in the same sentence as a proxy for the number of interactions. For any movie, we find the number of common mentions (i.e. interactions) for each pair of characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m char_pairs \u001b[39m=\u001b[39m character_pairs(movie_id, plot_df)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(char_pairs[:\u001b[39m10\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/ADA/ada-2022-project-toestewbrr/coreNLP_analysis.py:197\u001b[0m, in \u001b[0;36mcharacter_pairs\u001b[0;34m(movie_id, plot_df)\u001b[0m\n\u001b[1;32m    194\u001b[0m full_name_map \u001b[39m=\u001b[39m full_name_dict(characters) \u001b[39m# all maps (partial name : full name)\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# Split plot summary into sentences\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m plot_summary \u001b[39m=\u001b[39m plot_df\u001b[39m.\u001b[39;49mloc[plot_df[\u001b[39m'\u001b[39;49m\u001b[39mWikipedia ID\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39;49m movie_id][\u001b[39m'\u001b[39;49m\u001b[39mSummary\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues[\u001b[39m0\u001b[39;49m]\n\u001b[1;32m    198\u001b[0m sentences \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msplit(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(?<=[.!?])\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms+\u001b[39m\u001b[39m'\u001b[39m, plot_summary)\n\u001b[1;32m    200\u001b[0m \u001b[39m# For each sentence, find the characters mentioned and their full names, then add count for each pair between them\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "char_pairs = character_pairs(movie_id, plot_df)\n",
    "print(char_pairs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_interaction = character_pairs(movie_id, plot_df)[0][0]\n",
    "print('Main interaction in the movie:', main_interaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Extracting characters and interactions <a class=\"anchor\" id=\"section2-5\"></a>\n",
    "\n",
    "We will now use the above code to obtain the main character and main interaction for every plot summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_char_filename = 'Data/MovieSummaries/plot_characters.csv'\n",
    "\n",
    "# NOTE: If we've already run this code, we can load the dataframe from a file\n",
    "if os.path.exists(plot_char_filename):\n",
    "    pairs_df = pd.read_csv(plot_char_filename, sep='\\t', index_col=0)\n",
    "    pairs_df\n",
    "\n",
    "# Otherwise: get main character and number of mentions for each movie and store it into a file (takes a while to run)\n",
    "else: \n",
    "    pairs_df = plot_df.copy(deep=True)\n",
    "    pairs_df['Main character'] = pairs_df['Wikipedia ID'].apply(most_mentioned)\n",
    "    pairs_df['Number of mentions'] = pairs_df['Main character'].apply(lambda x: np.nan if x is None else x[0][1])\n",
    "    pairs_df['Main character'] = pairs_df['Main character'].apply(lambda x: np.nan if x is None else x[0][0])\n",
    "\n",
    "    # Get main pairs of characters for each movie and number of interactions \n",
    "    pairs_df['Main interaction'] = pairs_df['Wikipedia ID'].apply(lambda x: character_pairs(x, plot_df))\n",
    "    pairs_df['Number of interactions'] = pairs_df['Main interaction'].apply(lambda x: np.nan if x is None else x[0][1])\n",
    "    pairs_df['Main interaction'] = pairs_df['Main interaction'].apply(lambda x: np.nan if x is None else x[0][0])\n",
    "\n",
    "    # Store data into csv file\n",
    "    pairs_df.to_csv(plot_char_filename, sep='\\t')\n",
    "    pairs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, the coreNLP files provided with the datasets are useful to extract the characters mentioned. \n",
    "\n",
    " However, our goal is to extract love relationships as well as the persona of characters in love. Using common mentions as a proxy for love relationships is a vulgar approximation and so we must run our own NLP analysis on the plot summaries to extract useful information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom CoreNLP Analysis <a class=\"anchor\" id=\"section3\"></a>\n",
    "\n",
    "We now construct a **custom CoreNLP pipeline** to analyze the plot summaries. \n",
    "\n",
    "### 3.1. Custom CoreNLP pipeline <a class=\"anchor\" id=\"section3-1\"></a>\n",
    "\n",
    "Our custom pipeline consists of the following annotators: \n",
    "\n",
    "1. [Tokenization (tokenize)](https://stanfordnlp.github.io/CoreNLP/tokenize.html): Turns the whole text into tokens. \n",
    "\n",
    "2. [Parts Of Speech (POS)](https://stanfordnlp.github.io/CoreNLP/pos.html): Tags each token with part of speech labels (e.g. determinants, verbs and nouns). \n",
    "\n",
    "3. [Lemmatization (lemma)](https://stanfordnlp.github.io/CoreNLP/lemma.html): Reduces each word to its lemma (e.g. *was* becomes *be*). \n",
    "\n",
    "4. [Named Entity Recognition (NER)](https://stanfordnlp.github.io/CoreNLP/ner.html): Identifies named entities from the text, including characters, locations and organizations. \n",
    "\n",
    "5. [Constituency parsing (parse)](https://stanfordnlp.github.io/CoreNLP/parse.html): Performs a syntactic analysis of each sentence in the form of a tree. \n",
    "\n",
    "6. [Coreference resolution (coref)](https://stanfordnlp.github.io/CoreNLP/coref.html): Aggregates mentions of the same entities in a text (e.g. when 'Harry' and 'he' refer to the same person). \n",
    "\n",
    "7. [Dependency parsing (depparse)](https://stanfordnlp.github.io/CoreNLP/depparse.html): Syntactic dependency parser. \n",
    "\n",
    "8. [Natural Logic (natlog)](https://stanfordnlp.github.io/CoreNLP/natlog.html): Identifies quantifier scope and token polarity. Required as preliminary for OpenIE. \n",
    "\n",
    "9. [Open Information Extraction (OpenIE)](https://stanfordnlp.github.io/CoreNLP/openie.html): Identifies relation between words as triples *(subject, relation, object of relation)*. We use this to extract relationships between characters, as well as character traits. \n",
    "\n",
    "10. [Knowledge Base Population (KBP)](https://stanfordnlp.github.io/CoreNLP/kbp.html): Identifies meaningful relation triples. \n",
    "\n",
    "\n",
    "### 3.2. Running our pipeline  <a class=\"anchor\" id=\"section3-2\"></a>\n",
    "\n",
    "We now run our own CoreNLP analysis on the plot summaries. This allows us to extract love relationships from the plot summaries much more accurately.\n",
    "\n",
    "**Goal**: Run our custom CoreNLP pipeline. \n",
    "\n",
    "**Recommendation**: Be careful about memory storage (takes a lot of memory to run!)\n",
    "\n",
    "**Prerequisite**: [java](https://www.java.com). \n",
    "\n",
    "**Installation steps**:\n",
    "1. Download the CoreNLP toolkit [here](https://stanfordnlp.github.io/CoreNLP/download.html).\n",
    "\n",
    "2. Data preparation: Extract plot summaries into `.txt` files in the `Plots` folder. Create a filelist containing the name of all the files which need to be processed using the following command: \n",
    "\n",
    "        find Plots/*.txt > filelist.txt\n",
    "\n",
    "3. Change directory (`cd`) into the downloaded `stanford-corenlp` directory. \n",
    "        \n",
    "4. Run the custom CoreNLP pipeline via your terminal using the following command:\n",
    "\n",
    "        java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,pos,lemma,ner,parse,coref,depparse,natlog,openie,kbp -coref.md.type RULE -filelist filelist.txt -outputDirectory PlotsOutputs/ -outputFormat xml\n",
    "\n",
    "## 4. Extracting data  <a class=\"anchor\" id=\"section4\"></a>\n",
    "\n",
    "Now that we have run the coreNLP pipeline and that the analysis of each movie has been a stored into a .xml output file, we can extract the information from these files. \n",
    "\n",
    "### 4.1. What to look for? <a class=\"anchor\" id=\"section4-1\"></a>\n",
    "\n",
    "We will first extract the attributes and actions related to entities in the plot summaries. We will extract verbs and attributes independently. \n",
    "- Agent verb: character does the action\n",
    "- Patient verb: character is the object of the action\n",
    "- Attributes: character attributes\n",
    "\n",
    "**Dependency parsing extraction**\n",
    "| Relation | Description |  Type  |  Example |\n",
    "|---|---|---|---|\n",
    "| obl:agent | Agent | Agent verb | 'They were rescued by Dumbledore' -> obl:agent(rescued, Dumbledore) |\n",
    "| nsubj  | Nominal subject | Agent verb | 'Harry confronts Snape' -> nsubj(confronts, Harry) |\n",
    "| nsubj:pass | Passive nominal subject | Patient verb | 'Goyle casts a curse and is burned to death' -> nsubj:pass(burned, Goyle)|\n",
    "| nsubj:xsubj | Indirect nominal subject | Patient verb | 'Goyle casts a curse and is unable to control it' -> nsubj:xsubj(control, Goyle)|\n",
    "| obj |  Direct object | Patient verb | 'To protect Harry' -> obj(protect, Harry) |\n",
    "| appos | Appositional modifier | Attribute | 'Harry's mother, Lily' -> appos(mother, Lily) |\n",
    "| amod | Adjectival modifier | Attribute | 'After burrying Dobby' -> amod(Dobby, burrying) |\n",
    "| nmod:poss | Possessive nominal modifier | Attribute | 'Snape's memories' -> nmod:poss(memories, Snape) |\n",
    "| nmod:of | 'Of' nominal modifier | Attribute |'With the help of Griphook' -> nmod:of(help, Griphook) |\n",
    "\n",
    "We will also extract KBP outputs, which stores data including the main role, spouse, age and religion for each character if specified. \n",
    "\n",
    "**KBP Extraction**\n",
    "| Attributes | Relation name | \n",
    "|---|---|\n",
    "| Main role | per:title |\n",
    "| Marital relationship | per:spouse  |  \n",
    "| Age  | per:age | \n",
    "| Religion  | per:religion | \n",
    "| Death | per:cause_of_death |\n",
    "\n",
    "The [KBP documentation](https://stanfordnlp.github.io/CoreNLP/kbp.html) contains a description of all available KBP tags.\n",
    "\n",
    "### 4.2. Running extraction <a class=\"anchor\" id=\"section4-2\"></a>\n",
    "\n",
    "We first extract data from the CoreNLP outputs of all movies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain descriptions and relationships\n",
    "descriptions, relations = load_descr_relations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Processing extracted data <a class=\"anchor\" id=\"section5\"></a>\n",
    "\n",
    "\n",
    "### 5.1. Processing descriptions <a class=\"anchor\" id=\"section5-1\"></a>\n",
    "\n",
    "We now pre-process the extracted character analysis, merge it with the pre-existing character and movie metadata and store it into a cute data file. \n",
    "\n",
    "1. Convert to lists, remove non-English words, remove stopwords, move all non-verbs outside of actions, convert to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lists of words\n",
    "descriptions['attributes'] = descriptions['attributes'].apply(lambda x: literal_eval(x) if type(x) == str else x)\n",
    "descriptions['agent_verbs'] = descriptions['agent_verbs'].apply(lambda x: literal_eval(x) if type(x) == str else x)\n",
    "descriptions['patient_verbs'] = descriptions['patient_verbs'].apply(lambda x: literal_eval(x) if type(x) == str else x)\n",
    "\n",
    "# For every word in actions, if the word is not a verb, move it to attributes\n",
    "for i, row in descriptions.iterrows():\n",
    "    # If agent_verbs or patient_verbs are NaN, skip\n",
    "    if type(row['agent_verbs']) == float or type(row['patient_verbs']) == float:\n",
    "        continue\n",
    "    for word in row['agent_verbs']:\n",
    "        if not wn.synsets(word, pos=wn.VERB):\n",
    "            descriptions.at[i, 'agent_verbs'].remove(word)\n",
    "            if type(descriptions.at[i, 'attributes']) == float:\n",
    "                descriptions.at[i, 'attributes'] = []\n",
    "            descriptions.at[i, 'attributes'].append(word)\n",
    "    for word in row['patient_verbs']:\n",
    "        if not wn.synsets(word, pos=wn.VERB):\n",
    "            descriptions.at[i, 'patient_verbs'].remove(word)\n",
    "            if type(descriptions.at[i, 'attributes']) == float:\n",
    "                descriptions.at[i, 'attributes'] = []\n",
    "            descriptions.at[i, 'attributes'].append(word)\n",
    "\n",
    "# Remove all words that are not recognized by WordNet, lowercase\n",
    "descriptions['attributes'] = descriptions['attributes'].apply(\n",
    "    lambda x: [word.lower() for word in x if wn.synsets(word)] if type(x) == list else x)\n",
    "descriptions['agent_verbs'] = descriptions['agent_verbs'].apply(\n",
    "    lambda x: [word.lower() for word in x if wn.synsets(word)] if type(x) == list else x)\n",
    "descriptions['patient_verbs'] = descriptions['patient_verbs'].apply(\n",
    "    lambda x: [word.lower() for word in x if wn.synsets(word)] if type(x) == list else x)\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Remove all stopwords that may have seeped in\n",
    "stop_words = set(stopwords.words('english'))\n",
    "descriptions['attributes'] = descriptions['attributes'].apply(\n",
    "    lambda x: [word for word in x if word not in stop_words] if type(x) == list else x)\n",
    "descriptions['agent_verbs'] = descriptions['agent_verbs'].apply(\n",
    "    lambda x: [word for word in x if word not in stop_words] if type(x) == list else x)\n",
    "descriptions['patient_verbs'] = descriptions['patient_verbs'].apply(\n",
    "    lambda x: [word for word in x if word not in stop_words] if type(x) == list else x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Lemmatize all words to their lexical root and verbs to their infinitive present tense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize all words\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_verb(x): return [lem.lemmatize((word), 'v') for word in x]\n",
    "def lemmatize_noun(x): return [lem.lemmatize(word) for word in x]\n",
    "\n",
    "descriptions['agent_verbs'] = descriptions['agent_verbs'].apply(\n",
    "    lambda x: lemmatize_verb(x) if type(x) == list else x)\n",
    "descriptions['patient_verbs'] = descriptions['patient_verbs'].apply(\n",
    "    lambda x: lemmatize_verb(x) if type(x) == list else x)\n",
    "descriptions['attributes'] = descriptions['attributes'].apply(\n",
    "    lambda x: lemmatize_noun(x) if type(x) == list else x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Aggregate descriptions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all descriptions\n",
    "descriptions['all_descriptions'] = descriptions[['agent_verbs', 'patient_verbs', 'attributes']].apply(\n",
    "    lambda x: [item for sublist in x if type(sublist) == list for item in sublist], axis=1)\n",
    "\n",
    "# Append title to descriptions\n",
    "descriptions['all_descriptions'] = descriptions.apply(\n",
    "    lambda x: [x['title']] + x['all_descriptions'] if not pd.isnull(x['title']) else x['all_descriptions'], axis=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Synchronize character names to the character metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each movie_id in descriptions, synchronize the names and store in name_sync_df\n",
    "unique_ids = descriptions['movie_id'].unique()\n",
    "\n",
    "name_sync_df = pd.DataFrame(columns=['movie_id', 'name_sync'])\n",
    "for i, movie_id in enumerate(unique_ids):\n",
    "    name_sync_df = pd.concat([name_sync_df, pd.DataFrame({\n",
    "        'movie_id': [movie_id], \n",
    "        'name_sync': [synchronize_name(movie_id, char_df, descriptions, col_name='character')]})], \n",
    "        ignore_index=True)\n",
    "    if i % 1000 == 0:\n",
    "        print('Extracted names for movie {} out of {} ({}%)'.format(i, len(unique_ids), round(i/len(unique_ids)*100, 2)))\n",
    "    \n",
    "# Index name_sync by movie_id\n",
    "name_sync_df = name_sync_df.set_index('movie_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each character in descriptions, get the corresponding name_sync\n",
    "descriptions['plot_name'] = descriptions['character']\n",
    "descriptions['character'] = descriptions[['movie_id', 'character']].apply(\n",
    "    lambda x: name_sync_df.loc[x['movie_id']].values[0][x['character']], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Merge with character metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all string movie_id to integers\n",
    "descriptions = descriptions[descriptions['movie_id'] != '34808485_delete']\n",
    "descriptions['movie_id'] = descriptions['movie_id'].apply(lambda x: int(x))\n",
    "char_df['Wikipedia ID'] = char_df['Wikipedia ID'].apply(lambda x: int(x))\n",
    "# Drop Nan wikipedia IDs\n",
    "char_df = char_df[char_df['Wikipedia ID'].notna()]\n",
    "\n",
    "\n",
    "# Merge descriptions with char_df on character name and movie_id\n",
    "descriptions = descriptions.merge(\n",
    "    char_df,\n",
    "    left_on=['character', 'movie_id'],\n",
    "    right_on=['Character name', 'Wikipedia ID'],\n",
    "    how='left')\n",
    "descriptions = descriptions[descriptions['Wikipedia ID'].notna()]\n",
    "\n",
    "# Drop columns movie_id, character\n",
    "descriptions = descriptions.drop(\n",
    "    ['movie_id', 'character'], axis=1)\n",
    "\n",
    "# Reorder columns\n",
    "cols = descriptions.columns.tolist()\n",
    "cols = [col for col in cols if col not in ['Wikipedia ID', 'Character name']]\n",
    "cols = ['Wikipedia ID', 'Character name'] + cols\n",
    "descriptions = descriptions[cols]\n",
    "\n",
    "descriptions['Wikipedia ID'] = descriptions['Wikipedia ID'].apply(\n",
    "    lambda x: int(x))\n",
    "\n",
    "# Save descriptions to csv\n",
    "descriptions.to_csv('Data/CoreNLP/full_descriptions.csv',\n",
    "                    sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Aggregate all descriptions over all movies for each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new DataFrame with a single character per row\n",
    "char_descriptions = pd.DataFrame(\n",
    "    descriptions[['Character name', 'Freebase character ID']].drop_duplicates())\n",
    "\n",
    "# Aggregate all agent_verbs together\n",
    "agent_verbs = descriptions.groupby(['Freebase character ID'])['agent_verbs'].aggregate(\n",
    "    lambda x: list(itertools.chain.from_iterable(x.dropna())))\n",
    "char_descriptions = char_descriptions.merge(\n",
    "    agent_verbs, left_on='Freebase character ID', right_index=True, how='left')\n",
    "\n",
    "# Aggregate all patient_verbs together\n",
    "patient_verbs = descriptions.groupby(['Freebase character ID'])['patient_verbs'].aggregate(\n",
    "    lambda x: list(itertools.chain.from_iterable(x.dropna())))\n",
    "char_descriptions = char_descriptions.merge(\n",
    "    patient_verbs, left_on='Freebase character ID', right_index=True, how='left')\n",
    "\n",
    "# Aggregate all attributes together\n",
    "attributes = descriptions.groupby(['Freebase character ID'])['attributes'].aggregate(\n",
    "    lambda x: list(itertools.chain.from_iterable(x.dropna())))\n",
    "char_descriptions = char_descriptions.merge(\n",
    "    attributes, left_on='Freebase character ID', right_index=True, how='left')\n",
    "\n",
    "# Aggregate all titles together into a list of titles\n",
    "titles = descriptions.groupby(['Freebase character ID'])['title'].aggregate(\n",
    "    lambda x: list(x.dropna()))\n",
    "char_descriptions = char_descriptions.merge(\n",
    "    titles, left_on='Freebase character ID', right_index=True, how='left')\n",
    "\n",
    "# Concatenate all agent_verbs, patient_verbs, attributes, titles into a single list of descriptions\n",
    "char_descriptions['descriptions'] = char_descriptions[['agent_verbs', 'patient_verbs',\n",
    "                                                       'attributes', 'title']].apply(lambda x: list(itertools.chain.from_iterable(x.dropna())), axis=1)\n",
    "\n",
    "# Replace all empty lists with NaN\n",
    "char_descriptions = char_descriptions.dropna(subset=['Character name', 'Freebase character ID']) \n",
    "char_descriptions['agent_verbs'] = char_descriptions['agent_verbs'].apply(lambda x: np.nan if (type(x) == list and len(x) == 0) else x).copy()\n",
    "char_descriptions['patient_verbs'] = char_descriptions['patient_verbs'].apply(lambda x: np.nan if (type(x) == list and len(x) == 0) else x).copy()\n",
    "char_descriptions['attributes'] = char_descriptions['attributes'].apply(lambda x: np.nan if (type(x) == list and len(x) == 0) else x).copy()\n",
    "char_descriptions['title'] = char_descriptions['title'].apply(lambda x: np.nan if (type(x) == list and len(x) == 0) else x).copy()\n",
    "char_descriptions['descriptions'] = char_descriptions['descriptions'].apply(lambda x: np.nan if (type(x) == list and len(x) == 0) else x).copy()\n",
    "\n",
    "\n",
    "# Save char_descriptions to csv\n",
    "char_descriptions.to_csv(\n",
    "    'Data/CoreNLP/char_descriptions.csv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Processing relations <a class=\"anchor\" id=\"section5-2\"></a>\n",
    "\n",
    "We synchronize the names of characters in relationships to their names in the character metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each movie_id in relations, synchronize the subject and object names and store in name_rel_df\n",
    "name_rel_df = pd.DataFrame(columns=['movie_id', 'subject_sync', 'object_sync'])\n",
    "for movie_id in relations['movie_id'].unique():\n",
    "    name_rel_df = pd.concat([\n",
    "        name_rel_df, \n",
    "        pd.DataFrame({\n",
    "            'movie_id': [movie_id], \n",
    "            'subject_sync': [synchronize_name(movie_id, char_df, df=relations, col_name='subject')], \n",
    "            'object_sync': [synchronize_name(movie_id, char_df, df=relations, col_name='object')]})], ignore_index=True)\n",
    "\n",
    "\n",
    "# Index name_sync by movie_id\n",
    "name_rel_df = name_rel_df.set_index('movie_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each character in relations, get the corresponding name_sync (except if you can't find the character in the given movie)\n",
    "relations['subject'] = relations[['movie_id', 'subject']].apply(\n",
    "    lambda x: name_rel_df.loc[x['movie_id']]['subject_sync'][x['subject']] if x['subject'] in name_rel_df.loc[x['movie_id']]['subject_sync'] else np.nan, axis=1)\n",
    "relations['object'] = relations[['movie_id', 'object']].apply(\n",
    "    lambda x: name_rel_df.loc[x['movie_id']]['object_sync'][x['object']] if x['object'] in name_rel_df.loc[x['movie_id']]['object_sync']else np.nan, axis=1)\n",
    "\n",
    "# Rename columns to Wikipedia ID, Subject, Object, Romance\n",
    "relations = relations.rename(columns={'movie_id': 'Wikipedia ID', 'subject': 'Subject', 'object': 'Object', 'romance': 'Romance'})\n",
    "\n",
    "# Add subject and object Freebase character IDs and rename them to Subject/object freebase character ID column\n",
    "relations = relations.merge(\n",
    "    char_df[['Wikipedia ID', 'Character name', 'Freebase character ID']], \n",
    "    left_on=['Subject', 'Wikipedia ID'], \n",
    "    right_on=['Character name', 'Wikipedia ID'], \n",
    "    how='left').rename(columns={'Freebase character ID': 'Subject freebase character ID'})\n",
    "relations = relations.merge(\n",
    "    char_df[['Wikipedia ID', 'Character name', 'Freebase character ID']], \n",
    "    left_on=['Object', 'Wikipedia ID'], \n",
    "    right_on=['Character name', 'Wikipedia ID'], \n",
    "    how='left').rename(columns={'Freebase character ID': 'Object freebase character ID'})\n",
    "\n",
    "# Drop columns character name_x and character name_y\n",
    "relations = relations.drop(columns=['Character name_x', 'Character name_y']).copy()\n",
    "\n",
    "# Save relations to csv\n",
    "relations.to_csv('Data/CoreNLP/char_relations.csv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
