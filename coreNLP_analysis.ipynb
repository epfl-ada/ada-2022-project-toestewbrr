{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Data Analysis Project\n",
    "**Team**: ToeStewBrr - Alexander Sternfeld, Marguerite Thery, Antoine Bonnet, Hugo Bordereaux\n",
    "\n",
    "**Dataset**: CMU Movie Summary Corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoreNLP Analysis\n",
    "\n",
    "[**CoreNLP**](https://nlp.stanford.edu/software/) is an incredible natural language processing toolkit created at Stanford University. CoreNLP is applied through a **pipeline** of sequential analysis steps called annotators. The full list of available annotators is available [here](https://stanfordnlp.github.io/CoreNLP/annotators.html). \n",
    "\n",
    "As described by its creators: \n",
    "\n",
    "*\"CoreNLP is your one stop shop for natural language processing in Java! CoreNLP enables users to derive linguistic annotations for text, including token and sentence boundaries, parts of speech, named entities, numeric and time values, dependency and constituency parses, coreference, sentiment, quote attributions, and relations. CoreNLP currently supports 8 languages: Arabic, Chinese, English, French, German, Hungarian, Italian, and Spanish.\"* \n",
    "\n",
    "You can create your own pipeline to extract the desired information. You can try it out for yourself in this [online shell](https://corenlp.run).\n",
    "\n",
    "### Loading data\n",
    "We first load data files and download the pre-processed dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from load_data import *\n",
    "from coreNLP_analysis import *\n",
    "from extraction import *\n",
    "\n",
    "download_data(coreNLP=False)\n",
    "plot_df = load_plot_df()\n",
    "movie_df = load_movie_df()\n",
    "char_df = load_char_df()\n",
    "names_df = load_names_df()\n",
    "cluster_df = load_cluster_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Exploring pre-processed CoreNLP data\n",
    "\n",
    "The authors of the Movie CMU dataset used CoreNLP to parse each plot summary to extract various linguistic insights. In this section, we explore how much information we can gather from these pre-processed files. \n",
    "\n",
    "We will use *Harry Potter*'s character throughout this section.\n",
    "\n",
    "#### 1.1. Character data\n",
    "\n",
    "For any character, we first extract related information from the provided name clusters and character metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies with character Harry Potter :\n",
      "\tMovie IDs: [858575, 667372, 670407, 31941988, 9834441, 667368, 667371, 667361, 667361]\n",
      "Selecting as example: \n",
      "\tMovie ID: 31941988 \n",
      "\tMovie title: Harry Potter and the Deathly Hallows – Part 2\n"
     ]
    }
   ],
   "source": [
    "# Given character, extract all pre-processed dataframe data\n",
    "char_name = 'Harry Potter'\n",
    "movie_ids = list(char_df[char_df['Character name'] == char_name]['Wikipedia ID'])\n",
    "\n",
    "print('Movies with character', char_name, ':')\n",
    "print('\\tMovie IDs:', movie_ids)\n",
    "\n",
    "movie_id = movie_ids[3]\n",
    "movie_name = movie_df.loc[movie_df['Wikipedia ID'] == movie_id]['Name'].iloc[0]\n",
    "\n",
    "print('Selecting as example: \\n\\tMovie ID:', movie_id, '\\n\\tMovie title:', movie_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Extracting sentences\n",
    "\n",
    "We now extract information from the CoreNLP plot summary analysis. The authors of the dataset stored the analysis output of each movie into a `.xml` file. Each file has a tree structure detailing each word of each sentence as well as the parsed sentence in tree form. \n",
    "\n",
    "We now extract all parsed sentences from the `.xml` files. \n",
    "\n",
    "A **parsed sentence** is a syntactic analysis tree, where each word is a leaf tagged by its lexical function (e.g. *VBZ* for verbs or *DT* for determinants). Semantic interactions between different words are also indicated within the structure of the tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT (S (PP (IN In) (NP (NP (NNP Bellatrix) (POS 's)) (NN vault))) (, ,) (NP (NNP Harry)) (VP (VBZ discovers) (SBAR (S (NP (DT the) (NNP Horcrux)) (VP (VBZ is) (NP (NP (NNP Helga) (NNP Hufflepuff) (POS 's)) (NN cup)))))) (. .))) \n",
      "                                                ROOT                                                 \n",
      "                                                 |                                                    \n",
      "                                                 S                                                   \n",
      "                _________________________________|_________________________________________________   \n",
      "               |             |    |                               VP                               | \n",
      "               |             |    |        _______________________|____                            |  \n",
      "               |             |    |       |                           SBAR                         | \n",
      "               |             |    |       |                            |                           |  \n",
      "               |             |    |       |                            S                           | \n",
      "               |             |    |       |            ________________|_______                    |  \n",
      "               PP            |    |       |           |                        VP                  | \n",
      "  _____________|___          |    |       |           |            ____________|_______            |  \n",
      " |                 NP        |    |       |           |           |                    NP          | \n",
      " |              ___|____     |    |       |           |           |             _______|_______    |  \n",
      " |             NP       |    |    NP      |           NP          |            NP              |   | \n",
      " |       ______|___     |    |    |       |       ____|_____      |     _______|___________    |   |  \n",
      " IN    NNP        POS   NN   ,   NNP     VBZ     DT        NNP   VBZ  NNP     NNP         POS  NN  . \n",
      " |      |          |    |    |    |       |      |          |     |    |       |           |   |   |  \n",
      " In Bellatrix      's vault  ,  Harry discovers the      Horcrux  is Helga Hufflepuff      's cup  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the tree of xml file and all parsed sentences\n",
    "tree = get_tree(movie_id)\n",
    "sentences = get_parsed_sentences(tree)\n",
    "\n",
    "# Picking the fifth sentence as example\n",
    "parsed_str = sentences[5]\n",
    "print(parsed_str)\n",
    "print_tree(parsed_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Extracting characters\n",
    "\n",
    "We also want to extract all character names directly from the xml file. Note that we aggregate consecutive words tagged as NNP (noun, proper, singular) as the same character name (this assumes that plot summaries never contain two distinct names side by side without delimiting punctuation). This is a reasonable assumption since list of names are almost always separated by commas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Voldemort', 'Albus Dumbledore', 'Severus Snape', 'Dobby', 'Harry Potter', 'Ron', 'Hermione', 'Griphook', 'Harry', 'Ollivander', 'Ollivander', 'Draco Malfoy', 'Malfoy', 'Harry', 'Harry', 'Helga Hufflepuff', 'Griphook', 'Harry', 'Voldemort', 'Griphook']\n"
     ]
    }
   ],
   "source": [
    "characters = get_characters(tree)\n",
    "print(characters[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that some characters are sometimes mentioned by their full name, and sometimes by a partial name (e.g. Harry Potter is most often mentioned as simply Harry). To get a more precise idea of how many times each character is mentioned, we wish to denote each character by their full name, i.e. the longest version of their name that appears in the plot summary. \n",
    "\n",
    "*NOTE*: The dataset has the character metadata of only a third of the movies, so we need to extract full names from the plot summary itself and not the provided dataframes. \n",
    "\n",
    "To optimize full name lookup, for each plot summary we construct a dictionary which stores as key every partial name mentioned, and as corresponding values the full name of each character.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: the full name of \"Albus\" is \"Albus Dumbledore\".\n",
      "Full name dictionary: {'Voldemort': 'Voldemort', 'Albus Dumbledore': 'Albus Dumbledore', 'Severus Snape': 'Severus Snape', 'Dobby': 'Dobby', 'Harry Potter': 'Harry Potter', 'Ron': 'Ron', 'Hermione': 'Hermione Weasley', 'Griphook': 'Griphook', 'Harry': 'Harry Potter', 'Ollivander': 'Ollivander', 'Draco Malfoy': 'Draco Malfoy', 'Malfoy': 'Draco Malfoy', 'Helga Hufflepuff': 'Helga Hufflepuff', 'Rowena Ravenclaw': 'Rowena Ravenclaw', 'Hogsmeade': 'Hogsmeade', 'Aberforth Dumbledore': 'Aberforth Dumbledore', 'Ariana': 'Ariana', 'Neville Longbottom': 'Neville Longbottom', 'Snape': 'Severus Snape', 'Minerva McGonagall': 'Minerva McGonagall', 'Luna Lovegood': 'Luna Lovegood', 'Helena Ravenclaw': 'Helena Ravenclaw', 'Gregory Goyle': 'Gregory Goyle', 'Blaise Zabini': 'Blaise Zabini', 'Nagini': 'Nagini', 'Fred': 'Fred', 'Lily': 'Lily', 'James': 'James', 'Dumbledore': 'Albus Dumbledore', 'Neville': 'Neville Longbottom', 'Molly Weasley': 'Molly Weasley', 'Ginny Potter': 'Ginny Potter', 'Hermione Weasley': 'Hermione Weasley'}\n"
     ]
    }
   ],
   "source": [
    "char_name = 'Albus'\n",
    "full_name = get_full_name(char_name, characters)\n",
    "print('Example: the full name of \"{}\" is \"{}\".'.format(char_name,full_name))\n",
    "print('Full name dictionary:', full_name_dict(characters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now extract the most mentioned characters in any plot summary, in descending order of frequency. We can then see that Harry Potter is indeed the main character of the movie, as he is mentioned 26 times, more than any other character in the summary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Harry Potter', 26), ('Voldemort', 21), ('Severus Snape', 11), ('Ron', 6), ('Hermione Weasley', 6), ('Albus Dumbledore', 5), ('Griphook', 3), ('Draco Malfoy', 3), ('Neville Longbottom', 3), ('Nagini', 3), ('Ollivander', 2), ('Lily', 2), ('Dobby', 1), ('Helga Hufflepuff', 1), ('Rowena Ravenclaw', 1), ('Hogsmeade', 1), ('Aberforth Dumbledore', 1), ('Ariana', 1), ('Minerva McGonagall', 1), ('Luna Lovegood', 1), ('Helena Ravenclaw', 1), ('Gregory Goyle', 1), ('Blaise Zabini', 1), ('Fred', 1), ('James', 1), ('Molly Weasley', 1), ('Ginny Potter', 1)]\n"
     ]
    }
   ],
   "source": [
    "char_mentions = most_mentioned(movie_id)\n",
    "print(char_mentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 1.4. Extracting interactions\n",
    "\n",
    "We are also interested in character interactions. We can use the number of common mentions of two characters in the same sentence as a proxy for the number of interactions. For any movie, we find the number of common mentions (i.e. interactions) for each pair of characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('Hermione Weasley', 'Ron'), 4), (('Harry Potter', 'Voldemort'), 4), (('Albus Dumbledore', 'Voldemort'), 3), (('Albus Dumbledore', 'Severus Snape'), 2), (('Harry Potter', 'Hermione Weasley'), 2), (('Harry Potter', 'Ron'), 2), (('Nagini', 'Voldemort'), 2), (('Harry Potter', 'Lily'), 2), (('Albus Dumbledore', 'Harry Potter'), 2), (('Severus Snape', 'Voldemort'), 1)]\n"
     ]
    }
   ],
   "source": [
    "char_pairs = character_pairs(movie_id, plot_df)\n",
    "print(char_pairs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main interaction in the movie: ('Hermione Weasley', 'Ron')\n"
     ]
    }
   ],
   "source": [
    "main_interaction = character_pairs(movie_id, plot_df)[0][0]\n",
    "print('Main interaction in the movie:', main_interaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5. Extracting characters and interactions of all movies\n",
    "\n",
    "We will now use the above code to obtain the main character and main interaction for every plot summary. \n",
    "\n",
    "*NOTE*: This code takes a while to run, so you can load the analysis from a pre-processed file instead.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wikipedia ID</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Main character</th>\n",
       "      <th>Number of mentions</th>\n",
       "      <th>Main interaction</th>\n",
       "      <th>Number of interactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23890098</td>\n",
       "      <td>Shlykov, a hard-working taxi driver and Lyosha...</td>\n",
       "      <td>Shlykov</td>\n",
       "      <td>1.0</td>\n",
       "      <td>('Lyosha', 'Shlykov')</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31186339</td>\n",
       "      <td>The nation of Panem consists of a wealthy Capi...</td>\n",
       "      <td>Katniss</td>\n",
       "      <td>18.0</td>\n",
       "      <td>('Katniss', 'Peeta Mellark')</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20663735</td>\n",
       "      <td>Poovalli Induchoodan  is sentenced for six yea...</td>\n",
       "      <td>Maranchery Karunakara Menon</td>\n",
       "      <td>9.0</td>\n",
       "      <td>('Manapally Madhavan Nambiar', 'judge Menon')</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2231378</td>\n",
       "      <td>The Lemon Drop Kid , a New York City swindler,...</td>\n",
       "      <td>Charley</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>595909</td>\n",
       "      <td>Seventh-day Adventist Church pastor Michael Ch...</td>\n",
       "      <td>Lindy</td>\n",
       "      <td>7.0</td>\n",
       "      <td>('Azaria', 'Lindy')</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42298</th>\n",
       "      <td>34808485</td>\n",
       "      <td>The story is about Reema , a young Muslim scho...</td>\n",
       "      <td>Reema</td>\n",
       "      <td>1.0</td>\n",
       "      <td>('Muslim', 'Reema')</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42299</th>\n",
       "      <td>1096473</td>\n",
       "      <td>In 1928 Hollywood, director Leo Andreyev  look...</td>\n",
       "      <td>Leo Andreyev</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42300</th>\n",
       "      <td>35102018</td>\n",
       "      <td>American Luthier focuses on Randy Parsons’ tra...</td>\n",
       "      <td>Randy Parsons</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42301</th>\n",
       "      <td>8628195</td>\n",
       "      <td>Abdur Rehman Khan , a middle-aged dry fruit se...</td>\n",
       "      <td>Abdur Rehman Khan</td>\n",
       "      <td>9.0</td>\n",
       "      <td>('Abdur Rehman Khan', 'Amina')</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42302</th>\n",
       "      <td>6040782</td>\n",
       "      <td>1940 - Operation Dynamo has just taken place. ...</td>\n",
       "      <td>George Mainwaring</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42303 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Wikipedia ID                                            Summary  \\\n",
       "0          23890098  Shlykov, a hard-working taxi driver and Lyosha...   \n",
       "1          31186339  The nation of Panem consists of a wealthy Capi...   \n",
       "2          20663735  Poovalli Induchoodan  is sentenced for six yea...   \n",
       "3           2231378  The Lemon Drop Kid , a New York City swindler,...   \n",
       "4            595909  Seventh-day Adventist Church pastor Michael Ch...   \n",
       "...             ...                                                ...   \n",
       "42298      34808485  The story is about Reema , a young Muslim scho...   \n",
       "42299       1096473  In 1928 Hollywood, director Leo Andreyev  look...   \n",
       "42300      35102018  American Luthier focuses on Randy Parsons’ tra...   \n",
       "42301       8628195  Abdur Rehman Khan , a middle-aged dry fruit se...   \n",
       "42302       6040782  1940 - Operation Dynamo has just taken place. ...   \n",
       "\n",
       "                    Main character  Number of mentions  \\\n",
       "0                          Shlykov                 1.0   \n",
       "1                          Katniss                18.0   \n",
       "2      Maranchery Karunakara Menon                 9.0   \n",
       "3                          Charley                18.0   \n",
       "4                            Lindy                 7.0   \n",
       "...                            ...                 ...   \n",
       "42298                        Reema                 1.0   \n",
       "42299                 Leo Andreyev                 7.0   \n",
       "42300                Randy Parsons                 4.0   \n",
       "42301            Abdur Rehman Khan                 9.0   \n",
       "42302            George Mainwaring                 9.0   \n",
       "\n",
       "                                    Main interaction  Number of interactions  \n",
       "0                              ('Lyosha', 'Shlykov')                     1.0  \n",
       "1                       ('Katniss', 'Peeta Mellark')                     2.0  \n",
       "2      ('Manapally Madhavan Nambiar', 'judge Menon')                     1.0  \n",
       "3                                                NaN                     NaN  \n",
       "4                                ('Azaria', 'Lindy')                     1.0  \n",
       "...                                              ...                     ...  \n",
       "42298                            ('Muslim', 'Reema')                     1.0  \n",
       "42299                                            NaN                     NaN  \n",
       "42300                                            NaN                     NaN  \n",
       "42301                 ('Abdur Rehman Khan', 'Amina')                     1.0  \n",
       "42302                                            NaN                     NaN  \n",
       "\n",
       "[42303 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: If we've already run this code, we can load the dataframe from a file\n",
    "plot_char_filename = 'Data/MovieSummaries/plot_characters.csv'\n",
    "pairs_df = pd.read_csv(plot_char_filename, sep='\\t', index_col=0)\n",
    "pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otherwise: get main character and number of mentions for each movie and store it into a file (takes a while to run)\n",
    "if not os.path.exists(plot_char_filename):\n",
    "    pairs_df = plot_df.copy(deep=True)\n",
    "    pairs_df['Main character'] = pairs_df['Wikipedia ID'].apply(most_mentioned)\n",
    "    pairs_df['Number of mentions'] = pairs_df['Main character'].apply(lambda x: np.nan if x is None else x[0][1])\n",
    "    pairs_df['Main character'] = pairs_df['Main character'].apply(lambda x: np.nan if x is None else x[0][0])\n",
    "\n",
    "    # Get main pairs of characters for each movie and number of interactions \n",
    "    pairs_df['Main interaction'] = pairs_df['Wikipedia ID'].apply(lambda x: character_pairs(x, plot_df))\n",
    "    pairs_df['Number of interactions'] = pairs_df['Main interaction'].apply(lambda x: np.nan if x is None else x[0][1])\n",
    "    pairs_df['Main interaction'] = pairs_df['Main interaction'].apply(lambda x: np.nan if x is None else x[0][0])\n",
    "\n",
    "    # Store data into csv file\n",
    "    pairs_df.to_csv(plot_char_filename, sep='\\t')\n",
    "    pairs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, the coreNLP files provided with the datasets are useful to extract the characters mentioned. \n",
    "\n",
    " However, our goal is to extract love relationships as well as the persona of characters in love. Using common mentions as a proxy for love relationships is a vulgar approximation and so we must run our own NLP analysis on the plot summaries to extract useful information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Custom CoreNLP Analysis\n",
    "\n",
    "We now use a **custom CoreNLP pipeline** to analyze the plot summaries. For now, due to the weakness of our available computing power, we only analyze romantic comedy movies. \n",
    "\n",
    "\n",
    "#### 2.1. Data preparation\n",
    "\n",
    "We extract the romantic comedy plot summaries that we will pass through our pipeline and store them as `.txt` files to be able to run them through the new coreNLP pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For later use: romance_genres = ['Romantic comedy', 'Romance Film', 'Romantic drama', 'Romantic fantasy', 'Romantic thriller']\n",
    "\n",
    "# Get a dataframe with romantic movies and their corresponding plots\n",
    "romance_genres = ['Romantic comedy'] \n",
    "rom_com_plots = get_plots(romance_genres, movie_df, plot_df)\n",
    "#display(rom_com_plots)\n",
    "\n",
    "# Store each plot summary as .txt file\n",
    "for index, row in rom_com_plots.iterrows():\n",
    "    # If directory doesn't exist, create it\n",
    "    if not os.path.exists('Data/MovieSummaries/RomancePlots'):\n",
    "        os.makedirs('Data/MovieSummaries/RomancePlots')\n",
    "    with open(\"Data/MovieSummaries/RomancePlots/{}.txt\".format(row['Wikipedia ID']), 'w', encoding='utf8') as f:\n",
    "        if type(row['Summary']) == str:\n",
    "            f.write(row['Summary'])\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Custom CoreNLP pipeline\n",
    "\n",
    "Our custom pipeline consists of the following annotators: \n",
    "\n",
    "1. [Tokenization (tokenize)](https://stanfordnlp.github.io/CoreNLP/tokenize.html): Turns the whole text into tokens. \n",
    "\n",
    "2. [Parts Of Speech (POS)](https://stanfordnlp.github.io/CoreNLP/pos.html): Tags each token with part of speech labels (e.g. determinants, verbs and nouns). \n",
    "\n",
    "3. [Lemmatization (lemma)](https://stanfordnlp.github.io/CoreNLP/lemma.html): Reduces each word to its lemma (e.g. *was* becomes *be*). \n",
    "\n",
    "4. [Named Entity Recognition (NER)](https://stanfordnlp.github.io/CoreNLP/ner.html): Identifies named entities from the text, including characters, locations and organizations. \n",
    "\n",
    "5. [Constituency parsing (parse)](https://stanfordnlp.github.io/CoreNLP/parse.html): Performs a syntactic analysis of each sentence in the form of a tree. \n",
    "\n",
    "6. [Coreference resolution (coref)](https://stanfordnlp.github.io/CoreNLP/coref.html): Aggregates mentions of the same entities in a text (e.g. when 'Harry' and 'he' refer to the same person). \n",
    "\n",
    "7. [Dependency parsing (depparse)](https://stanfordnlp.github.io/CoreNLP/depparse.html): Syntactic dependency parser. \n",
    "\n",
    "8. [Natural Logic (natlog)](https://stanfordnlp.github.io/CoreNLP/natlog.html): Identifies quantifier scope and token polarity. Required as preliminary for OpenIE. \n",
    "\n",
    "9. [Open Information Extraction (OpenIE)](https://stanfordnlp.github.io/CoreNLP/openie.html): Identifies relation between words as triples *(subject, relation, object of relation)*. We use this to extract relationships between characters, as well as character traits. \n",
    "\n",
    "10. [Knowledge Base Population (KBP)](https://stanfordnlp.github.io/CoreNLP/kbp.html): Identifies meaningful relation triples. \n",
    "\n",
    "\n",
    "#### 2.2. Running our pipeline\n",
    "\n",
    "We now run our own CoreNLP analysis on the plot summaries. This allows us to extract love relationships from the plot summaries much more accurately.\n",
    "\n",
    "**Goal**: Run our custom CoreNLP pipeline. \n",
    "\n",
    "**Recommendation**: Be careful about memory storage (takes a lot of memory to run!)\n",
    "\n",
    "**Prerequisite**: [java](https://www.java.com). \n",
    "\n",
    "**Installation steps**:\n",
    "1. Download the CoreNLP toolkit [here](https://stanfordnlp.github.io/CoreNLP/download.html).\n",
    "\n",
    "2. Data preparation: Extract plot summaries for romantic comedies into `.txt` files. Create a filelist containing the name of all the files which need to be processed using the following command: \n",
    "\n",
    "        find RomancePlots/*.txt > filelist.txt\n",
    "\n",
    "3. Change directory (`cd`) into the downloaded `stanford-corenlp` directory. \n",
    "        \n",
    "4. Run the custom CoreNLP pipeline via your terminal using the following command:\n",
    "\n",
    "        java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,pos,lemma,ner,parse,coref,depparse,natlog,openie,kbp -coref.md.type RULE -filelist filelist.txt -outputDirectory RomancePlotsOutputs/ -outputFormat xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis outputs are now stored as `.xml` files in the `RomancePlotsOutputs` directory. We now unzip them. RomancePlotsOutputs has 1491 readable files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all the romance plots xml files\n",
    "# If r`RomancePlotsOutputs directory doesn't exist, create it\n",
    "if not os.path.exists('Data/CoreNLP/RomancePlotsOutputs'):\n",
    "   with ZipFile('CoreNLP/RomanceOutputs.zip', 'r') as zipObj:\n",
    "      zipObj.extractall('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extracting data\n",
    "\n",
    "Now that we have run the coreNLP pipeline and that the analysis of each movie has been a stored into a .xml output file, we can extract the information from these files. \n",
    "\n",
    "We will first extract the attributes and actions related to entities in the plot summaries. We will extract verbs and attributes independently. \n",
    "Agent verb: character does the action\n",
    "Patient verb: character is the object of the action\n",
    "Attributes: character attributes\n",
    "\n",
    "**Dependency parsing extraction**\n",
    "| Relation | Description |  Type  |  Example |\n",
    "|---|---|---|---|\n",
    "| obl:agent | Agent | Agent verb | 'They were rescued by Dumbledore' -> obl:agent(rescued, Dumbledore) |\n",
    "| nsubj  | Nominal subject | Agent verb | 'Harry confronts Snape' -> nsubj(confronts, Harry) |\n",
    "| nsubj:pass | Passive nominal subject | Patient verb | 'Goyle casts a curse and is burned to death' -> nsubj:pass(burned, Goyle)|\n",
    "| nsubj:xsubj | Indirect nominal subject | Patient verb | 'Goyle casts a curse and is unable to control it' -> nsubj:xsubj(control, Goyle)|\n",
    "| obj |  Direct object | Patient verb | 'To protect Harry' -> obj(protect, Harry) |\n",
    "| appos | Appositional modifier | Attribute | 'Harry's mother, Lily' -> appos(mother, Lily) |\n",
    "| amod | Adjectival modifier | Attribute | 'After burrying Dobby' -> amod(Dobby, burrying) |\n",
    "| nmod:poss | Possessive nominal modifier | Attribute | 'Snape's memories' -> nmod:poss(memories, Snape) |\n",
    "| nmod:of | 'Of' nominal modifier | Attribute |'With the help of Griphook' -> nmod:of(help, Griphook) |\n",
    "\n",
    "We will also extract KBP outputs, which stores data including the main role, spouse, age and religion for each character if specified. \n",
    "\n",
    "**KBP Extraction**\n",
    "| Attributes | Relation name | \n",
    "|---|---|\n",
    "| Main role | per:title |\n",
    "| Marital relationship | per:spouse  |  \n",
    "| Age  | per:age | \n",
    "| Religion  | per:religion | \n",
    "| Death | per:cause_of_death |\n",
    "\n",
    "The [KBP documentation](https://stanfordnlp.github.io/CoreNLP/kbp.html) contains a description of all available KBP tags.\n",
    "\n",
    "**Running extraction**\n",
    "\n",
    "We can now extract the character descriptions of all characters in all non-romantic movies and store the results in a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_path = 'Data/CoreNLP/descriptions.csv'\n",
    "relations_path = 'Data/CoreNLP/relations.csv'\n",
    "\n",
    "if not os.path.exists(description_path) and not os.path.exists(relations_path):\n",
    "\n",
    "    # Extract descriptions and relations from all xml files\n",
    "    output_dir = 'Data/CoreNLP/PlotsOutputs'\n",
    "    descriptions, relations = extract_descriptions_relations(output_dir)\n",
    "\n",
    "    # Save descriptions and relations into csv files\n",
    "    descriptions.to_csv(description_path, sep='\\t')\n",
    "    relations.to_csv(relations_path, sep='\\t')\n",
    "\n",
    "# If we've already run the extraction, we can load the dataframe from a file\n",
    "else: \n",
    "    non_rom_descriptions = pd.read_csv(description_path, sep='\\t', index_col=0, low_memory=False)\n",
    "    non_rom_relations = pd.read_csv(relations_path, sep='\\t', index_col=0, low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same thing for romantic movies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "romance_description_path = 'Data/CoreNLP/romance_descriptions.csv'\n",
    "romance_relations_path = 'Data/CoreNLP/romance_relations.csv'\n",
    "\n",
    "if not os.path.exists(romance_description_path) and not os.path.exists(romance_relations_path):\n",
    "\n",
    "    # Extract descriptions and relations from all romance xml files\n",
    "    romance_output_dir = 'Data/CoreNLP/RomancePlotsOutputs'\n",
    "\n",
    "    # Remove file '43849.xml' from the directory, as it is not a valid xml file\n",
    "    if os.path.exists(f'{romance_output_dir}/43849.xml'):\n",
    "        os.remove(f'{romance_output_dir}/43849.xml')\n",
    "    \n",
    "    romance_descriptions, romance_relations = extract_descriptions_relations(romance_output_dir)\n",
    "\n",
    "    # Save descriptions and relations into csv files\n",
    "    romance_descriptions.to_csv(romance_description_path, sep='\\t')\n",
    "    romance_relations.to_csv(romance_relations_path, sep='\\t')\n",
    "\n",
    "# If we've already run the extraction, we can load the dataframe from a file\n",
    "else: \n",
    "    rom_descriptions = pd.read_csv(romance_description_path, sep='\\t', index_col=0, low_memory=False)\n",
    "    rom_relations = pd.read_csv(romance_relations_path, sep='\\t', index_col=0, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the descriptions of romance and non-romance movies by adding a column with romance boolean\n",
    "non_rom_descriptions['romance'] = False\n",
    "rom_descriptions['romance'] = True\n",
    "descriptions = pd.concat([non_rom_descriptions, rom_descriptions], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Processing extracted data\n",
    "\n",
    "We now pre-process the extracted character analysis, merge it with the pre-existing character and movie metadata and store it into a cute data file. \n",
    "\n",
    "1. Convert to lists and aggregate all descriptions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions['attributes'] = descriptions['attributes'].apply(lambda x: x if pd.isnull(x) else x[1:-1].replace(\"'\", \"\").replace(\" \", \"\").split(','))\n",
    "descriptions['agent_verbs'] = descriptions['agent_verbs'].apply(lambda x: x if pd.isnull(x) else x[1:-1].replace(\"'\", \"\").replace(\" \", \"\").split(','))\n",
    "descriptions['patient_verbs'] = descriptions['patient_verbs'].apply(lambda x: x if pd.isnull(x) else x[1:-1].replace(\"'\", \"\").replace(\" \", \"\").split(','))\n",
    "\n",
    "descriptions['all_descriptions'] = descriptions[['agent_verbs', 'patient_verbs', 'attributes', 'title']].apply(lambda x: [item for sublist in x if type(sublist) == list for item in sublist], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Lemmatize all words to their lexical root and verbs to their infinitive present tense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "lemmatize = lambda x: [lem.lemmatize(lem.lemmatize(word), 'v') for word in x]\n",
    "\n",
    "descriptions['agent_verbs'] = descriptions['agent_verbs'].apply(lambda x: lemmatize(x) if type(x) == list else x)\n",
    "descriptions['patient_verbs'] = descriptions['patient_verbs'].apply(lambda x: lemmatize(x) if type(x) == list else x)\n",
    "descriptions['attributes'] = descriptions['attributes'].apply(lambda x: lemmatize(x) if type(x) == list else x)\n",
    "descriptions['all_descriptions'] = descriptions['all_descriptions'].apply(lambda x: lemmatize(x) if type(x) == list else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Synchronize character names to the character metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_subset(name1, name2):\n",
    "    set1 = set(name1.split(' '))\n",
    "    set2 = set(name2.split(' '))\n",
    "    return set(set1).issubset(set(set2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synchronize_name(movie_id): \n",
    "    # Get list of characters from plots\n",
    "    plot_chars = descriptions[descriptions['movie_id'] == movie_id]['character'].values\n",
    "\n",
    "    # Check that movie_id is in Wikipedia ID of char_df\n",
    "    if movie_id not in char_df['Wikipedia ID'].values:\n",
    "        return {name: name for name in plot_chars}\n",
    "\n",
    "    # Get list of characters from movie metadata\n",
    "    movie_chars = char_df[char_df['Wikipedia ID'] == movie_id]['Character name'].values\n",
    "    \n",
    "    # Remove nan values in movie_chars, plot_chars\n",
    "    movie_chars = movie_chars[~pd.isnull(movie_chars)]\n",
    "    plot_chars = plot_chars[~pd.isnull(plot_chars)]\n",
    "    \n",
    "    # Create a dictionary to store the synchronized names\n",
    "    name_sync = {}\n",
    "\n",
    "    # First pass: check if plot name is subset of some movie name\n",
    "    for plot_char in plot_chars: \n",
    "        for movie_char in movie_chars:\n",
    "            if is_subset(plot_char, movie_char):\n",
    "                name_sync[plot_char] = movie_char\n",
    "                movie_chars = movie_chars[movie_chars != movie_char]\n",
    "                plot_chars = plot_chars[plot_chars != plot_char]\n",
    "\n",
    "    # Second pass: check if remaining movie name is subset of some remaining plot name\n",
    "    for movie_char in movie_chars:\n",
    "        for plot_char in plot_chars:\n",
    "            if is_subset(movie_char, plot_char):\n",
    "                name_sync[plot_char] = movie_char\n",
    "                movie_chars = movie_chars[movie_chars != movie_char]\n",
    "                plot_chars = plot_chars[plot_chars != plot_char]\n",
    "\n",
    "    # Remaining: if some plot_chars are not synchronized, we just keep them as they are\n",
    "    for plot_char in plot_chars:\n",
    "        name_sync[plot_char] = plot_char\n",
    "    return name_sync\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each movie_id in descriptions, synchronize the names and store in name_sync_df\n",
    "name_sync_df = pd.DataFrame(columns=['movie_id', 'name_sync'])\n",
    "for movie_id in descriptions['movie_id'].unique():\n",
    "    name_sync_df = pd.concat([name_sync_df, pd.DataFrame({'movie_id': [movie_id], 'name_sync': [synchronize_name(movie_id)]})], ignore_index=True)\n",
    "\n",
    "# Index name_sync by movie_id\n",
    "name_sync_df = name_sync_df.set_index('movie_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each character in descriptions, get the corresponding name_sync\n",
    "descriptions['synced_name'] = descriptions[['movie_id', 'character']].apply(lambda x: name_sync_df.loc[x['movie_id']].values[0][x['character']], axis=1)\n",
    "descriptions['plot_name'] = descriptions['character']\n",
    "descriptions['character'] = descriptions['synced_name']\n",
    "descriptions = descriptions.drop('synced_name', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Merge with character metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each movie_id in descriptions, get the corresponding Freebase ID (except if you can't find it)\n",
    "descriptions['freebase_id'] = descriptions['movie_id'].apply(lambda x: movie_df.loc[x]['Freebase ID'] if x in movie_df.index else np.nan)\n",
    "\n",
    "# Merge descriptions with char_df on character name and movie_id\n",
    "descriptions = descriptions.merge(char_df, left_on=['character', 'movie_id'], right_on=['Character name', 'Wikipedia ID'], how='left')\n",
    "\n",
    "# Drop columns movie_id, character\n",
    "descriptions = descriptions.drop(['movie_id', 'character', 'freebase_id'], axis=1)\n",
    "\n",
    "# Put columns Character name, Wikipedia ID at the beginning\n",
    "cols = descriptions.columns.tolist()\n",
    "cols = [col for col in cols if col not in ['Wikipedia ID', 'Character name']]\n",
    "cols = ['Wikipedia ID', 'Character name'] + cols\n",
    "descriptions = descriptions[cols]\n",
    "\n",
    "# Save descriptions to csv \n",
    "descriptions.to_csv('full_descriptions.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wikipedia ID</th>\n",
       "      <th>Character name</th>\n",
       "      <th>agent_verbs</th>\n",
       "      <th>patient_verbs</th>\n",
       "      <th>attributes</th>\n",
       "      <th>title</th>\n",
       "      <th>religion</th>\n",
       "      <th>age</th>\n",
       "      <th>death</th>\n",
       "      <th>children</th>\n",
       "      <th>...</th>\n",
       "      <th>Release date</th>\n",
       "      <th>Date of birth</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Actor name</th>\n",
       "      <th>Actor age at release</th>\n",
       "      <th>Freebase character/map ID</th>\n",
       "      <th>Freebase character ID</th>\n",
       "      <th>Freebase actor ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30843639.0</td>\n",
       "      <td>Christopher Isherwood</td>\n",
       "      <td>[take, see, take, find, try, meet, suggest, pr...</td>\n",
       "      <td>[secure, leave, publish, remain, see, entreat,...</td>\n",
       "      <td>[death, life, family]</td>\n",
       "      <td>artist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2011-02-20</td>\n",
       "      <td>1982-10-28</td>\n",
       "      <td>M</td>\n",
       "      <td>1.81</td>\n",
       "      <td>/m/02w7gg</td>\n",
       "      <td>Matt Smith</td>\n",
       "      <td>28.0</td>\n",
       "      <td>/m/0ggcwv7</td>\n",
       "      <td>/m/0gy62jz</td>\n",
       "      <td>/m/04qbht5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30843639.0</td>\n",
       "      <td>Caspar</td>\n",
       "      <td>[disappear]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[one, rentboys]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2011-02-20</td>\n",
       "      <td>1983-02-07</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alexander Doetsch</td>\n",
       "      <td>28.0</td>\n",
       "      <td>/m/0gj2y0d</td>\n",
       "      <td>/m/0j3zpc5</td>\n",
       "      <td>/m/0gj2y0h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30843639.0</td>\n",
       "      <td>Wilfrid Landauer</td>\n",
       "      <td>[burn]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[see, last]</td>\n",
       "      <td>owner</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2011-02-20</td>\n",
       "      <td>1975-08-05</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Iddo Goldberg</td>\n",
       "      <td>35.0</td>\n",
       "      <td>/m/0gj2y2f</td>\n",
       "      <td>/m/0j3zpcm</td>\n",
       "      <td>/m/052_5mh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30843639.0</td>\n",
       "      <td>Gerhardt Neddermayer</td>\n",
       "      <td>[advise]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[brother]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2011-02-20</td>\n",
       "      <td>1973-06-20</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thomas Wlaschiha</td>\n",
       "      <td>37.0</td>\n",
       "      <td>/m/0j3zpl1</td>\n",
       "      <td>/m/0j3zpl4</td>\n",
       "      <td>/m/0gbp8_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30843639.0</td>\n",
       "      <td>Heinz Neddermayer</td>\n",
       "      <td>[decide, express]</td>\n",
       "      <td>[deny, leave, advise, welcome, sentence, arrest]</td>\n",
       "      <td>[brother]</td>\n",
       "      <td>street sweeper</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2011-02-20</td>\n",
       "      <td>1992-07-09</td>\n",
       "      <td>M</td>\n",
       "      <td>1.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Douglas Booth</td>\n",
       "      <td>18.0</td>\n",
       "      <td>/m/0j3zpfd</td>\n",
       "      <td>/m/0j3zpfh</td>\n",
       "      <td>/m/0cc9052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163190</th>\n",
       "      <td>22430724.0</td>\n",
       "      <td>Colleen Peck</td>\n",
       "      <td>[tell, express, appear]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2010-11-10</td>\n",
       "      <td>1946-01-05</td>\n",
       "      <td>F</td>\n",
       "      <td>1.69</td>\n",
       "      <td>/m/033tf_</td>\n",
       "      <td>Diane Keaton</td>\n",
       "      <td>64.0</td>\n",
       "      <td>/m/0648r04</td>\n",
       "      <td>/m/087srb3</td>\n",
       "      <td>/m/01csvq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163191</th>\n",
       "      <td>22430724.0</td>\n",
       "      <td>Mike Pomeroy</td>\n",
       "      <td>[prof, show, manage, tell, break, end, end, co...</td>\n",
       "      <td>[escape, tell, go, choose]</td>\n",
       "      <td>[reluctant, rans]</td>\n",
       "      <td>journalist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2010-11-10</td>\n",
       "      <td>1942-07-13</td>\n",
       "      <td>M</td>\n",
       "      <td>1.85</td>\n",
       "      <td>/m/01qhm_</td>\n",
       "      <td>Harrison Ford</td>\n",
       "      <td>68.0</td>\n",
       "      <td>/m/0648q_y</td>\n",
       "      <td>/m/087sr8c</td>\n",
       "      <td>/m/0c0k1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163192</th>\n",
       "      <td>22430724.0</td>\n",
       "      <td>Adam Bennett</td>\n",
       "      <td>[tease, realize]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>producer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2010-11-10</td>\n",
       "      <td>1973-07-03</td>\n",
       "      <td>M</td>\n",
       "      <td>1.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Patrick Wilson</td>\n",
       "      <td>37.0</td>\n",
       "      <td>/m/0648r0m</td>\n",
       "      <td>/m/087srbx</td>\n",
       "      <td>/m/055zkh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163193</th>\n",
       "      <td>22430724.0</td>\n",
       "      <td>Jerry Barnes</td>\n",
       "      <td>[hire, remain]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2010-11-10</td>\n",
       "      <td>1952-10-22</td>\n",
       "      <td>M</td>\n",
       "      <td>1.94</td>\n",
       "      <td>/m/041rx</td>\n",
       "      <td>Jeff Goldblum</td>\n",
       "      <td>58.0</td>\n",
       "      <td>/m/0648r09</td>\n",
       "      <td>/m/087sr91</td>\n",
       "      <td>/m/01y_px</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163194</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[persuade]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>163195 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Wikipedia ID         Character name  \\\n",
       "0         30843639.0  Christopher Isherwood   \n",
       "1         30843639.0                 Caspar   \n",
       "2         30843639.0       Wilfrid Landauer   \n",
       "3         30843639.0   Gerhardt Neddermayer   \n",
       "4         30843639.0      Heinz Neddermayer   \n",
       "...              ...                    ...   \n",
       "163190    22430724.0           Colleen Peck   \n",
       "163191    22430724.0           Mike Pomeroy   \n",
       "163192    22430724.0           Adam Bennett   \n",
       "163193    22430724.0           Jerry Barnes   \n",
       "163194           NaN                    NaN   \n",
       "\n",
       "                                              agent_verbs  \\\n",
       "0       [take, see, take, find, try, meet, suggest, pr...   \n",
       "1                                             [disappear]   \n",
       "2                                                  [burn]   \n",
       "3                                                [advise]   \n",
       "4                                       [decide, express]   \n",
       "...                                                   ...   \n",
       "163190                            [tell, express, appear]   \n",
       "163191  [prof, show, manage, tell, break, end, end, co...   \n",
       "163192                                   [tease, realize]   \n",
       "163193                                     [hire, remain]   \n",
       "163194                                                NaN   \n",
       "\n",
       "                                            patient_verbs  \\\n",
       "0       [secure, leave, publish, remain, see, entreat,...   \n",
       "1                                                     NaN   \n",
       "2                                                     NaN   \n",
       "3                                                     NaN   \n",
       "4        [deny, leave, advise, welcome, sentence, arrest]   \n",
       "...                                                   ...   \n",
       "163190                                                NaN   \n",
       "163191                         [escape, tell, go, choose]   \n",
       "163192                                                NaN   \n",
       "163193                                                NaN   \n",
       "163194                                                NaN   \n",
       "\n",
       "                   attributes           title religion  age death children  \\\n",
       "0       [death, life, family]          artist      NaN  NaN   NaN      NaN   \n",
       "1             [one, rentboys]             NaN      NaN  NaN   NaN      NaN   \n",
       "2                 [see, last]           owner      NaN  NaN   NaN      NaN   \n",
       "3                   [brother]             NaN      NaN  NaN   NaN      NaN   \n",
       "4                   [brother]  street sweeper      NaN  NaN   NaN      NaN   \n",
       "...                       ...             ...      ...  ...   ...      ...   \n",
       "163190                    NaN             NaN      NaN  NaN   NaN      NaN   \n",
       "163191      [reluctant, rans]      journalist      NaN  NaN   NaN      NaN   \n",
       "163192                    NaN        producer      NaN  NaN   NaN      NaN   \n",
       "163193                    NaN             NaN      NaN  NaN   NaN      NaN   \n",
       "163194             [persuade]             NaN      NaN  NaN   NaN      NaN   \n",
       "\n",
       "        ...  Release date Date of birth Gender Height  Ethnicity  \\\n",
       "0       ...    2011-02-20    1982-10-28      M   1.81  /m/02w7gg   \n",
       "1       ...    2011-02-20    1983-02-07      M    NaN        NaN   \n",
       "2       ...    2011-02-20    1975-08-05      M    NaN        NaN   \n",
       "3       ...    2011-02-20    1973-06-20      M    NaN        NaN   \n",
       "4       ...    2011-02-20    1992-07-09      M   1.85        NaN   \n",
       "...     ...           ...           ...    ...    ...        ...   \n",
       "163190  ...    2010-11-10    1946-01-05      F   1.69  /m/033tf_   \n",
       "163191  ...    2010-11-10    1942-07-13      M   1.85  /m/01qhm_   \n",
       "163192  ...    2010-11-10    1973-07-03      M   1.85        NaN   \n",
       "163193  ...    2010-11-10    1952-10-22      M   1.94   /m/041rx   \n",
       "163194  ...           NaN           NaN    NaN    NaN        NaN   \n",
       "\n",
       "               Actor name Actor age at release  Freebase character/map ID  \\\n",
       "0              Matt Smith                 28.0                 /m/0ggcwv7   \n",
       "1       Alexander Doetsch                 28.0                 /m/0gj2y0d   \n",
       "2           Iddo Goldberg                 35.0                 /m/0gj2y2f   \n",
       "3        Thomas Wlaschiha                 37.0                 /m/0j3zpl1   \n",
       "4           Douglas Booth                 18.0                 /m/0j3zpfd   \n",
       "...                   ...                  ...                        ...   \n",
       "163190       Diane Keaton                 64.0                 /m/0648r04   \n",
       "163191      Harrison Ford                 68.0                 /m/0648q_y   \n",
       "163192     Patrick Wilson                 37.0                 /m/0648r0m   \n",
       "163193      Jeff Goldblum                 58.0                 /m/0648r09   \n",
       "163194                NaN                  NaN                        NaN   \n",
       "\n",
       "       Freebase character ID Freebase actor ID  \n",
       "0                 /m/0gy62jz        /m/04qbht5  \n",
       "1                 /m/0j3zpc5        /m/0gj2y0h  \n",
       "2                 /m/0j3zpcm        /m/052_5mh  \n",
       "3                 /m/0j3zpl4        /m/0gbp8_1  \n",
       "4                 /m/0j3zpfh        /m/0cc9052  \n",
       "...                      ...               ...  \n",
       "163190            /m/087srb3         /m/01csvq  \n",
       "163191            /m/087sr8c          /m/0c0k1  \n",
       "163192            /m/087srbx         /m/055zkh  \n",
       "163193            /m/087sr91         /m/01y_px  \n",
       "163194                   NaN               NaN  \n",
       "\n",
       "[163195 rows x 24 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Aggregate all descriptions over all movies for each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new DataFrame with a single character per row\n",
    "char_descriptions = pd.DataFrame(descriptions[['Character name', 'Freebase character ID']].drop_duplicates())\n",
    "\n",
    "# Aggregate all agent_verbs together\n",
    "agent_verbs = descriptions.groupby(['Freebase character ID'])['agent_verbs'].aggregate(lambda x: list(itertools.chain.from_iterable(x.dropna())))\n",
    "char_descriptions = char_descriptions.merge(agent_verbs, left_on='Freebase character ID', right_index=True, how='left')\n",
    "\n",
    "# Aggregate all patient_verbs together\n",
    "patient_verbs = descriptions.groupby(['Freebase character ID'])['patient_verbs'].aggregate(lambda x: list(itertools.chain.from_iterable(x.dropna())))\n",
    "char_descriptions = char_descriptions.merge(patient_verbs, left_on='Freebase character ID', right_index=True, how='left')\n",
    "\n",
    "# Aggregate all attributes together\n",
    "attributes = descriptions.groupby(['Freebase character ID'])['attributes'].aggregate(lambda x: list(itertools.chain.from_iterable(x.dropna())))\n",
    "char_descriptions = char_descriptions.merge(attributes, left_on='Freebase character ID', right_index=True, how='left')\n",
    "\n",
    "# Aggregate all string titles together into a list of titles\n",
    "titles = descriptions.groupby(['Freebase character ID'])['title'].aggregate(lambda x: list(x.dropna()))\n",
    "char_descriptions = char_descriptions.merge(titles, left_on='Freebase character ID', right_index=True, how='left')\n",
    "\n",
    "# Concatenate all agent_verbs, patient_verbs, attributes, titles into a single list of descriptions\n",
    "char_descriptions['descriptions'] = char_descriptions[['agent_verbs', 'patient_verbs', 'attributes', 'title']].apply(lambda x: list(itertools.chain.from_iterable(x.dropna())), axis=1)\n",
    "\n",
    "# Save char_descriptions to csv\n",
    "char_descriptions.to_csv('char_descriptions.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character name</th>\n",
       "      <th>Freebase character ID</th>\n",
       "      <th>agent_verbs</th>\n",
       "      <th>patient_verbs</th>\n",
       "      <th>attributes</th>\n",
       "      <th>title</th>\n",
       "      <th>descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Christopher Isherwood</td>\n",
       "      <td>/m/0gy62jz</td>\n",
       "      <td>[take, see, take, find, try, meet, suggest, pr...</td>\n",
       "      <td>[secure, leave, publish, remain, see, entreat,...</td>\n",
       "      <td>[death, life, family]</td>\n",
       "      <td>[artist]</td>\n",
       "      <td>[take, see, take, find, try, meet, suggest, pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Caspar</td>\n",
       "      <td>/m/0j3zpc5</td>\n",
       "      <td>[disappear]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[one, rentboys]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[disappear, one, rentboys]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wilfrid Landauer</td>\n",
       "      <td>/m/0j3zpcm</td>\n",
       "      <td>[burn]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[see, last]</td>\n",
       "      <td>[owner]</td>\n",
       "      <td>[burn, see, last, owner]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gerhardt Neddermayer</td>\n",
       "      <td>/m/0j3zpl4</td>\n",
       "      <td>[advise]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[brother]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[advise, brother]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Heinz Neddermayer</td>\n",
       "      <td>/m/0j3zpfh</td>\n",
       "      <td>[decide, express]</td>\n",
       "      <td>[deny, leave, advise, welcome, sentence, arrest]</td>\n",
       "      <td>[brother]</td>\n",
       "      <td>[street sweeper]</td>\n",
       "      <td>[decide, express, deny, leave, advise, welcome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163188</th>\n",
       "      <td>Becky Fuller</td>\n",
       "      <td>/m/087sr8v</td>\n",
       "      <td>[receive, snap, decide, get, realize, watch, g...</td>\n",
       "      <td>[drive, know, hire, tease, producer, tell, rem...</td>\n",
       "      <td>[demise, campaign, demise, campaign]</td>\n",
       "      <td>[executive producer, producer]</td>\n",
       "      <td>[receive, snap, decide, get, realize, watch, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163190</th>\n",
       "      <td>Colleen Peck</td>\n",
       "      <td>/m/087srb3</td>\n",
       "      <td>[tell, express, appear]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tell, express, appear]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163191</th>\n",
       "      <td>Mike Pomeroy</td>\n",
       "      <td>/m/087sr8c</td>\n",
       "      <td>[prof, show, manage, tell, break, end, end, co...</td>\n",
       "      <td>[escape, tell, go, choose]</td>\n",
       "      <td>[reluctant, rans]</td>\n",
       "      <td>[journalist]</td>\n",
       "      <td>[prof, show, manage, tell, break, end, end, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163192</th>\n",
       "      <td>Adam Bennett</td>\n",
       "      <td>/m/087srbx</td>\n",
       "      <td>[tease, realize]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[producer]</td>\n",
       "      <td>[tease, realize, producer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163193</th>\n",
       "      <td>Jerry Barnes</td>\n",
       "      <td>/m/087sr91</td>\n",
       "      <td>[hire, remain]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[hire, remain]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45133 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Character name Freebase character ID  \\\n",
       "0       Christopher Isherwood            /m/0gy62jz   \n",
       "1                      Caspar            /m/0j3zpc5   \n",
       "2            Wilfrid Landauer            /m/0j3zpcm   \n",
       "3        Gerhardt Neddermayer            /m/0j3zpl4   \n",
       "4           Heinz Neddermayer            /m/0j3zpfh   \n",
       "...                       ...                   ...   \n",
       "163188           Becky Fuller            /m/087sr8v   \n",
       "163190           Colleen Peck            /m/087srb3   \n",
       "163191           Mike Pomeroy            /m/087sr8c   \n",
       "163192           Adam Bennett            /m/087srbx   \n",
       "163193           Jerry Barnes            /m/087sr91   \n",
       "\n",
       "                                              agent_verbs  \\\n",
       "0       [take, see, take, find, try, meet, suggest, pr...   \n",
       "1                                             [disappear]   \n",
       "2                                                  [burn]   \n",
       "3                                                [advise]   \n",
       "4                                       [decide, express]   \n",
       "...                                                   ...   \n",
       "163188  [receive, snap, decide, get, realize, watch, g...   \n",
       "163190                            [tell, express, appear]   \n",
       "163191  [prof, show, manage, tell, break, end, end, co...   \n",
       "163192                                   [tease, realize]   \n",
       "163193                                     [hire, remain]   \n",
       "\n",
       "                                            patient_verbs  \\\n",
       "0       [secure, leave, publish, remain, see, entreat,...   \n",
       "1                                                      []   \n",
       "2                                                      []   \n",
       "3                                                      []   \n",
       "4        [deny, leave, advise, welcome, sentence, arrest]   \n",
       "...                                                   ...   \n",
       "163188  [drive, know, hire, tease, producer, tell, rem...   \n",
       "163190                                                 []   \n",
       "163191                         [escape, tell, go, choose]   \n",
       "163192                                                 []   \n",
       "163193                                                 []   \n",
       "\n",
       "                                  attributes                           title  \\\n",
       "0                      [death, life, family]                        [artist]   \n",
       "1                            [one, rentboys]                              []   \n",
       "2                                [see, last]                         [owner]   \n",
       "3                                  [brother]                              []   \n",
       "4                                  [brother]                [street sweeper]   \n",
       "...                                      ...                             ...   \n",
       "163188  [demise, campaign, demise, campaign]  [executive producer, producer]   \n",
       "163190                                    []                              []   \n",
       "163191                     [reluctant, rans]                    [journalist]   \n",
       "163192                                    []                      [producer]   \n",
       "163193                                    []                              []   \n",
       "\n",
       "                                             descriptions  \n",
       "0       [take, see, take, find, try, meet, suggest, pr...  \n",
       "1                              [disappear, one, rentboys]  \n",
       "2                                [burn, see, last, owner]  \n",
       "3                                       [advise, brother]  \n",
       "4       [decide, express, deny, leave, advise, welcome...  \n",
       "...                                                   ...  \n",
       "163188  [receive, snap, decide, get, realize, watch, g...  \n",
       "163190                            [tell, express, appear]  \n",
       "163191  [prof, show, manage, tell, break, end, end, co...  \n",
       "163192                         [tease, realize, producer]  \n",
       "163193                                     [hire, remain]  \n",
       "\n",
       "[45133 rows x 7 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Analyzing results\n",
    "\n",
    "#### 4.1 Filtering relationships\n",
    "Let us first get a list of everyone who is in a relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7079 relationships in non-romance movies, consisting of 7060 unique characters.\n",
      "There are 640 relationships in romance movies, consisting of 571 unique characters.\n"
     ]
    }
   ],
   "source": [
    "def get_characters_rom(df):\n",
    "    # Get a dataframe with the unique characters from the relations dataframe and the wikipedia ID of the movie they appear in\n",
    "    only_subjects = df[['subject', 'movie_id']].drop_duplicates()\n",
    "    only_objects = df[['object', 'movie_id']].drop_duplicates()\n",
    "\n",
    "    # remove rows in only_objects where the object is in the column subject of only_subjects\n",
    "    only_objects = only_objects[~only_objects['object'].isin(only_subjects['subject'])]\n",
    "\n",
    "    # Concatenate the two dataframes\n",
    "    characters_df = pd.concat([only_subjects, only_objects], ignore_index=True)\n",
    "\n",
    "    # Combine the subject and object columns into one column\n",
    "    characters_df['character'] = characters_df['subject'].combine_first(characters_df['object'])\n",
    "    characters_df = characters_df.drop(columns=['subject', 'object'])\n",
    "    return characters_df\n",
    "\n",
    "characters_non_romance = get_characters_rom(relations)\n",
    "characters_romance = get_characters_rom(romance_relations)\n",
    "\n",
    "print('There are {} relationships in non-romance movies, consisting of {} unique characters.'.format(len(relations), len(characters_non_romance)))\n",
    "print('There are {} relationships in romance movies, consisting of {} unique characters.'.format(len(romance_relations), len(characters_romance)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we merge the rows with the same character, and aggregate the titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each unique movie id and character combination, add an identifier for the character\n",
    "descriptions['character_id'] = descriptions.groupby(['movie_id', 'character']).ngroup()\n",
    "romance_descriptions['character_id'] = romance_descriptions.groupby(['movie_id', 'character']).ngroup()\n",
    "len_desc = len(descriptions)\n",
    "len_rom_desc = len(romance_descriptions)\n",
    "\n",
    "# Combine the rows with the same character_id into one row, with the title being the concatenation of all the titles. Preserve all other columns. Ignore NaN for titles\n",
    "\n",
    "descriptions = descriptions.groupby('character_id').agg(\n",
    "    {'movie_id': 'first', 'character': 'first', 'title': lambda x: ' '.join(x.dropna()), 'agent_verbs': 'first',\n",
    "     'patient_verbs': 'first', 'attributes': 'first', 'religion': 'first', 'age': 'first'})\n",
    "romance_descriptions = romance_descriptions.groupby('character_id').agg(\n",
    "    {'movie_id': 'first', 'character': 'first', 'title': lambda x: ' '.join(x.dropna()), 'agent_verbs': 'first',\n",
    "        'patient_verbs': 'first', 'attributes': 'first', 'religion': 'first', 'age': 'first'})\n",
    "\n",
    "# print reduction in number of rows for each dataframe\n",
    "print('The number of rows in the descriptions dataframe was reduced from {} to {}.'.format(len_desc, len(descriptions)))\n",
    "print('The number of rows in the romance descriptions dataframe was reduced from {} to {}.'.format(len_rom_desc, len(romance_descriptions)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we merge the characters with their attributes. `full_char_rom` and `full_char` contains all characters in romantic relationships, alongside their descriptions, which also includes the gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we merge the characters with their descriptions\n",
    "def map_char_attributes(char, descr):\n",
    "    # Join the two dataframes based on the character name and the movie ID\n",
    "    char_descr = char.merge(descr, on=['character', 'movie_id'], how='left')\n",
    "    return char_descr\n",
    "\n",
    "# convert movie_id column to int\n",
    "# from movie_id column of romance_descriptions, delete all elements of string that are not digits\n",
    "romance_descriptions['movie_id'] = romance_descriptions['movie_id'].str.replace(\n",
    "    r'\\D', '')\n",
    "romance_descriptions['movie_id'] = romance_descriptions['movie_id'].astype(int)\n",
    "\n",
    "full_char = map_char_attributes(characters_non_romance, descriptions)\n",
    "full_char_rom = map_char_attributes(characters_romance, romance_descriptions)\n",
    "\n",
    "# Merge full_char with char_df, map movie_id to Wikipedia ID and map character to Character name. Keep only column Gender from char_df, keep all columns from full_char\n",
    "full_char = full_char.merge(char_df, left_on=['movie_id', 'character'], \n",
    "                            right_on=['Wikipedia ID', 'Character name'], how='left')\n",
    "# From the new dataframe, drop all columns from char_df except gender\n",
    "full_char = full_char.drop(['Character name', 'Wikipedia ID', 'Freebase ID', \n",
    "                            'Release date','Ethnicity', 'Date of birth', 'Height',\n",
    "                            'Actor name', 'Actor age at release', 'Freebase character/map ID',\n",
    "                            'Freebase character ID', 'Freebase actor ID'], axis=1)\n",
    "# Same for romance movies\n",
    "full_char_rom = full_char_rom.merge(char_df, left_on=['movie_id', 'character'],\n",
    "                                    right_on=['Wikipedia ID', 'Character name'], how='left')\n",
    "full_char_rom = full_char_rom.drop(['Character name', 'Wikipedia ID', 'Freebase ID',\n",
    "                                    'Release date','Ethnicity', 'Date of birth', 'Height',\n",
    "                                    'Actor name', 'Actor age at release', 'Freebase character/map ID',\n",
    "                                    'Freebase character ID', 'Freebase actor ID'], axis=1)\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_char = pd.read_csv('Data/CoreNLP/full_char.csv', sep='\\t', index_col=0)\n",
    "full_char_rom = pd.read_csv('Data/CoreNLP/full_char_rom.csv', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first embed the titles, to then be able to cluster them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed the titles using spacy and nltk\n",
    "loading = False\n",
    "if loading:\n",
    "    nlp_spacy = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# get the embeddings for the titles\n",
    "def embed_titles(df):\n",
    "    titles = df['title'].values\n",
    "    embeddings = np.concatenate([nlp_spacy(title).vector.reshape(1, -1) for title in titles])\n",
    "    # add the embeddings to the dataframe\n",
    "    df['title_embeddings'] = list(embeddings)\n",
    "    return df\n",
    "\n",
    "title_indices = full_char[~full_char['title'].isnull() & (full_char['title'] != '')].index\n",
    "char_with_title = full_char.loc[title_indices]\n",
    "title_indices_rom = full_char_rom[~full_char_rom['title'].isnull() & (full_char_rom['title'] != '')].index\n",
    "char_with_title_rom = full_char_rom.loc[title_indices_rom]\n",
    "char_embeddings = embed_titles(char_with_title)\n",
    "char_embeddings_rom = embed_titles(char_with_title_rom)\n",
    "\n",
    "print('There are {} characters with titles in non-romance movies.'.format(len(char_embeddings)))\n",
    "print('There are {} characters with titles in romance movies.'.format(len(char_embeddings_rom)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster the titles using kmeans\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Create a list of silhouette scores for different k values\n",
    "silhouette_scores = []\n",
    "for k in range(2, 150):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(char_embeddings['title_embeddings'].tolist())\n",
    "    silhouette_scores.append(silhouette_score(char_embeddings['title_embeddings'].tolist(), kmeans.labels_))\n",
    "\n",
    "# Plot the silhouette scores\n",
    "plt.plot(range(2, 150), silhouette_scores)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans with , only assign title to a cluster if the silhouette score is above 0.1\n",
    "kmeans = KMeans(n_clusters=60, random_state=0).fit(\n",
    "    char_embeddings['title_embeddings'].tolist())\n",
    "char_embeddings['Cluster'] = kmeans.labels_\n",
    "title_embeddings = char_embeddings[char_embeddings['Cluster'].apply(\n",
    "    lambda x: silhouette_score(char_embeddings['title_embeddings'].tolist(), kmeans.labels_) > 0.05)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import counter\n",
    "from collections import Counter\n",
    "# Get a dataframe, where each row is a cluster and the columns are the top 10 words in the cluster\n",
    "def get_cluster_words(df, n_words=10):\n",
    "    # Get the titles for each cluster\n",
    "    cluster_titles = df.groupby('Cluster')['title'].apply(lambda x: ' '.join(x))\n",
    "    # Get the top n words for each cluster\n",
    "    cluster_words = cluster_titles.apply(lambda x: pd.Series(\n",
    "        [item[0] for item in Counter(x.split()).most_common(n_words)]))\n",
    "    # Add the cluster number as a column\n",
    "    cluster_words['Cluster'] = cluster_words.index\n",
    "    return cluster_words\n",
    "\n",
    "# Get the top 10 words for each cluster\n",
    "cluster_words = get_cluster_words(title_embeddings)\n",
    "cluster_words.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis common titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common titles men"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get from char_with_title the rows with gender male\n",
    "char_with_title_male = char_with_title[char_with_title['Gender'] == 'M']\n",
    "char_with_title_female = char_with_title[char_with_title['Gender'] == 'F']\n",
    "char_with_title_male_rom = char_with_title_rom[char_with_title_rom['Gender'] == 'M']\n",
    "char_with_title_female_rom = char_with_title_rom[char_with_title_rom['Gender'] == 'F']\n",
    "len_male = len(char_with_title_male)\n",
    "len_male_rom = len(char_with_title_male_rom)\n",
    "len_female = len(char_with_title_female)\n",
    "len_female_rom = len(char_with_title_female_rom)\n",
    "len_unknown = len(char_with_title) - len_male - len_female\n",
    "len_unknown_rom = len(char_with_title_rom) - len_male_rom - len_female_rom\n",
    "\n",
    "print('Known genders: \\n_______________________________________________________________')\n",
    "print('There are {} male characters with titles in non-romance movies.'.format(len_male))\n",
    "print('There are {} female characters with titles in non-romance movies.'.format(len_female))\n",
    "print('There are {} male characters with titles in romance movies.'.format(len_male_rom))\n",
    "print('There are {} female characters with titles in romance movies.'.format(len_female_rom))\n",
    "print('')\n",
    "print('Unknown genders: \\n_______________________________________________________________')\n",
    "print('There are {} characters with unknown gender in non-romance movies.'.format(len_unknown))\n",
    "print('There are {} characters with unknown gender in romantic movies.'.format(len_unknown_rom))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each relationship, add the description of both the subject and the object\n",
    "def add_descriptions(relations, descriptions):\n",
    "    # add the description of the subject\n",
    "    relations = relations.merge(descriptions, left_on=['movie_id', 'subject'], right_on=['movie_id', 'character'], how='left')\n",
    "    # add the description of the object\n",
    "    relations = relations.merge(descriptions, left_on=['movie_id', 'object'], right_on=[\n",
    "                  'movie_id', 'character'], how='left')\n",
    "    return relations\n",
    "\n",
    "# add the descriptions to the relationships\n",
    "relations_char = add_descriptions(relations, full_char)\n",
    "relations_char = relations_char.rename(columns={'subject': 'x', 'object': 'y'})\n",
    "\n",
    "# Do the same for romance movies\n",
    "relations_char_rom = add_descriptions(romance_relations, full_char_rom)\n",
    "relations_char_rom = relations_char_rom.rename(columns={'subject': 'x', 'object': 'y'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the relationships where title_x and title_y are both not null and not empty (\"\")\n",
    "title_indices = relations_char[~relations_char['title_x'].isnull() & (relations_char['title_x'] != '') &\n",
    "                                 ~relations_char['title_y'].isnull() & (relations_char['title_y'] != '')].index\n",
    "relations_titles = relations_char.loc[title_indices][['movie_id', 'x', 'y', 'title_x', 'title_y']]\n",
    "\n",
    "title_indices_rom = relations_char_rom[~relations_char_rom['title_x'].isnull() & (relations_char_rom['title_x'] != '') &\n",
    "                                    ~relations_char_rom['title_y'].isnull() & (relations_char_rom['title_y'] != '')].index\n",
    "relations_titles_rom = relations_char_rom.loc[title_indices_rom][['movie_id', 'x', 'y', 'title_x', 'title_y']]\n",
    "\n",
    "print('There are {} relationships with titles for both persons in the couple in non-romance movies.'.format(len(relations_titles)))\n",
    "print('There are {} relationships with titles for both persons in the couple in romance movies.'.format(len(relations_titles_rom)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find rows title x and title y are the same\n",
    "same_title = relations_titles[relations_titles['title_x'] == relations_titles['title_y']]\n",
    "same_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 pairs of title_x and title_y appearing together\n",
    "relations_titles.groupby(['title_x', 'title_y']).size().sort_values(ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attributes for characters in relationships\n",
    "\n",
    "Let us first look at the most common attributes for characters in relationships\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = full_char['attributes']\n",
    "attributes_rom = full_char_rom['attributes']\n",
    "\n",
    "# Get a dictionary with the attributes as keys and the number of times they appear as values\n",
    "def get_attribute_counts(attributes):\n",
    "    attribute_counts = {}\n",
    "    for attribute_list in attributes:\n",
    "        # check if the attribute list is not NaN\n",
    "        if not pd.isnull(attribute_list):\n",
    "            # # remove first and last character (the brackets)\n",
    "            attribute_list = attribute_list[1:-1]\n",
    "            # remove all apostrophes\n",
    "            attribute_list = attribute_list.replace(\"'\", \"\")\n",
    "            # remove all spaces\n",
    "            attribute_list = attribute_list.replace(\" \", \"\")\n",
    "            # # convert string to list\n",
    "            attribute_list = attribute_list.split(',')\n",
    "            # iterate over the attributes in the list\n",
    "            for attribute in attribute_list:\n",
    "                if attribute in attribute_counts:\n",
    "                    attribute_counts[attribute] += 1\n",
    "                else:\n",
    "                    attribute_counts[attribute] = 1\n",
    "    return attribute_counts\n",
    "\n",
    "# Get the attribute counts for non-romance movies\n",
    "attribute_counts = get_attribute_counts(attributes)\n",
    "# Get the attribute counts for romance movies\n",
    "attribute_counts_rom = get_attribute_counts(attributes_rom)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the attributes that appear most often \n",
    "def get_most_common_attributes(attribute_counts, n_attributes):\n",
    "    # Sort the attribute counts from highest to lowest\n",
    "    sorted_attributes = sorted(attribute_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    # Get the top n_attributes\n",
    "    most_common_attributes = sorted_attributes[:n_attributes]\n",
    "    return most_common_attributes\n",
    "\n",
    "# Get the most common attributes for non-romance movies\n",
    "most_common_attributes = get_most_common_attributes(attribute_counts, 10)\n",
    "# Get the most common attributes for romance movies\n",
    "most_common_attributes_rom = get_most_common_attributes(attribute_counts_rom, 10)\n",
    "\n",
    "# Plot the 10 most common attributes for non-romance movies in a seaborn barplot, do the same for romance movies, plot side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "sns.barplot(x=[attribute[0] for attribute in most_common_attributes], y=[attribute[1] for attribute in most_common_attributes], ax=ax1)\n",
    "sns.barplot(x=[attribute[0] for attribute in most_common_attributes_rom], y=[attribute[1] for attribute in most_common_attributes_rom], ax=ax2)\n",
    "# shared title\n",
    "fig.suptitle('10 most common attributes in non-romance and romance movies')\n",
    "# titles for the subplots\n",
    "ax1.set_title('Non-romance movies')\n",
    "ax2.set_title('Romance movies')\n",
    "# log scale for y axis\n",
    "ax1.set_yscale('log')\n",
    "ax2.set_yscale('log')\n",
    "# pastel palette\n",
    "sns.set_palette(\"pastel\")\n",
    "# rotate x labels\n",
    "plt.setp(ax1.get_xticklabels(), rotation=90)\n",
    "plt.setp(ax2.get_xticklabels(), rotation=90)\n",
    "# y labels\n",
    "ax1.set_ylabel('Number of appearances')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# for all attributes, lemmatize the words\n",
    "def lemmatize_attributes(attributes):\n",
    "    lemmatized_attributes = []\n",
    "    for attribute in attributes:\n",
    "        lemmatized_attributes.append(lemmatizer.lemmatize(attribute))\n",
    "    return lemmatized_attributes\n",
    "\n",
    "def lemmatize_verbs(verbs):\n",
    "    lemmatized_verbs = []\n",
    "    for verb in verbs:\n",
    "        lemmatized_verbs.append(lemmatizer.lemmatize(verb, 'v'))\n",
    "    return lemmatized_verbs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert column attributes to list\n",
    "full_char['attributes'] = full_char['attributes'].apply(lambda x: x if pd.isnull(x) else x[1:-1].replace(\"'\", \"\").replace(\" \", \"\").split(','))\n",
    "# convert column agent_verbs to list\n",
    "full_char['agent_verbs'] = full_char['agent_verbs'].apply(lambda x: x if pd.isnull(x) else x[1:-1].replace(\"'\", \"\").replace(\" \", \"\").split(','))\n",
    "# convert column patient_verbs to list\n",
    "full_char['patient_verbs'] = full_char['patient_verbs'].apply(lambda x: x if pd.isnull(x) else x[1:-1].replace(\"'\", \"\").replace(\" \", \"\").split(','))\n",
    "\n",
    "# same for romance movies\n",
    "full_char_rom['attributes'] = full_char_rom['attributes'].apply(lambda x: x if pd.isnull(x) else x[1:-1].replace(\"'\", \"\").replace(\" \", \"\").split(','))\n",
    "full_char_rom['agent_verbs'] = full_char_rom['agent_verbs'].apply(lambda x: x if pd.isnull(x) else x[1:-1].replace(\"'\", \"\").replace(\" \", \"\").split(','))\n",
    "full_char_rom['patient_verbs'] = full_char_rom['patient_verbs'].apply(lambda x: x if pd.isnull(x) else x[1:-1].replace(\"'\", \"\").replace(\" \", \"\").split(','))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize agent verbs and patient verbs if type is list\n",
    "full_char['agent_verbs'] = full_char['agent_verbs'].apply(lambda x: lemmatize_verbs(x) if type(x) == list else x)\n",
    "full_char['patient_verbs'] = full_char['patient_verbs'].apply(lambda x: lemmatize_verbs(x) if type(x) == list else x)\n",
    "full_char['attributes'] = full_char['attributes'].apply(lambda x: lemmatize_attributes(x) if type(x) == list else x)\n",
    "\n",
    "# same for romance\n",
    "full_char_rom['attributes'] = full_char_rom['attributes'].apply(lambda x: x if type(x) == list else x)\n",
    "full_char_rom['agent_verbs'] = full_char_rom['agent_verbs'].apply(lambda x: x if type(x) == list else x)\n",
    "full_char_rom['patient_verbs'] = full_char_rom['patient_verbs'].apply(lambda x: x if type(x) == list else x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find 15 most frequent agent_verbs, patient_verbs and attributes\n",
    "agent_verbs = full_char['agent_verbs']\n",
    "patient_verbs = full_char['patient_verbs']\n",
    "attributes = full_char['attributes']\n",
    "agent_verbs_rom = full_char_rom['agent_verbs']\n",
    "patient_verbs_rom = full_char_rom['patient_verbs']\n",
    "attributes_rom = full_char_rom['attributes']\n",
    "\n",
    "# Get a dictionary with the agent verbs as keys and the number of times they appear as values\n",
    "def get_agent_verb_counts(agent_verbs):\n",
    "    agent_verb_counts = {}\n",
    "    for agent_verb_list in agent_verbs:\n",
    "        # check if the agent verb list is not NaN\n",
    "        if type(agent_verb_list) == list:\n",
    "            # iterate over the agent verbs in the list\n",
    "            for agent_verb in agent_verb_list:\n",
    "                if agent_verb in agent_verb_counts:\n",
    "                    agent_verb_counts[agent_verb] += 1\n",
    "                else:\n",
    "                    agent_verb_counts[agent_verb] = 1\n",
    "    return agent_verb_counts\n",
    "\n",
    "# Get the agent verb counts for non-romance movies\n",
    "agent_verb_counts = get_agent_verb_counts(agent_verbs)\n",
    "# Get the agent verb counts for romance movies\n",
    "agent_verb_counts_rom = get_agent_verb_counts(agent_verbs_rom)\n",
    "\n",
    "# print the top 10\n",
    "print('The top 10 agent verbs in non-romance movies are:')\n",
    "print(get_most_common_attributes(agent_verb_counts, 10))\n",
    "print('The top 10 agent verbs in romance movies are:')\n",
    "print(get_most_common_attributes(agent_verb_counts_rom, 10))\n",
    "\n",
    "# Get a dictionary with the patient verbs as keys and the number of times they appear as values\n",
    "def get_patient_verb_counts(patient_verbs):\n",
    "    patient_verb_counts = {}\n",
    "    for patient_verb_list in patient_verbs:\n",
    "        # check if the patient verb list is not NaN\n",
    "        if type(patient_verb_list) == list:\n",
    "            # iterate over the patient verbs in the list\n",
    "            for patient_verb in patient_verb_list:\n",
    "                if patient_verb in patient_verb_counts:\n",
    "                    patient_verb_counts[patient_verb] += 1\n",
    "                else:\n",
    "                    patient_verb_counts[patient_verb] = 1\n",
    "    return patient_verb_counts\n",
    "\n",
    "# Get the patient verb counts for non-romance movies\n",
    "patient_verb_counts = get_patient_verb_counts(patient_verbs)\n",
    "# Get the patient verb counts for romance movies\n",
    "patient_verb_counts_rom = get_patient_verb_counts(patient_verbs_rom)\n",
    "\n",
    "# print the top 10\n",
    "print('The top 10 patient verbs in non-romance movies are:')\n",
    "print(get_most_common_attributes(patient_verb_counts, 10))\n",
    "print('The top 10 patient verbs in romance movies are:')\n",
    "print(get_most_common_attributes(patient_verb_counts_rom, 10))\n",
    "\n",
    "# Get a dictionary with the attributes as keys and the number of times they appear as values\n",
    "def get_attribute_counts(attributes):\n",
    "    attribute_counts = {}\n",
    "    for attribute_list in attributes:\n",
    "        # check if the attribute list is not NaN\n",
    "        if type(attribute_list) == list:\n",
    "            # iterate over the attributes in the list\n",
    "            for attribute in attribute_list:\n",
    "                if attribute in attribute_counts:\n",
    "                    attribute_counts[attribute] += 1\n",
    "                else:\n",
    "                    attribute_counts[attribute] = 1\n",
    "    return attribute_counts\n",
    "\n",
    "# Get the attribute counts for non-romance movies\n",
    "attribute_counts = get_attribute_counts(attributes)\n",
    "# Get the attribute counts for romance movies\n",
    "attribute_counts_rom = get_attribute_counts(attributes_rom)\n",
    "\n",
    "# print the top 10\n",
    "print('The top 10 attributes in non-romance movies are:')\n",
    "print(get_most_common_attributes(attribute_counts, 10))\n",
    "print('The top 10 attributes in romance movies are:')\n",
    "print(get_most_common_attributes(attribute_counts_rom, 10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLD CODE FROM HERE ON OUT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the clusters in a 3d diagram\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Reduce the dimensionality of the embeddings to 3, store each coordinate in a column\n",
    "tsne = TSNE(n_components=3, random_state=0)\n",
    "title_embeddings['X'] = tsne.fit_transform(title_embeddings['Embedding'].tolist())[:,0]\n",
    "title_embeddings['Y'] = tsne.fit_transform(title_embeddings['Embedding'].tolist())[:,1]\n",
    "title_embeddings['Z'] = tsne.fit_transform(title_embeddings['Embedding'].tolist())[:,2]\n",
    "\n",
    "# Plot the clusters in a 3d diagram\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(title_embeddings['X'], title_embeddings['Y'], title_embeddings['Z'], c=title_embeddings['Cluster'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print 5 titles from each cluster\n",
    "for i in range(24):\n",
    "    print('Cluster {}:'.format(i))\n",
    "    print(title_embeddings[title_embeddings['Cluster'] == i]['Title'].sample(5).values)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 10 most common character roles \n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "sns.countplot(x='Relation', data=title_df, order=title_df.groupby(['Relation']).count().sort_values(by = 'Wikipedia ID', ascending=False).head(10).index, ax=ax)\n",
    "\n",
    "ax.set_title('Most common character role in romance movies')\n",
    "ax.set_ylabel('Number of characters')\n",
    "xlabels = ['{}'.format(x) for x in title_df.groupby(['Relation']).count().sort_values(by = 'Wikipedia ID', ascending=False).head(10).index]\n",
    "ax.set_xticklabels(xlabels)\n",
    "ax.set_xlabel('')\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('flare')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
