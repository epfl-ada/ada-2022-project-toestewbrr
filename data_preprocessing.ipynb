{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "AYC13i4R39dN"
   },
   "source": [
    "# Applied Data Analysis Project\n",
    "**Team**: ToeStewBrr - Alexander Sternfeld, Marguerite Thery, Antoine Bonnet, Hugo Bordereaux\n",
    "\n",
    "**Dataset**: CMU Movie Summary Corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "MlSSpJAG39dP"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import tarfile\n",
    "import urllib\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TadIrDF39dR"
   },
   "source": [
    "## 1. Loading data\n",
    "\n",
    "We first extract all files from the [MoviesSummaries dataset](http://www.cs.cmu.edu/~ark/personas/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('Data/movieSummaries'):\n",
    "    filename = 'http://www.cs.cmu.edu/~ark/personas/data/MovieSummaries.tar.gz'\n",
    "    my_tar = tarfile.open(fileobj=urllib.request.urlopen(filename), mode=\"r:gz\") \n",
    "    my_tar.extractall('./Data') # specify which folder to extract to\n",
    "    my_tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "r1zL8zJr39dR"
   },
   "outputs": [],
   "source": [
    "# Temporary configuration: Until we figure out how to extract the tar file directly from the url, please download the file and place it in the Data directory of this project. \n",
    "#if not os.path.exists('Data/MovieSummaries'):\n",
    "#    my_tar = tarfile.open('Data/MovieSummaries.tar') # Can't upload voluminous data to Github\n",
    "#    my_tar.extractall('./Data') # specify which folder to extract to\n",
    "#    my_tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoIsNdG539dR"
   },
   "source": [
    "## 2. Pre-processing data\n",
    "\n",
    "### 1. Plot summaries\n",
    "\n",
    "`plot_summaries.txt [29 M]`: Plot summaries of 42,306 movies extracted from the November 2, 2012 dump of English-language Wikipedia.  Each line contains the Wikipedia movie ID (which indexes into movie.metadata.tsv) followed by the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "z6AUVYUc39dS",
    "outputId": "f909218b-7eaa-40d6-faee-e6937677b4c9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wikipedia ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23890098</th>\n",
       "      <td>Shlykov, a hard-working taxi driver and Lyosha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31186339</th>\n",
       "      <td>The nation of Panem consists of a wealthy Capi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20663735</th>\n",
       "      <td>Poovalli Induchoodan  is sentenced for six yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2231378</th>\n",
       "      <td>The Lemon Drop Kid , a New York City swindler,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595909</th>\n",
       "      <td>Seventh-day Adventist Church pastor Michael Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34808485</th>\n",
       "      <td>The story is about Reema , a young Muslim scho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096473</th>\n",
       "      <td>In 1928 Hollywood, director Leo Andreyev  look...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35102018</th>\n",
       "      <td>American Luthier focuses on Randy Parsons’ tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8628195</th>\n",
       "      <td>Abdur Rehman Khan , a middle-aged dry fruit se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6040782</th>\n",
       "      <td>1940 - Operation Dynamo has just taken place. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42303 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        Summary\n",
       "Wikipedia ID                                                   \n",
       "23890098      Shlykov, a hard-working taxi driver and Lyosha...\n",
       "31186339      The nation of Panem consists of a wealthy Capi...\n",
       "20663735      Poovalli Induchoodan  is sentenced for six yea...\n",
       "2231378       The Lemon Drop Kid , a New York City swindler,...\n",
       "595909        Seventh-day Adventist Church pastor Michael Ch...\n",
       "...                                                         ...\n",
       "34808485      The story is about Reema , a young Muslim scho...\n",
       "1096473       In 1928 Hollywood, director Leo Andreyev  look...\n",
       "35102018      American Luthier focuses on Randy Parsons’ tra...\n",
       "8628195       Abdur Rehman Khan , a middle-aged dry fruit se...\n",
       "6040782       1940 - Operation Dynamo has just taken place. ...\n",
       "\n",
       "[42303 rows x 1 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_path = 'Data/MovieSummaries/plot_summaries.txt'\n",
    "plot_cols = ['Wikipedia ID', 'Summary']\n",
    "plot_df = pd.read_csv(plot_path, sep='\\t', header=None, names=plot_cols, index_col=0)\n",
    "plot_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8vh3IEW39dS"
   },
   "source": [
    "### 2. Movie metadata\n",
    "\n",
    "`movie.metadata.tsv.gz [3.4 M]`: Metadata for 81,741 movies, extracted from the Noverber 4, 2012 dump of Freebase.  Tab-separated; columns:\n",
    "\n",
    "1. Wikipedia movie ID\n",
    "2. Freebase movie ID\n",
    "3. Movie name\n",
    "4. Movie release date\n",
    "5. Movie box office revenue\n",
    "6. Movie runtime\n",
    "7. Movie languages (Freebase ID:name tuples)\n",
    "8. Movie countries (Freebase ID:name tuples)\n",
    "9. Movie genres (Freebase ID:name tuples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_encoding = lambda x: x.split(':', 1)[1].split('\"')[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 898
    },
    "id": "p5NKaQHm39dT",
    "outputId": "3b973e13-6efd-4c48-95a7-0346d5ec753c"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data type '{\"/m/09c7w0\": \"United States of America\"}' not understood",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [38], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m movie_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mData/MovieSummaries/movie.metadata.tsv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      2\u001b[0m movie_cols \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mWikipedia ID\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mFreebase ID\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mName\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mRelease date\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m      3\u001b[0m               \u001b[39m'\u001b[39m\u001b[39mBox office revenue\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mRuntime\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLanguages\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCountries\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mGenres\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m movie_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(movie_path, sep\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m'\u001b[39;49m, header\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, names\u001b[39m=\u001b[39;49mmovie_cols, index_col\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, dtype \u001b[39m=\u001b[39;49m {\u001b[39m'\u001b[39;49m\u001b[39mFreebase ID\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39mstr\u001b[39;49m})\n\u001b[1;32m      5\u001b[0m \u001b[39m#movie_df['Languages'] = movie_df['Languages'].apply(strip_encoding)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m movie_df\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/util/_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    312\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    313\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    314\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    315\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(inspect\u001b[39m.\u001b[39mcurrentframe()),\n\u001b[1;32m    316\u001b[0m     )\n\u001b[0;32m--> 317\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1747\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1744\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1746\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m mapping[engine](f, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions)\n\u001b[1;32m   1748\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1749\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/c_parser_wrapper.py:91\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m (\n\u001b[1;32m     82\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mstorage_options\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     83\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mwarn_bad_lines\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     88\u001b[0m ):\n\u001b[1;32m     89\u001b[0m     kwds\u001b[39m.\u001b[39mpop(key, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m---> 91\u001b[0m kwds[\u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m ensure_dtype_objs(kwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mdtype\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m))\n\u001b[1;32m     92\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader \u001b[39m=\u001b[39m parsers\u001b[39m.\u001b[39mTextReader(src, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munnamed_cols \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader\u001b[39m.\u001b[39munnamed_cols\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/c_parser_wrapper.py:448\u001b[0m, in \u001b[0;36mensure_dtype_objs\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[39mreturn\u001b[39;00m dtype_converted\n\u001b[1;32m    447\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, \u001b[39mdict\u001b[39m):\n\u001b[0;32m--> 448\u001b[0m     \u001b[39mreturn\u001b[39;00m {k: pandas_dtype(dtype[k]) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m dtype}\n\u001b[1;32m    449\u001b[0m \u001b[39melif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m pandas_dtype(dtype)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/c_parser_wrapper.py:448\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[39mreturn\u001b[39;00m dtype_converted\n\u001b[1;32m    447\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, \u001b[39mdict\u001b[39m):\n\u001b[0;32m--> 448\u001b[0m     \u001b[39mreturn\u001b[39;00m {k: pandas_dtype(dtype[k]) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m dtype}\n\u001b[1;32m    449\u001b[0m \u001b[39melif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m pandas_dtype(dtype)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/dtypes/common.py:1782\u001b[0m, in \u001b[0;36mpandas_dtype\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[39m# try a numpy dtype\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[39m# raise a consistent TypeError if failed\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1782\u001b[0m     npdtype \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdtype(dtype)\n\u001b[1;32m   1783\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mSyntaxError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m   1784\u001b[0m     \u001b[39m# np.dtype uses `eval` which can raise SyntaxError\u001b[39;00m\n\u001b[1;32m   1785\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdata type \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdtype\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not understood\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: data type '{\"/m/09c7w0\": \"United States of America\"}' not understood"
     ]
    }
   ],
   "source": [
    "movie_path = 'Data/MovieSummaries/movie.metadata.tsv'\n",
    "movie_cols = ['Wikipedia ID', 'Freebase ID', 'Name', 'Release date', \n",
    "              'Box office revenue', 'Runtime', 'Languages', 'Countries', 'Genres']\n",
    "movie_df = pd.read_csv(movie_path, sep='\\t', header=None, names=movie_cols, index_col=0, dtype = {'Freebase ID': str})\n",
    "movie_df[['Languages', 'Countries', 'Genres']] = movie_df[['Languages', 'Countries', 'Genres']].apply(strip_encoding)\n",
    "movie_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMGsQrxV39dT"
   },
   "source": [
    "### 3. Character metadata\n",
    "\n",
    "`character.metadata.tsv.gz [14 M]`: Metadata for 450,669 characters aligned to the movies above, extracted from the November 4, 2012 dump of Freebase.  Tab-separated; columns:\n",
    "\n",
    "1. Wikipedia movie ID\n",
    "2. Freebase movie ID\n",
    "3. Movie release date\n",
    "4. Character name\n",
    "5. Actor date of birth\n",
    "6. Actor gender\n",
    "7. Actor height (in meters)\n",
    "8. Actor ethnicity (Freebase ID)\n",
    "9. Actor name\n",
    "10. Actor age at movie release\n",
    "11. Freebase character/actor map ID\n",
    "12. Freebase character ID\n",
    "13. Freebase actor ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 762
    },
    "id": "7xdjzblS39dT",
    "outputId": "9c609dff-a8d6-495a-f3c5-7e12ffb2639d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Freebase ID</th>\n",
       "      <th>Release date</th>\n",
       "      <th>Character name</th>\n",
       "      <th>Date of birth</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Actor name</th>\n",
       "      <th>Actor age at release</th>\n",
       "      <th>Freebase character/map ID</th>\n",
       "      <th>Freebase character ID</th>\n",
       "      <th>Freebase actor ID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wikipedia ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>975900</th>\n",
       "      <td>/m/03vyhn</td>\n",
       "      <td>2001-08-24</td>\n",
       "      <td>Akooshay</td>\n",
       "      <td>1958-08-26</td>\n",
       "      <td>F</td>\n",
       "      <td>1.620</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wanda De Jesus</td>\n",
       "      <td>42.0</td>\n",
       "      <td>/m/0bgchxw</td>\n",
       "      <td>/m/0bgcj3x</td>\n",
       "      <td>/m/03wcfv7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975900</th>\n",
       "      <td>/m/03vyhn</td>\n",
       "      <td>2001-08-24</td>\n",
       "      <td>Lieutenant Melanie Ballard</td>\n",
       "      <td>1974-08-15</td>\n",
       "      <td>F</td>\n",
       "      <td>1.780</td>\n",
       "      <td>/m/044038p</td>\n",
       "      <td>Natasha Henstridge</td>\n",
       "      <td>27.0</td>\n",
       "      <td>/m/0jys3m</td>\n",
       "      <td>/m/0bgchn4</td>\n",
       "      <td>/m/0346l4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975900</th>\n",
       "      <td>/m/03vyhn</td>\n",
       "      <td>2001-08-24</td>\n",
       "      <td>Desolation Williams</td>\n",
       "      <td>1969-06-15</td>\n",
       "      <td>M</td>\n",
       "      <td>1.727</td>\n",
       "      <td>/m/0x67</td>\n",
       "      <td>Ice Cube</td>\n",
       "      <td>32.0</td>\n",
       "      <td>/m/0jys3g</td>\n",
       "      <td>/m/0bgchn_</td>\n",
       "      <td>/m/01vw26l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975900</th>\n",
       "      <td>/m/03vyhn</td>\n",
       "      <td>2001-08-24</td>\n",
       "      <td>Sgt Jericho Butler</td>\n",
       "      <td>1967-09-12</td>\n",
       "      <td>M</td>\n",
       "      <td>1.750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jason Statham</td>\n",
       "      <td>33.0</td>\n",
       "      <td>/m/02vchl6</td>\n",
       "      <td>/m/0bgchnq</td>\n",
       "      <td>/m/034hyc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975900</th>\n",
       "      <td>/m/03vyhn</td>\n",
       "      <td>2001-08-24</td>\n",
       "      <td>Bashira Kincaid</td>\n",
       "      <td>1977-09-25</td>\n",
       "      <td>F</td>\n",
       "      <td>1.650</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Clea DuVall</td>\n",
       "      <td>23.0</td>\n",
       "      <td>/m/02vbb3r</td>\n",
       "      <td>/m/0bgchp9</td>\n",
       "      <td>/m/01y9xg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913762</th>\n",
       "      <td>/m/03pcrp</td>\n",
       "      <td>1992-05-21</td>\n",
       "      <td>Elensh</td>\n",
       "      <td>1970-05</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dorothy Elias-Fahn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/m/0kr406c</td>\n",
       "      <td>/m/0kr406h</td>\n",
       "      <td>/m/0b_vcv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913762</th>\n",
       "      <td>/m/03pcrp</td>\n",
       "      <td>1992-05-21</td>\n",
       "      <td>Hibiki</td>\n",
       "      <td>1965-04-12</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jonathan Fahn</td>\n",
       "      <td>27.0</td>\n",
       "      <td>/m/0kr405_</td>\n",
       "      <td>/m/0kr4090</td>\n",
       "      <td>/m/0bx7_j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28308153</th>\n",
       "      <td>/m/0cp05t9</td>\n",
       "      <td>1957</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1941-11-18</td>\n",
       "      <td>M</td>\n",
       "      <td>1.730</td>\n",
       "      <td>/m/02w7gg</td>\n",
       "      <td>David Hemmings</td>\n",
       "      <td>15.0</td>\n",
       "      <td>/m/0g8ngmc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/m/022g44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28308153</th>\n",
       "      <td>/m/0cp05t9</td>\n",
       "      <td>1957</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Roberta Paterson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/m/0g8ngmj</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/m/0g8ngmm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28308153</th>\n",
       "      <td>/m/0cp05t9</td>\n",
       "      <td>1957</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Rogers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/m/0g8ngmw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/m/0btz19d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>450669 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Freebase ID Release date              Character name  \\\n",
       "Wikipedia ID                                                        \n",
       "975900         /m/03vyhn   2001-08-24                    Akooshay   \n",
       "975900         /m/03vyhn   2001-08-24  Lieutenant Melanie Ballard   \n",
       "975900         /m/03vyhn   2001-08-24         Desolation Williams   \n",
       "975900         /m/03vyhn   2001-08-24          Sgt Jericho Butler   \n",
       "975900         /m/03vyhn   2001-08-24             Bashira Kincaid   \n",
       "...                  ...          ...                         ...   \n",
       "913762         /m/03pcrp   1992-05-21                      Elensh   \n",
       "913762         /m/03pcrp   1992-05-21                      Hibiki   \n",
       "28308153      /m/0cp05t9         1957                         NaN   \n",
       "28308153      /m/0cp05t9         1957                         NaN   \n",
       "28308153      /m/0cp05t9         1957                         NaN   \n",
       "\n",
       "             Date of birth Gender  Height   Ethnicity          Actor name  \\\n",
       "Wikipedia ID                                                                \n",
       "975900          1958-08-26      F   1.620         NaN      Wanda De Jesus   \n",
       "975900          1974-08-15      F   1.780  /m/044038p  Natasha Henstridge   \n",
       "975900          1969-06-15      M   1.727     /m/0x67            Ice Cube   \n",
       "975900          1967-09-12      M   1.750         NaN       Jason Statham   \n",
       "975900          1977-09-25      F   1.650         NaN         Clea DuVall   \n",
       "...                    ...    ...     ...         ...                 ...   \n",
       "913762             1970-05      F     NaN         NaN  Dorothy Elias-Fahn   \n",
       "913762          1965-04-12      M     NaN         NaN       Jonathan Fahn   \n",
       "28308153        1941-11-18      M   1.730   /m/02w7gg      David Hemmings   \n",
       "28308153               NaN    NaN     NaN         NaN    Roberta Paterson   \n",
       "28308153               NaN    NaN     NaN         NaN         John Rogers   \n",
       "\n",
       "              Actor age at release Freebase character/map ID  \\\n",
       "Wikipedia ID                                                   \n",
       "975900                        42.0                /m/0bgchxw   \n",
       "975900                        27.0                 /m/0jys3m   \n",
       "975900                        32.0                 /m/0jys3g   \n",
       "975900                        33.0                /m/02vchl6   \n",
       "975900                        23.0                /m/02vbb3r   \n",
       "...                            ...                       ...   \n",
       "913762                         NaN                /m/0kr406c   \n",
       "913762                        27.0                /m/0kr405_   \n",
       "28308153                      15.0                /m/0g8ngmc   \n",
       "28308153                       NaN                /m/0g8ngmj   \n",
       "28308153                       NaN                /m/0g8ngmw   \n",
       "\n",
       "             Freebase character ID Freebase actor ID  \n",
       "Wikipedia ID                                          \n",
       "975900                  /m/0bgcj3x        /m/03wcfv7  \n",
       "975900                  /m/0bgchn4         /m/0346l4  \n",
       "975900                  /m/0bgchn_        /m/01vw26l  \n",
       "975900                  /m/0bgchnq         /m/034hyc  \n",
       "975900                  /m/0bgchp9         /m/01y9xg  \n",
       "...                            ...               ...  \n",
       "913762                  /m/0kr406h         /m/0b_vcv  \n",
       "913762                  /m/0kr4090         /m/0bx7_j  \n",
       "28308153                       NaN         /m/022g44  \n",
       "28308153                       NaN        /m/0g8ngmm  \n",
       "28308153                       NaN        /m/0btz19d  \n",
       "\n",
       "[450669 rows x 12 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_path = 'Data/MovieSummaries/character.metadata.tsv'\n",
    "char_cols = ['Wikipedia ID', 'Freebase ID', 'Release date', 'Character name', 'Date of birth', \n",
    "             'Gender', 'Height', 'Ethnicity', 'Actor name', 'Actor age at release', \n",
    "             'Freebase character/map ID', 'Freebase character ID', 'Freebase actor ID']\n",
    "char_df = pd.read_csv(char_path, sep='\\t', header=None, names=char_cols, index_col=0)\n",
    "char_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifnIh1uE39dU"
   },
   "source": [
    "### 4. CoreNLP Plot Summaries\n",
    "\n",
    "`corenlp_plot_summaries.tar.gz [628 M, separate download]`: The plot summaries from above, run through the Stanford CoreNLP pipeline (tagging, parsing, NER and coref). Each filename begins with the Wikipedia movie ID (which indexes into movie.metadata.tsv).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m coreNLPfilename \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhttp://www.cs.cmu.edu/~ark/personas/data/corenlp_plot_summaries.tar\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      3\u001b[0m my_tar \u001b[39m=\u001b[39m tarfile\u001b[39m.\u001b[39mopen(fileobj\u001b[39m=\u001b[39murllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39murlopen(coreNLPfilename), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mr|\u001b[39m\u001b[39m\"\u001b[39m) \n\u001b[0;32m----> 4\u001b[0m my_tar\u001b[39m.\u001b[39;49mextractall(\u001b[39m'\u001b[39;49m\u001b[39m./Data/CoreNLP\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39m# specify which folder to extract to\u001b[39;00m\n\u001b[1;32m      5\u001b[0m my_tar\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/tarfile.py:2036\u001b[0m, in \u001b[0;36mTarFile.extractall\u001b[0;34m(self, path, members, numeric_owner)\u001b[0m\n\u001b[1;32m   2034\u001b[0m         tarinfo\u001b[39m.\u001b[39mmode \u001b[39m=\u001b[39m \u001b[39m0o700\u001b[39m\n\u001b[1;32m   2035\u001b[0m     \u001b[39m# Do not set_attrs directories, as we will do that further down\u001b[39;00m\n\u001b[0;32m-> 2036\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract(tarinfo, path, set_attrs\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m tarinfo\u001b[39m.\u001b[39;49misdir(),\n\u001b[1;32m   2037\u001b[0m                  numeric_owner\u001b[39m=\u001b[39;49mnumeric_owner)\n\u001b[1;32m   2039\u001b[0m \u001b[39m# Reverse sort directories.\u001b[39;00m\n\u001b[1;32m   2040\u001b[0m directories\u001b[39m.\u001b[39msort(key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m a: a\u001b[39m.\u001b[39mname)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/tarfile.py:2077\u001b[0m, in \u001b[0;36mTarFile.extract\u001b[0;34m(self, member, path, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2074\u001b[0m     tarinfo\u001b[39m.\u001b[39m_link_target \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path, tarinfo\u001b[39m.\u001b[39mlinkname)\n\u001b[1;32m   2076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2077\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extract_member(tarinfo, os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(path, tarinfo\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m   2078\u001b[0m                          set_attrs\u001b[39m=\u001b[39;49mset_attrs,\n\u001b[1;32m   2079\u001b[0m                          numeric_owner\u001b[39m=\u001b[39;49mnumeric_owner)\n\u001b[1;32m   2080\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   2081\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrorlevel \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/tarfile.py:2150\u001b[0m, in \u001b[0;36mTarFile._extract_member\u001b[0;34m(self, tarinfo, targetpath, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dbg(\u001b[39m1\u001b[39m, tarinfo\u001b[39m.\u001b[39mname)\n\u001b[1;32m   2149\u001b[0m \u001b[39mif\u001b[39;00m tarinfo\u001b[39m.\u001b[39misreg():\n\u001b[0;32m-> 2150\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmakefile(tarinfo, targetpath)\n\u001b[1;32m   2151\u001b[0m \u001b[39melif\u001b[39;00m tarinfo\u001b[39m.\u001b[39misdir():\n\u001b[1;32m   2152\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmakedir(tarinfo, targetpath)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/tarfile.py:2199\u001b[0m, in \u001b[0;36mTarFile.makefile\u001b[0;34m(self, tarinfo, targetpath)\u001b[0m\n\u001b[1;32m   2197\u001b[0m     target\u001b[39m.\u001b[39mtruncate()\n\u001b[1;32m   2198\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2199\u001b[0m     copyfileobj(source, target, tarinfo\u001b[39m.\u001b[39;49msize, ReadError, bufsize)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/tarfile.py:253\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[0;34m(src, dst, length, exception, bufsize)\u001b[0m\n\u001b[1;32m    250\u001b[0m     dst\u001b[39m.\u001b[39mwrite(buf)\n\u001b[1;32m    252\u001b[0m \u001b[39mif\u001b[39;00m remainder \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 253\u001b[0m     buf \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39;49mread(remainder)\n\u001b[1;32m    254\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(buf) \u001b[39m<\u001b[39m remainder:\n\u001b[1;32m    255\u001b[0m         \u001b[39mraise\u001b[39;00m exception(\u001b[39m\"\u001b[39m\u001b[39munexpected end of data\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/tarfile.py:521\u001b[0m, in \u001b[0;36m_Stream.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[39m\"\"\"Return the next size number of bytes from the stream.\"\"\"\u001b[39;00m\n\u001b[1;32m    520\u001b[0m \u001b[39massert\u001b[39;00m size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 521\u001b[0m buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read(size)\n\u001b[1;32m    522\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(buf)\n\u001b[1;32m    523\u001b[0m \u001b[39mreturn\u001b[39;00m buf\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/tarfile.py:529\u001b[0m, in \u001b[0;36m_Stream._read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[39m\"\"\"Return size bytes from the stream.\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomptype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtar\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 529\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__read(size)\n\u001b[1;32m    531\u001b[0m c \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdbuf)\n\u001b[1;32m    532\u001b[0m t \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdbuf]\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/tarfile.py:559\u001b[0m, in \u001b[0;36m_Stream.__read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    557\u001b[0m t \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf]\n\u001b[1;32m    558\u001b[0m \u001b[39mwhile\u001b[39;00m c \u001b[39m<\u001b[39m size:\n\u001b[0;32m--> 559\u001b[0m     buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfileobj\u001b[39m.\u001b[39;49mread(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbufsize)\n\u001b[1;32m    560\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m buf:\n\u001b[1;32m    561\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py:459\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    457\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 459\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    461\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py:503\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    498\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[1;32m    500\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 503\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[1;32m    505\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    506\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    507\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not os.path.exists('Data/CoreNLP'):\n",
    "    coreNLPfilename = 'http://www.cs.cmu.edu/~ark/personas/data/corenlp_plot_summaries.tar'\n",
    "    my_tar = tarfile.open(fileobj=urllib.request.urlopen(coreNLPfilename), mode=\"r|\") \n",
    "    my_tar.extractall(path='./Data/CoreNLP') # specify which folder to extract to\n",
    "    my_tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "0FrHh7wN39dU",
    "outputId": "4258b30a-cb08-45ae-86c3-744e9633ffc4"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (774559364.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [10], line 14\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "#NER tag = person can give us the characters mention in the plot summary. \n",
    "\n",
    "import gzip\n",
    "import xml.etree.ElementTree as ET\n",
    "directory = './corenlp_plot_summaries'\n",
    "#TODO: Need to extract each file and convert them to xml \n",
    "#For loop to open every file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "  f = os.path.join(directory, filename) \n",
    "  if os.path.isfile(f):\n",
    "    #open and store file as xml \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gfYMmpwSGxMD",
    "outputId": "c45a3a93-e2ae-4785-f362-814836b0be1a"
   },
   "outputs": [],
   "source": [
    "#Use file I already extracted on my computer to run some tests\n",
    "tree = ET.parse('3217.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "print(len(root.findall('.//*governor'))) #use parse or basic-dependencies to have more info \n",
    "#print(root.findall('.//*governor').text())\n",
    "for l in root.findall('.//*NER'): \n",
    "  if len(l.text) > 1:\n",
    "    print(l.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BHCfeenl6-3"
   },
   "source": [
    "### 5. TV Tropes Clusters\n",
    "\n",
    "We reformat the file `tvtropes.clusters.txt` so it is easier to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "id": "25-cuhaOhxw9",
    "outputId": "40ae44d8-796f-48e2-8031-d686eea18f07"
   },
   "outputs": [],
   "source": [
    "path = 'Data/MovieSummaries/'\n",
    "cluster_path = path+'tvtropes.clusters.format.txt'\n",
    "cluster_cols = ['Cluster', 'Character name', 'Movie', 'Freebase character/map ID', 'Actor']\n",
    "cluster_df = pd.read_csv(cluster_path, sep=',', header=None, names=cluster_cols, dtype = {'Freebase ID': str})\n",
    "cluster_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFbHAkx-mHwA"
   },
   "source": [
    "### 6. Join cluster and movies\n",
    "\n",
    " We now join the TV tropes clusters with movie.metadata so we are able to access movie genre and filter on romance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1J_1KXB40qbR",
    "outputId": "02b73165-d4d5-4d24-e13f-1b8c8e5b7560"
   },
   "outputs": [],
   "source": [
    "cluster_char = cluster_df.merge(char_df, on='Freebase character/map ID')\n",
    "cluster_char_movie = cluster_char.merge(movie_df, on='Freebase ID')\n",
    "romance_cluster = cluster_char_movie[cluster_char_movie['Genres'].apply(lambda x: 'Roman' in x)]\n",
    "romance_cluster.groupby(romance_cluster['Cluster']).size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
