{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "AYC13i4R39dN"
      },
      "source": [
        "# Applied Data Analysis Project\n",
        "**Team**: ToeStewBrr - Alexander Sternfeld, Marguerite Thery, Antoine Bonnet, Hugo Bordereaux\n",
        "\n",
        "**Dataset**: CMU Movie Summary Corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlSSpJAG39dP"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import tarfile\n",
        "import urllib\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "import re\n",
        "import gzip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TadIrDF39dR"
      },
      "source": [
        "## 1. Loading data\n",
        "\n",
        "We first extract all files from the [MoviesSummaries dataset](http://www.cs.cmu.edu/~ark/personas/). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uUOaU9hTpGn"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('Data/MovieSummaries'):\n",
        "    filename = 'http://www.cs.cmu.edu/~ark/personas/data/MovieSummaries.tar.gz'\n",
        "    my_tar = tarfile.open(fileobj=urllib.request.urlopen(filename), mode=\"r:gz\") \n",
        "    my_tar.extractall('./Data') # specify which folder to extract to\n",
        "    my_tar.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoIsNdG539dR"
      },
      "source": [
        "## 2. Pre-processing data\n",
        "\n",
        "### 1. Plot summaries\n",
        "\n",
        "`plot_summaries.txt [29 M]`: Plot summaries of 42,306 movies extracted from the November 2, 2012 dump of English-language Wikipedia.  Each line contains the Wikipedia movie ID (which indexes into movie.metadata.tsv) followed by the summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6AUVYUc39dS"
      },
      "outputs": [],
      "source": [
        "plot_path = 'Data/MovieSummaries/plot_summaries.txt'\n",
        "plot_cols = ['Wikipedia ID', 'Summary']\n",
        "plot_df = pd.read_csv(plot_path, sep='\\t', header=None, names=plot_cols, index_col=0)\n",
        "plot_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8vh3IEW39dS"
      },
      "source": [
        "### 2. Movie metadata\n",
        "\n",
        "`movie.metadata.tsv.gz [3.4 M]`: Metadata for 81,741 movies, extracted from the Noverber 4, 2012 dump of Freebase.  Tab-separated; columns:\n",
        "\n",
        "1. Wikipedia movie ID\n",
        "2. Freebase movie ID\n",
        "3. Movie name\n",
        "4. Movie release date\n",
        "5. Movie box office revenue\n",
        "6. Movie runtime\n",
        "7. Movie languages (Freebase ID:name tuples)\n",
        "8. Movie countries (Freebase ID:name tuples)\n",
        "9. Movie genres (Freebase ID:name tuples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJQlI3uATpGr"
      },
      "outputs": [],
      "source": [
        "strip_encoding = lambda x: np.nan if x == '{}' else \\\n",
        "    [w.replace(' Language', '').replace(' language', '') for w in re.findall(r'\"(.*?)\"', x)[1::2]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5NKaQHm39dT"
      },
      "outputs": [],
      "source": [
        "movie_path = 'Data/MovieSummaries/movie.metadata.tsv'\n",
        "movie_cols = ['Wikipedia ID', 'Freebase ID', 'Name', 'Release date', \n",
        "              'Box office revenue', 'Runtime', 'Languages', 'Countries', 'Genres']\n",
        "movie_df = pd.read_csv(movie_path, sep='\\t', header=None, names=movie_cols, index_col=0, dtype = {'Freebase ID': str})\n",
        "movie_df['Languages'] = movie_df['Languages'].apply(strip_encoding)\n",
        "movie_df['Countries'] = movie_df['Countries'].apply(strip_encoding)\n",
        "movie_df['Genres'] = movie_df['Genres'].apply(strip_encoding)\n",
        "movie_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMGsQrxV39dT"
      },
      "source": [
        "### 3. Character metadata\n",
        "\n",
        "`character.metadata.tsv.gz [14 M]`: Metadata for 450,669 characters aligned to the movies above, extracted from the November 4, 2012 dump of Freebase.  Tab-separated; columns:\n",
        "\n",
        "1. Wikipedia movie ID\n",
        "2. Freebase movie ID\n",
        "3. Movie release date\n",
        "4. Character name\n",
        "5. Actor date of birth\n",
        "6. Actor gender\n",
        "7. Actor height (in meters)\n",
        "8. Actor ethnicity (Freebase ID)\n",
        "9. Actor name\n",
        "10. Actor age at movie release\n",
        "11. Freebase character/actor map ID\n",
        "12. Freebase character ID\n",
        "13. Freebase actor ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987
        },
        "id": "7xdjzblS39dT",
        "outputId": "f807e6e8-c98f-45d4-eee0-4c1d90f41a9f"
      },
      "outputs": [],
      "source": [
        "char_path = 'Data/MovieSummaries/character.metadata.tsv'\n",
        "char_cols = ['Wikipedia ID', 'Freebase ID', 'Release date', 'Character name', 'Date of birth', \n",
        "             'Gender', 'Height', 'Ethnicity', 'Actor name', 'Actor age at release', \n",
        "             'Freebase character/map ID', 'Freebase character ID', 'Freebase actor ID']\n",
        "char_df = pd.read_csv(char_path, sep='\\t', header=None, names=char_cols, index_col=False)\n",
        "char_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "o_yhkYyITpGv"
      },
      "source": [
        "## General analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "JOPzR8qDTpGv"
      },
      "source": [
        "### -Analysis romantic genres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsEvIscITpGv"
      },
      "outputs": [],
      "source": [
        "##One notices that there are several types of romantic movies: romantic comedy, romance film, romantic drama\n",
        "romance_genres = ['Romantic comedy', 'Romance Film', 'Romantic drama', 'Romantic fantasy', 'Romantic thriller']\n",
        "romance_movies = movie_df[movie_df['Genres'].apply(lambda x: any(y in romance_genres for y in x) if type(x) == list else False)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zuWitV-TpGw"
      },
      "outputs": [],
      "source": [
        "#Organize by category\n",
        "romantic_comedy = romance_movies[movie_df['Genres'].apply(lambda x: any(y in romance_genres[0] for y in x) if type(x) == list else False)]\n",
        "romantic_film = romance_movies[movie_df['Genres'].apply(lambda x: any(y in romance_genres[1] for y in x) if type(x) == list else False)]\n",
        "romantic_drama = romance_movies[movie_df['Genres'].apply(lambda x: any(y in romance_genres[2] for y in x) if type(x) == list else False)]\n",
        "romantic_fantasy = romance_movies[movie_df['Genres'].apply(lambda x: any(y in romance_genres[3] for y in x) if type(x) == list else False)]\n",
        "romantic_thriller = romance_movies[movie_df['Genres'].apply(lambda x: any(y in romance_genres[4] for y in x) if type(x) == list else False)]\n",
        "\n",
        "print('Roman' , romance_movies.shape[0])\n",
        "print('Romantic comedies: ', romantic_comedy.shape[0], '\\nRomantic films: ', romantic_film.shape[0], '\\nRomantic drama: ', romantic_drama.shape[0], '\\nRomantic fantasy: ', romantic_fantasy.shape[0], '\\nRomantic thriller: ', romantic_thriller.shape[0])\n",
        "print('Total number of films: ', movie_df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "XmWcTHOKTpGw"
      },
      "source": [
        "### Romantic movies runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS60uT0QTpGw"
      },
      "outputs": [],
      "source": [
        "##Should correct outliers\n",
        "#combined_runtime = pd.DataFrame({'Romantic comedy': romantic_comedy['Runtime'], 'Romance Film': romantic_film['Runtime'], 'Romantic drama': romantic_drama['Runtime'], 'Romantic fantasy': romantic_fantasy['Runtime']})\n",
        "#sns.boxplot(combined_runtime)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFHNHcT9TpGx"
      },
      "outputs": [],
      "source": [
        "ax = sns.kdeplot(romantic_comedy['Runtime'], color='blue')\n",
        "ax = sns.kdeplot(romantic_drama['Runtime'], color='green')\n",
        "ax = sns.kdeplot(romantic_film['Runtime'], color='red')\n",
        "ax = sns.kdeplot(romantic_fantasy['Runtime'], color='orange')\n",
        "ax.set_xlim(0,250)\n",
        "ax.legend(['Romantic comedy', 'Romantic drama', 'Romance Film', 'Romantic fantasy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "nlBkkBZWTpGx"
      },
      "source": [
        "### Romantic movies box office revenue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_6pffKWTpGx"
      },
      "outputs": [],
      "source": [
        "#Does not give a good view\n",
        "#combined_box_office = pd.DataFrame({'Romantic comedy': romantic_comedy['Box office revenue'], 'Romance Film': romantic_film['Box office revenue'], 'Romantic drama': romantic_drama['Box office revenue'], 'Romantic fantasy': romantic_fantasy['Box office revenue']})\n",
        "#sns.boxplot(combined_box_office)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWGXdTVmTpJ1"
      },
      "outputs": [],
      "source": [
        "ax = sns.kdeplot(romantic_comedy['Box office revenue'], log_scale=True, color='blue')\n",
        "ax = sns.kdeplot(romantic_drama['Box office revenue'], log_scale=True, color='green')\n",
        "ax = sns.kdeplot(romantic_film['Box office revenue'], log_scale=True, color='red')\n",
        "ax = sns.kdeplot(romantic_fantasy['Box office revenue'], log_scale=True, color='orange')\n",
        "ax.legend(['Romantic comedy', 'Romantic drama', 'Romance Film', 'Romantic fantasy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "PQX7BJ3PTpJ1"
      },
      "source": [
        "### Romantic movies countries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF2omRlITpKD"
      },
      "outputs": [],
      "source": [
        "romantic_comedy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2M7GMPNdTpKN"
      },
      "outputs": [],
      "source": [
        "get_countries = lambda x: len(x) if type(x) == list else np.nan\n",
        "romantic_comedy['number_countries'] = romantic_comedy['Countries'].apply(get_countries)\n",
        "romantic_fantasy['number_countries'] = romantic_fantasy['Countries'].apply(get_countries)\n",
        "romantic_film['number_countries'] = romantic_film['Countries'].apply(get_countries)\n",
        "romantic_drama['number_countries'] = romantic_drama['Countries'].apply(get_countries)\n",
        "\n",
        "combined_numb_countries = pd.DataFrame({\n",
        "    'Romantic comedy': romantic_comedy['number_countries'], \n",
        "    'Romance Film': romantic_film['number_countries'], \n",
        "    'Romantic drama': romantic_drama['number_countries'], \n",
        "    'Romantic fantasy': romantic_fantasy['number_countries']})\n",
        "\n",
        "print('Percentage romantic comedy movie countries > 1: ', round(romantic_comedy[romantic_comedy['number_countries']> 1].shape[0]/romantic_comedy.shape[0], 2), '%')\n",
        "print('Other countries can be added in code...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "CtJdTT-sTpKY"
      },
      "source": [
        "### Movie languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sbb5sh7UTpKq"
      },
      "outputs": [],
      "source": [
        "#Get languages whole movie set\n",
        "movies_language = movie_df[movie_df['Languages'].notnull()]\n",
        "languages=movies_language['Languages'].sum()\n",
        "values, counts = np.unique(languages, return_counts=True)\n",
        "print('5 most common languages in movies are: ')\n",
        "print(values[counts.argsort()[-5:][::-1]])\n",
        "\n",
        "#Get languages romantic movies overall\n",
        "romance_movies_lang = romance_movies[romance_movies['Languages'].notnull()]\n",
        "languages_romance = romance_movies_lang.Languages.sum()\n",
        "values, counts = np.unique(languages_romance, return_counts=True)\n",
        "print('5 most common languages in romantic movies: ')\n",
        "print(values[counts.argsort()[-5:][::-1]])\n",
        "\n",
        "\n",
        "rom_com_known = romantic_comedy[romantic_comedy['Languages'].notnull()]\n",
        "languages_romcom = rom_com_known.Languages.sum()\n",
        "values, counts = np.unique(languages_romcom, return_counts=True)\n",
        "print('\\n5 most common languages in romantic comedies: ')\n",
        "print(values[counts.argsort()[-5:][::-1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifnIh1uE39dU"
      },
      "source": [
        "### 4. CoreNLP Plot Summaries\n",
        "\n",
        "`corenlp_plot_summaries.tar.gz [628 M, separate download]`: The plot summaries from above, run through the Stanford CoreNLP pipeline (tagging, parsing, NER and coref). Each filename begins with the Wikipedia movie ID (which indexes into movie.metadata.tsv).\n",
        "\n",
        "Note: Extraction takes 15min42s, while conversion takes 33s. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4R_TbygA5Bu"
      },
      "outputs": [],
      "source": [
        "# Pour les Romantic Comedies \n",
        "# Avoir [wikipedia ID, name1, name2]\n",
        "# Append name1, name2 à tous les mêmes wiki id \n",
        "# une fois que j'ai ça essayer de voir si ça marche \n",
        "# transformer et avoir [wiki id, name1, name2, info char 1, info char 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTC1DyFmTpKq"
      },
      "outputs": [],
      "source": [
        "# Extract all coreNLP files to Data/CoreNLP\n",
        "if not os.path.exists('Data/CoreNLP'):\n",
        "    coreNLPfilename = 'http://www.cs.cmu.edu/~ark/personas/data/corenlp_plot_summaries.tar'\n",
        "    my_tar = tarfile.open(fileobj=urllib.request.urlopen(coreNLPfilename), mode=\"r|\") \n",
        "    my_tar.extractall(path='./Data/CoreNLP') # specify which folder to extract to\n",
        "    my_tar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umOIDRtgTpKq"
      },
      "outputs": [],
      "source": [
        "# Convert every file in directory Data/CoreNLP to xml format\n",
        "raw_dir = 'Data/CoreNLP/corenlp_plot_summaries'\n",
        "extracted_dir = 'Data/CoreNLP/corenlp_plot_summaries_xml'\n",
        "os.mkdir(extracted_dir)\n",
        "for filename in os.listdir(raw_dir):\n",
        "    f = os.path.join(raw_dir, filename) \n",
        "    if os.path.isfile(f):\n",
        "        # Open and store file as xml \n",
        "        with gzip.open(f, 'rb') as f_in:\n",
        "            gz_file = os.path.join(extracted_dir, filename)\n",
        "            with open(gz_file[:-3], 'wb') as f_out:\n",
        "                f_out.write(f_in.read())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "collapsed": true,
        "id": "OimpS3iNxlh1",
        "outputId": "007a39f1-2c59-4aa3-822a-c4dd2067f45d"
      },
      "outputs": [],
      "source": [
        "# Convert every file in directory Data/CoreNLP to xml format\n",
        "pairs = []\n",
        "raw_dir = 'Data/CoreNLP/corenlp_plot_summaries'\n",
        "extracted_dir = 'Data/CoreNLP/corenlp_plot_summaries_xml'\n",
        "#os.mkdir(extracted_dir)\n",
        "for filename in os.listdir(extracted_dir):\n",
        "    f = os.path.join(extracted_dir, filename) \n",
        "    if os.path.isfile(f):\n",
        "        # Open and store file as xml \n",
        "        # Only for the plots which are in romantic comedies \n",
        "        characters = []\n",
        "        tree = ET.parse(f)\n",
        "        root = tree.getroot()\n",
        "        for child in tree.iter():\n",
        "            if child.tag == \"word\":\n",
        "              current_word = child.text\n",
        "            if child.tag == \"NER\": \n",
        "              if child.text == \"PERSON\":\n",
        "                characters.append(current_word)\n",
        "        values, counts = np.unique(characters, return_counts=True)\n",
        "        two_most_frequent_characters = values[counts.argsort()[-2:][::-1]]\n",
        "        if len(two_most_frequent_characters) > 1:\n",
        "          pairs.append([f.replace('Data/CoreNLP/corenlp_plot_summaries_xml/', '').replace('.xml', ''), two_most_frequent_characters[0], two_most_frequent_characters[1]])\n",
        "\n",
        "#Convert into an array and as a dataframe\n",
        "pairs = np.asarray(pairs).reshape(-1, 3)  \n",
        "pairs_df = pd.DataFrame(pairs, columns=['Wikipedia ID', 'char1', 'char2'])                                                                                                                   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "4vH3YuHH3gBT",
        "outputId": "49757767-ca34-4659-d054-fc0def7d8788"
      },
      "outputs": [],
      "source": [
        "pairs = np.asarray(pairs).reshape(-1, 3)  \n",
        "pairs_df = pd.DataFrame(pairs, columns=['Wikipedia ID', 'char1', 'char2'])  \n",
        "pairs_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 945
        },
        "id": "KP96DgjiD384",
        "outputId": "8c257b0e-7eb7-441e-df6f-a1c2a24b82eb"
      },
      "outputs": [],
      "source": [
        "#Convert to string so it can do the merge\n",
        "char_df['Wikipedia ID'] = char_df['Wikipedia ID'].astype(str)\n",
        "pairs_df['Wikipedia ID'] = pairs_df['Wikipedia ID'].astype(str)\n",
        "\n",
        "#Merge\n",
        "pairs_char = pairs_df.merge(char_df, on=\"Wikipedia ID\")\n",
        "#Filter out the nan values\n",
        "pairs_char = pairs_char[~pairs_char['Character name'].isna()]\n",
        "\n",
        "#Create columns which indicates if char1 and char2 are in character name \n",
        "pairs_char['char1_is'] = pairs_char.apply(lambda x: 1 if x['char1'] in x['Character name'] else 0, axis=1)\n",
        "pairs_char['char2_is'] = pairs_char.apply(lambda x: 1 if x['char2'] in x['Character name'] else 0, axis=1)\n",
        "pairs_char[pairs_char['char1_is'] == 1 & pairs_char['char2_is']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1LKyn37chzj",
        "outputId": "4a59edf9-e84b-439f-a0c7-cb91f9cae980"
      },
      "outputs": [],
      "source": [
        "#Get all characters in plot summaries\n",
        "characters= []\n",
        "tree = ET.parse(\"3217.xml\")\n",
        "root = tree.getroot()\n",
        "for child in tree.iter():\n",
        "    if child.tag == \"word\":\n",
        "      current_word = child.text\n",
        "    if child.tag == \"NER\": \n",
        "      if child.text == \"PERSON\":\n",
        "        characters.append(current_word)\n",
        "characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mu9OnShNdxho",
        "outputId": "507d124b-1aaf-49dc-b539-a449305e6c1d"
      },
      "outputs": [],
      "source": [
        "#Make a pair with the two characters that appear most frequently in the plot summaries\n",
        "values, counts = np.unique(characters, return_counts=True)\n",
        "pair = values[counts.argsort()[-2:][::-1]]\n",
        "pair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFJBdH3bocu_"
      },
      "outputs": [],
      "source": [
        "#Match the name with the characters in the movie\n",
        "#Merge on the wikipedia movie ID \n",
        "char_df[\"Wikipedia ID\"] = wiki_id_of_the_xml\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSNHfah7TpKr"
      },
      "source": [
        "### 5. Name clusters\n",
        "\n",
        "From the file `name.clusters.txt`, we extract movie characters and their related character cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oR_7wd1YTpKr"
      },
      "outputs": [],
      "source": [
        "path = 'Data/MovieSummaries/'\n",
        "names_path = path+'name.clusters.txt'\n",
        "names_cols = ['Character name', 'Cluster']\n",
        "names_df = pd.read_csv(names_path, sep='\\t', header=None, names=names_cols, dtype = {'Freebase ID': str})\n",
        "names_df = names_df.groupby('Character name').aggregate(list)\n",
        "names_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BHCfeenl6-3"
      },
      "source": [
        "### 6. TV Tropes Clusters\n",
        "\n",
        "We reformat the file `tvtropes.clusters.txt` so it is easier to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25-cuhaOhxw9"
      },
      "outputs": [],
      "source": [
        "cluster_path = path+'tvtropes.clusters.format.txt'\n",
        "cluster_cols = ['Cluster', 'Character name', 'Movie', 'Freebase character/map ID', 'Actor']\n",
        "cluster_df = pd.read_csv(cluster_path, sep=',', header=None, names=cluster_cols, dtype = {'Freebase ID': str})\n",
        "cluster_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFbHAkx-mHwA"
      },
      "source": [
        "### 7. Join cluster and movies\n",
        "\n",
        " We now join the TV tropes clusters with movie.metadata so we are able to access movie genre and filter on romance. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1J_1KXB40qbR"
      },
      "outputs": [],
      "source": [
        "cluster_char = cluster_df.merge(char_df, on='Freebase character/map ID')\n",
        "cluster_char_movie = cluster_char.merge(movie_df, on='Freebase ID')\n",
        "romance_cluster = cluster_char_movie[cluster_char_movie['Genres'].apply(lambda x: 'Roman' in x)]\n",
        "romance_cluster.groupby(romance_cluster['Cluster']).size().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehGVG9DaTpKr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8ZkwkG-TpKs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
=======
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "AYC13i4R39dN"
   },
   "source": [
    "# Applied Data Analysis Project\n",
    "**Team**: ToeStewBrr - Alexander Sternfeld, Marguerite Thery, Antoine Bonnet, Hugo Bordereaux\n",
    "\n",
    "**Dataset**: CMU Movie Summary Corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MlSSpJAG39dP"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import tarfile\n",
    "import urllib\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import re\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TadIrDF39dR"
   },
   "source": [
    "## 1. Loading data\n",
    "\n",
    "We first extract all files from the [MoviesSummaries dataset](http://www.cs.cmu.edu/~ark/personas/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('Data/MovieSummaries'):\n",
    "    filename = 'http://www.cs.cmu.edu/~ark/personas/data/MovieSummaries.tar.gz'\n",
    "    my_tar = tarfile.open(fileobj=urllib.request.urlopen(filename), mode=\"r:gz\") \n",
    "    my_tar.extractall('./Data') # specify which folder to extract to\n",
    "    my_tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Extraction of CoreNLP files takes 15min42s, while conversion takes 33s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all coreNLP files to Data/CoreNLP\n",
    "if not os.path.exists('Data/CoreNLP'):\n",
    "    coreNLPfilename = 'http://www.cs.cmu.edu/~ark/personas/data/corenlp_plot_summaries.tar'\n",
    "    my_tar = tarfile.open(fileobj=urllib.request.urlopen(coreNLPfilename), mode=\"r|\") \n",
    "    my_tar.extractall(path='./Data/CoreNLP') # specify which folder to extract to\n",
    "    my_tar.close()\n",
    "\n",
    "# Convert every file in directory Data/CoreNLP to xml format\n",
    "raw_dir = 'Data/CoreNLP/corenlp_plot_summaries'\n",
    "extracted_dir = 'Data/CoreNLP/corenlp_plot_summaries_xml'\n",
    "if not os.path.exists(extracted_dir):\n",
    "    os.mkdir(extracted_dir)\n",
    "    for filename in os.listdir(raw_dir):\n",
    "        f = os.path.join(raw_dir, filename) \n",
    "        if os.path.isfile(f):\n",
    "            # Open and store file as xml \n",
    "            with gzip.open(f, 'rb') as f_in:\n",
    "                gz_file = os.path.join(extracted_dir, filename)\n",
    "                with open(gz_file[:-3], 'wb') as f_out:\n",
    "                    f_out.write(f_in.read())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoIsNdG539dR"
   },
   "source": [
    "## 2. Pre-processing data\n",
    "\n",
    "### 2.1. Plot summaries\n",
    "\n",
    "`plot_summaries.txt [29 M]`: Plot summaries of 42,306 movies extracted from the November 2, 2012 dump of English-language Wikipedia.  Each line contains the Wikipedia movie ID (which indexes into movie.metadata.tsv) followed by the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6AUVYUc39dS",
    "outputId": "f909218b-7eaa-40d6-faee-e6937677b4c9"
   },
   "outputs": [],
   "source": [
    "plot_path = 'Data/MovieSummaries/plot_summaries.txt'\n",
    "plot_cols = ['Wikipedia ID', 'Summary']\n",
    "plot_df = pd.read_csv(plot_path, sep='\\t', header=None, names=plot_cols, index_col=0)\n",
    "plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Hugo: this method stems the words to their lexical root. \n",
    "# Implement Stemming using out of the box Porter algorithm\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "plot_stemmed = [[stemmer.stem(word) for word in sentence.split(\" \")] for sentence in plot_df.iloc[:5].Summary]\n",
    "plot_stemmed = [\" \".join(sentence) for sentence in plot_stemmed]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The word count conversion and tf-idf weighting produce sparse matrices which are destined to be used by NNs. We need something different. \n",
    "# Word count conversion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(strip_accents='ascii',stop_words='english')\n",
    "plot_counts = count_vect.fit_transform(plot_stemmed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF weighting\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "plot_data = tfidf_transformer.fit_transform(plot_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "# from sklearn.preprocessing import normalize\n",
    "# normalize(newsgroups_trainData, norm='l1', axis=0, copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8vh3IEW39dS"
   },
   "source": [
    "### 2.2. Movie metadata\n",
    "\n",
    "`movie.metadata.tsv.gz [3.4 M]`: Metadata for 81,741 movies, extracted from the Noverber 4, 2012 dump of Freebase.  Tab-separated; columns:\n",
    "\n",
    "1. Wikipedia movie ID\n",
    "2. Freebase movie ID\n",
    "3. Movie name\n",
    "4. Movie release date\n",
    "5. Movie box office revenue\n",
    "6. Movie runtime\n",
    "7. Movie languages (Freebase ID:name tuples)\n",
    "8. Movie countries (Freebase ID:name tuples)\n",
    "9. Movie genres (Freebase ID:name tuples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_encoding = lambda x: np.nan if x == '{}' else \\\n",
    "    [w.replace(' Language', '').replace(' language', '') for w in re.findall(r'\"(.*?)\"', x)[1::2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 898
    },
    "id": "p5NKaQHm39dT",
    "outputId": "3b973e13-6efd-4c48-95a7-0346d5ec753c"
   },
   "outputs": [],
   "source": [
    "movie_path = 'Data/MovieSummaries/movie.metadata.tsv'\n",
    "movie_cols = ['Wikipedia ID', 'Freebase ID', 'Name', 'Release date', \n",
    "              'Box office revenue', 'Runtime', 'Languages', 'Countries', 'Genres']\n",
    "movie_df = pd.read_csv(movie_path, sep='\\t', header=None, names=movie_cols, index_col=0, dtype = {'Freebase ID': str})\n",
    "movie_df['Languages'] = movie_df['Languages'].apply(strip_encoding)\n",
    "movie_df['Countries'] = movie_df['Countries'].apply(strip_encoding)\n",
    "movie_df['Genres'] = movie_df['Genres'].apply(strip_encoding)\n",
    "movie_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMGsQrxV39dT"
   },
   "source": [
    "### 2.3. Character metadata\n",
    "\n",
    "`character.metadata.tsv.gz [14 M]`: Metadata for 450,669 characters aligned to the movies above, extracted from the November 4, 2012 dump of Freebase.  Tab-separated; columns:\n",
    "\n",
    "1. Wikipedia movie ID\n",
    "2. Freebase movie ID\n",
    "3. Movie release date\n",
    "4. Character name\n",
    "5. Actor date of birth\n",
    "6. Actor gender\n",
    "7. Actor height (in meters)\n",
    "8. Actor ethnicity (Freebase ID)\n",
    "9. Actor name\n",
    "10. Actor age at movie release\n",
    "11. Freebase character/actor map ID\n",
    "12. Freebase character ID\n",
    "13. Freebase actor ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 762
    },
    "id": "7xdjzblS39dT",
    "outputId": "9c609dff-a8d6-495a-f3c5-7e12ffb2639d"
   },
   "outputs": [],
   "source": [
    "char_path = 'Data/MovieSummaries/character.metadata.tsv'\n",
    "char_cols = ['Wikipedia ID', 'Freebase ID', 'Release date', 'Character name', 'Date of birth', \n",
    "             'Gender', 'Height', 'Ethnicity', 'Actor name', 'Actor age at release', \n",
    "             'Freebase character/map ID', 'Freebase character ID', 'Freebase actor ID']\n",
    "char_df = pd.read_csv(char_path, sep='\\t', header=None, names=char_cols, index_col=0)\n",
    "char_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Name clusters\n",
    "\n",
    "From the file `name.clusters.txt`, we extract movie characters and their related character cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Data/MovieSummaries/'\n",
    "names_path = path+'name.clusters.txt'\n",
    "names_cols = ['Character name', 'Cluster']\n",
    "names_df = pd.read_csv(names_path, sep='\\t', header=None, names=names_cols, dtype = {'Freebase ID': str})\n",
    "names_df = names_df.groupby('Character name').aggregate(list)\n",
    "names_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. TV Tropes Clusters\n",
    "\n",
    "We reformat the file `tvtropes.clusters.txt` so it is easier to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_path = path+'tvtropes.clusters.format.txt'\n",
    "cluster_cols = ['Cluster', 'Character name', 'Movie', 'Freebase character/map ID', 'Actor']\n",
    "cluster_df = pd.read_csv(cluster_path, sep=',', header=None, names=cluster_cols, dtype = {'Freebase ID': str})\n",
    "cluster_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now join the TV tropes clusters with movie.metadata so we are able to access movie genre and filter on romance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_char = cluster_df.merge(char_df, on='Freebase character/map ID')\n",
    "cluster_char_movie = cluster_char.merge(movie_df, on='Freebase ID')\n",
    "romance_cluster = cluster_char_movie[cluster_char_movie['Genres'].apply(lambda x: 'Roman' in x)]\n",
    "romance_cluster.groupby(romance_cluster['Cluster']).size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.1. Analysing romantic genres\n",
    "\n",
    "One notices that there are several types of romantic movies: romantic comedy, romance film, romantic drama. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "romance_genres = ['Romantic comedy', 'Romance Film', 'Romantic drama', 'Romantic fantasy', 'Romantic thriller']\n",
    "is_romantic = lambda i: lambda x: any(y in romance_genres[i] for y in x) if type(x) == list else False\n",
    "romance_movies = movie_df[movie_df['Genres'].apply(is_romantic(slice(0, 5)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Organize by category\n",
    "romantic_comedy = romance_movies.loc[movie_df['Genres'].apply(is_romantic(0))]\n",
    "romantic_film = romance_movies.loc[movie_df['Genres'].apply(is_romantic(1))]\n",
    "romantic_drama = romance_movies.loc[movie_df['Genres'].apply(is_romantic(2))]\n",
    "romantic_fantasy = romance_movies.loc[movie_df['Genres'].apply(is_romantic(3))]\n",
    "romantic_thriller = romance_movies.loc[movie_df['Genres'].apply(is_romantic(4))]\n",
    "\n",
    "print('Roman' , romance_movies.shape[0])\n",
    "print('Romantic comedies: ', romantic_comedy.shape[0], '\\nRomantic films: ', romantic_film.shape[0], '\\nRomantic drama: ', romantic_drama.shape[0], '\\nRomantic fantasy: ', romantic_fantasy.shape[0], '\\nRomantic thriller: ', romantic_thriller.shape[0])\n",
    "print('Total number of films: ', movie_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.2. Romantic movies runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Should correct outliers\n",
    "#combined_runtime = pd.DataFrame({'Romantic comedy': romantic_comedy['Runtime'], 'Romance Film': romantic_film['Runtime'], 'Romantic drama': romantic_drama['Runtime'], 'Romantic fantasy': romantic_fantasy['Runtime']})\n",
    "#sns.boxplot(combined_runtime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.kdeplot(romantic_comedy['Runtime'], color='blue')\n",
    "ax = sns.kdeplot(romantic_drama['Runtime'], color='green')\n",
    "ax = sns.kdeplot(romantic_film['Runtime'], color='red')\n",
    "ax = sns.kdeplot(romantic_fantasy['Runtime'], color='orange')\n",
    "ax.set_xlim(0,250)\n",
    "ax.legend(['Romantic comedy', 'Romantic drama', 'Romance Film', 'Romantic fantasy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.3. Romantic movies box office revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Does not give a good view\n",
    "#combined_box_office = pd.DataFrame({'Romantic comedy': romantic_comedy['Box office revenue'], 'Romance Film': romantic_film['Box office revenue'], 'Romantic drama': romantic_drama['Box office revenue'], 'Romantic fantasy': romantic_fantasy['Box office revenue']})\n",
    "#sns.boxplot(combined_box_office)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = sns.kdeplot(romantic_comedy['Box office revenue'], log_scale=True, color='blue')\n",
    "ax = sns.kdeplot(romantic_drama['Box office revenue'], log_scale=True, color='green')\n",
    "ax = sns.kdeplot(romantic_film['Box office revenue'], log_scale=True, color='red')\n",
    "ax = sns.kdeplot(romantic_fantasy['Box office revenue'], log_scale=True, color='orange')\n",
    "ax.legend(['Romantic comedy', 'Romantic drama', 'Romance Film', 'Romantic fantasy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.4. Romantic movies countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "romantic_comedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_countries = lambda x: len(x) if type(x) == list else np.nan\n",
    "romantic_comedy['number_countries'] = romantic_comedy['Countries'].apply(get_countries)\n",
    "romantic_fantasy['number_countries'] = romantic_fantasy['Countries'].apply(get_countries)\n",
    "romantic_film['number_countries'] = romantic_film['Countries'].apply(get_countries)\n",
    "romantic_drama['number_countries'] = romantic_drama['Countries'].apply(get_countries)\n",
    "\n",
    "combined_numb_countries = pd.DataFrame({\n",
    "    'Romantic comedy': romantic_comedy['number_countries'], \n",
    "    'Romance Film': romantic_film['number_countries'], \n",
    "    'Romantic drama': romantic_drama['number_countries'], \n",
    "    'Romantic fantasy': romantic_fantasy['number_countries']})\n",
    "\n",
    "print('Percentage romantic comedy movie countries > 1: ', round(romantic_comedy[romantic_comedy['number_countries']> 1].shape[0]/romantic_comedy.shape[0], 2), '%')\n",
    "print('Other countries can be added in code...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.5. Movie languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get languages whole movie set\n",
    "movies_language = movie_df[movie_df['Languages'].notnull()]\n",
    "languages=movies_language['Languages'].sum()\n",
    "values, counts = np.unique(languages, return_counts=True)\n",
    "print('5 most common languages in movies are: ')\n",
    "print(values[counts.argsort()[-5:][::-1]])\n",
    "\n",
    "#Get languages romantic movies overall\n",
    "romance_movies_lang = romance_movies[romance_movies['Languages'].notnull()]\n",
    "languages_romance = romance_movies_lang.Languages.sum()\n",
    "values, counts = np.unique(languages_romance, return_counts=True)\n",
    "print('5 most common languages in romantic movies: ')\n",
    "print(values[counts.argsort()[-5:][::-1]])\n",
    "\n",
    "\n",
    "rom_com_known = romantic_comedy[romantic_comedy['Languages'].notnull()]\n",
    "languages_romcom = rom_com_known.Languages.sum()\n",
    "values, counts = np.unique(languages_romcom, return_counts=True)\n",
    "print('\\n5 most common languages in romantic comedies: ')\n",
    "print(values[counts.argsort()[-5:][::-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifnIh1uE39dU"
   },
   "source": [
    "### 3.5. CoreNLP Plot summaries\n",
    "\n",
    "`corenlp_plot_summaries.tar.gz [628 M, separate download]`: The plot summaries from above, run through the Stanford CoreNLP pipeline (tagging, parsing, NER and coref). Each filename begins with the Wikipedia movie ID (which indexes into movie.metadata.tsv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gfYMmpwSGxMD",
    "outputId": "c45a3a93-e2ae-4785-f362-814836b0be1a"
   },
   "outputs": [],
   "source": [
    "#Use file I already extracted on my computer to run some tests\n",
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse('Data/CoreNLP/corenlp_plot_summaries_xml/3217.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "#NER tag = person can give us the characters mention in the plot summary. \n",
    "\n",
    "print(len(root.findall('.//*governor'))) #use parse or basic-dependencies to have more info \n",
    "#print(root.findall('.//*governor').text())\n",
    "for l in root.findall('.//*NER'): \n",
    "  if len(l.text) > 1:\n",
    "    print(l.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To print xml files as a pretty tree\n",
    "import xml.dom.minidom\n",
    "\n",
    "def print_tree_xml(xml_name):\n",
    "    tree_xml = xml.dom.minidom.parse(xml_name).toprettyxml()\n",
    "    print(tree_xml)\n",
    "\n",
    "#print_tree_xml('Data/CoreNLP/corenlp_plot_summaries_xml/3217.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any character, we want to extract related information (from name clusters, character metadata) as well as actions, characteristics and relations (from CoreNLP). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Freebase ID</th>\n",
       "      <th>Release date</th>\n",
       "      <th>Character name</th>\n",
       "      <th>Date of birth</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Actor name</th>\n",
       "      <th>Actor age at release</th>\n",
       "      <th>Freebase character/map ID</th>\n",
       "      <th>Freebase character ID</th>\n",
       "      <th>Freebase actor ID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wikipedia ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>975900</th>\n",
       "      <td>/m/03vyhn</td>\n",
       "      <td>2001-08-24</td>\n",
       "      <td>Akooshay</td>\n",
       "      <td>1958-08-26</td>\n",
       "      <td>F</td>\n",
       "      <td>1.620</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wanda De Jesus</td>\n",
       "      <td>42.0</td>\n",
       "      <td>/m/0bgchxw</td>\n",
       "      <td>/m/0bgcj3x</td>\n",
       "      <td>/m/03wcfv7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975900</th>\n",
       "      <td>/m/03vyhn</td>\n",
       "      <td>2001-08-24</td>\n",
       "      <td>Lieutenant Melanie Ballard</td>\n",
       "      <td>1974-08-15</td>\n",
       "      <td>F</td>\n",
       "      <td>1.780</td>\n",
       "      <td>/m/044038p</td>\n",
       "      <td>Natasha Henstridge</td>\n",
       "      <td>27.0</td>\n",
       "      <td>/m/0jys3m</td>\n",
       "      <td>/m/0bgchn4</td>\n",
       "      <td>/m/0346l4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975900</th>\n",
       "      <td>/m/03vyhn</td>\n",
       "      <td>2001-08-24</td>\n",
       "      <td>Desolation Williams</td>\n",
       "      <td>1969-06-15</td>\n",
       "      <td>M</td>\n",
       "      <td>1.727</td>\n",
       "      <td>/m/0x67</td>\n",
       "      <td>Ice Cube</td>\n",
       "      <td>32.0</td>\n",
       "      <td>/m/0jys3g</td>\n",
       "      <td>/m/0bgchn_</td>\n",
       "      <td>/m/01vw26l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975900</th>\n",
       "      <td>/m/03vyhn</td>\n",
       "      <td>2001-08-24</td>\n",
       "      <td>Sgt Jericho Butler</td>\n",
       "      <td>1967-09-12</td>\n",
       "      <td>M</td>\n",
       "      <td>1.750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jason Statham</td>\n",
       "      <td>33.0</td>\n",
       "      <td>/m/02vchl6</td>\n",
       "      <td>/m/0bgchnq</td>\n",
       "      <td>/m/034hyc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975900</th>\n",
       "      <td>/m/03vyhn</td>\n",
       "      <td>2001-08-24</td>\n",
       "      <td>Bashira Kincaid</td>\n",
       "      <td>1977-09-25</td>\n",
       "      <td>F</td>\n",
       "      <td>1.650</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Clea DuVall</td>\n",
       "      <td>23.0</td>\n",
       "      <td>/m/02vbb3r</td>\n",
       "      <td>/m/0bgchp9</td>\n",
       "      <td>/m/01y9xg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913762</th>\n",
       "      <td>/m/03pcrp</td>\n",
       "      <td>1992-05-21</td>\n",
       "      <td>Elensh</td>\n",
       "      <td>1970-05</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dorothy Elias-Fahn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/m/0kr406c</td>\n",
       "      <td>/m/0kr406h</td>\n",
       "      <td>/m/0b_vcv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913762</th>\n",
       "      <td>/m/03pcrp</td>\n",
       "      <td>1992-05-21</td>\n",
       "      <td>Hibiki</td>\n",
       "      <td>1965-04-12</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jonathan Fahn</td>\n",
       "      <td>27.0</td>\n",
       "      <td>/m/0kr405_</td>\n",
       "      <td>/m/0kr4090</td>\n",
       "      <td>/m/0bx7_j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28308153</th>\n",
       "      <td>/m/0cp05t9</td>\n",
       "      <td>1957</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1941-11-18</td>\n",
       "      <td>M</td>\n",
       "      <td>1.730</td>\n",
       "      <td>/m/02w7gg</td>\n",
       "      <td>David Hemmings</td>\n",
       "      <td>15.0</td>\n",
       "      <td>/m/0g8ngmc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/m/022g44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28308153</th>\n",
       "      <td>/m/0cp05t9</td>\n",
       "      <td>1957</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Roberta Paterson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/m/0g8ngmj</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/m/0g8ngmm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28308153</th>\n",
       "      <td>/m/0cp05t9</td>\n",
       "      <td>1957</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Rogers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/m/0g8ngmw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/m/0btz19d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>450669 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Freebase ID Release date              Character name  \\\n",
       "Wikipedia ID                                                        \n",
       "975900         /m/03vyhn   2001-08-24                    Akooshay   \n",
       "975900         /m/03vyhn   2001-08-24  Lieutenant Melanie Ballard   \n",
       "975900         /m/03vyhn   2001-08-24         Desolation Williams   \n",
       "975900         /m/03vyhn   2001-08-24          Sgt Jericho Butler   \n",
       "975900         /m/03vyhn   2001-08-24             Bashira Kincaid   \n",
       "...                  ...          ...                         ...   \n",
       "913762         /m/03pcrp   1992-05-21                      Elensh   \n",
       "913762         /m/03pcrp   1992-05-21                      Hibiki   \n",
       "28308153      /m/0cp05t9         1957                         NaN   \n",
       "28308153      /m/0cp05t9         1957                         NaN   \n",
       "28308153      /m/0cp05t9         1957                         NaN   \n",
       "\n",
       "             Date of birth Gender  Height   Ethnicity          Actor name  \\\n",
       "Wikipedia ID                                                                \n",
       "975900          1958-08-26      F   1.620         NaN      Wanda De Jesus   \n",
       "975900          1974-08-15      F   1.780  /m/044038p  Natasha Henstridge   \n",
       "975900          1969-06-15      M   1.727     /m/0x67            Ice Cube   \n",
       "975900          1967-09-12      M   1.750         NaN       Jason Statham   \n",
       "975900          1977-09-25      F   1.650         NaN         Clea DuVall   \n",
       "...                    ...    ...     ...         ...                 ...   \n",
       "913762             1970-05      F     NaN         NaN  Dorothy Elias-Fahn   \n",
       "913762          1965-04-12      M     NaN         NaN       Jonathan Fahn   \n",
       "28308153        1941-11-18      M   1.730   /m/02w7gg      David Hemmings   \n",
       "28308153               NaN    NaN     NaN         NaN    Roberta Paterson   \n",
       "28308153               NaN    NaN     NaN         NaN         John Rogers   \n",
       "\n",
       "              Actor age at release Freebase character/map ID  \\\n",
       "Wikipedia ID                                                   \n",
       "975900                        42.0                /m/0bgchxw   \n",
       "975900                        27.0                 /m/0jys3m   \n",
       "975900                        32.0                 /m/0jys3g   \n",
       "975900                        33.0                /m/02vchl6   \n",
       "975900                        23.0                /m/02vbb3r   \n",
       "...                            ...                       ...   \n",
       "913762                         NaN                /m/0kr406c   \n",
       "913762                        27.0                /m/0kr405_   \n",
       "28308153                      15.0                /m/0g8ngmc   \n",
       "28308153                       NaN                /m/0g8ngmj   \n",
       "28308153                       NaN                /m/0g8ngmw   \n",
       "\n",
       "             Freebase character ID Freebase actor ID  \n",
       "Wikipedia ID                                          \n",
       "975900                  /m/0bgcj3x        /m/03wcfv7  \n",
       "975900                  /m/0bgchn4         /m/0346l4  \n",
       "975900                  /m/0bgchn_        /m/01vw26l  \n",
       "975900                  /m/0bgchnq         /m/034hyc  \n",
       "975900                  /m/0bgchp9         /m/01y9xg  \n",
       "...                            ...               ...  \n",
       "913762                  /m/0kr406h         /m/0b_vcv  \n",
       "913762                  /m/0kr4090         /m/0bx7_j  \n",
       "28308153                       NaN         /m/022g44  \n",
       "28308153                       NaN        /m/0g8ngmm  \n",
       "28308153                       NaN        /m/0btz19d  \n",
       "\n",
       "[450669 rows x 12 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "975900"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For any character in char_df, get the wikipedia movie ID\n",
    "def get_character_data(char_name):\n",
    "    movie_ids = list(char_df[char_df['Character name'] == 'Harry Potter'].index)\n",
    "    # Get cluster of character from name_clusters\n",
    "    cluster = name_clusters[char_name]\n",
    "char_name = \n",
    "get_wiki_movie_id('Harry Potter')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to identify the two main characters in a plot summary. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. An Analysis of Romance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. ?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
