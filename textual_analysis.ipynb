{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Data Analysis Project\n",
    "**Team**: ToeStewBrr - Alexander Sternfeld, Marguerite Thery, Antoine Bonnet, Hugo Bordereaux\n",
    "\n",
    "**Dataset**: CMU Movie Summary Corpus\n",
    "\n",
    "## Textual Analysis\n",
    "\n",
    "**Overview**:\n",
    "* 1. [Removing stopwords](#Section_1)\n",
    "* 2. [Semantic scoring](#Section_2)\n",
    "* 3. [Love words extraction](#Section_3)\n",
    "* 4. [Representing the data](#Section_4)\n",
    "* 5. [Next steps](#Section_5)\n",
    "    - 5.1 [Reference vector](#Section_5.1)\n",
    "    - 5.2 [Threshold](#Section_5.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import *\n",
    "from coreNLP_analysis import *\n",
    "from textual_analysis import *\n",
    "import spacy\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Load Natural Language Tool Kit (NLTK)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "#Load the spacy model for the semantic analysis\n",
    "nlp_spacy = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "download_data()\n",
    "plot_df = load_plot_df()\n",
    "movie_df = load_movie_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Removing stopwords\n",
    "\n",
    "A stop word is a frequently used term that a search engine has been configured to ignore, both while indexing entries for searching and when retrieving them as the result of a search query. Examples of stop words include \"the,\" \"a,\" \"an,\" and \"in.\"\n",
    "We don't want these terms to take up any unnecessary storage space or processing time in our database. By keeping a record of the terms you believe to be stop words, we may easily eliminate them for this reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy the plot_df to a new dataframe\n",
    "plot_df_removed = plot_df.copy()\n",
    "#Remove stopwords from the summaries\n",
    "plot_df_removed['Summary'] = plot_df['Summary'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Semantic scoring\n",
    "\n",
    "The semantic scoring is based on spaCy, which is a library for advanced natural language processing. Similarity is determined based on word vectors that are generated thanks to the Word2vec technique. We then look at the distance between the vectors to assess on the similarity between words. Vectors can represent a word only but also a text in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The reference word is the word that we want to find the similarity with\n",
    "words = nlp_spacy(\"love\")\n",
    "\n",
    "#Create a column with the similarity score of the summaries to each word in words\n",
    "for word in words:\n",
    "        plot_df_removed[word.text] = np.nan\n",
    "        plot_df_removed[word.text] = plot_df_removed['Summary'].apply(lambda x: nlp_spacy(' '.join(x)).similarity(words))\n",
    "\n",
    "#sort the dataframe by the similarity score\n",
    "plot_df_removed.sort_values(by='love', ascending=False, inplace=True)\n",
    "plot_df_removed.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the similarity score of the summaries to the reference word\n",
    "plot_df_removed['love'].plot(kind='hist', bins=100)\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Similarity score to the word \"love\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Love words extraction\n",
    "\n",
    "This part's aim is to find the words in the summary that are love-related in case we need them later. They are extracted based on the same word2vec method by setting a threshold on the similarity they have with the word \"love\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The threshold is the minimum similarity score to be considered a love-related word\n",
    "love_threshold = 0.35\n",
    "\n",
    "#Create a column with the love-related words in the summaries\n",
    "plot_df_removed['love_words'] = np.nan\n",
    "plot_df_removed['love_words'] = plot_df_removed['Summary'].apply(lambda x :extract_love_words(x, words=words, threshold=love_threshold))\n",
    "\n",
    "#sort love-related words by similarity to love\n",
    "for word in words:\n",
    "    plot_df_removed['love_words'] = plot_df_removed['love_words'].apply(lambda x: sorted(x, key=lambda y: nlp_spacy(y).similarity(words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df_removed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate all the love-related words in a list\n",
    "love_words = []\n",
    "love_words = [love_words + word for word in plot_df_removed[\"love_words\"]]\n",
    "love_words = np.unique(list(np.concatenate(love_words).flat))\n",
    "print(love_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Representing the semantic proximity\n",
    "\n",
    "Here we will present the pipeline to plot our word vectors and the semantic proximity between them. We can also perform clustering on the vectors to see if we can find some interesting patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of semantic vectors for each love-related word\n",
    "love_words_vectors = [nlp_spacy(str(word)).vector for word in love_words]\n",
    "\n",
    "#reduce the dimensionality of the word vectors to 3D\n",
    "pca = PCA(n_components=3)\n",
    "love_words_vectors_3D = pca.fit_transform(love_words_vectors)\n",
    "\n",
    "#plot the 3D word vectors\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(love_words_vectors_3D[:,0], love_words_vectors_3D[:,1], love_words_vectors_3D[:,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster the word vectors with kmeans\n",
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(love_words_vectors_3D)\n",
    "\n",
    "#plot the clusters\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(love_words_vectors_3D[:,0], love_words_vectors_3D[:,1], love_words_vectors_3D[:,2], c=kmeans.labels_)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Next Steps\n",
    "\n",
    "We have 2 main problems with this approach:\n",
    "- How shall we define the reference vector ?\n",
    "- How is the threshold set ?\n",
    "\n",
    "#### 5.1. Reference vector\n",
    "\n",
    "The reference vector for now is just the semantic vector of the word \"love\". We have to implement a methodology that allows us to find the best reference vector. We have to find a way to define the reference vector in a way that it scores high all the summaries that depicts a relationship. To do this we are thinking of a Monte Carlo tree search approach and use the movies labeled as romantic by their genres as the target true positives.\n",
    "\n",
    "#### 5.2. Threshold\n",
    "\n",
    "Once we have a correct reference vector that scores the summaries as wanted, we need to find the best way to set the threshold splitting the movies that depicts a relation from the ones that don't. We are thinking of using an F1-scoring method to find it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "645a45bafa7a70bf663118869c5a8c62ee426a479322e8202ebe39ad8b921733"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
