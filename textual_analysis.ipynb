{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Applied Data Analysis Project\n",
    "**Team**: ToeStewBrr - Alexander Sternfeld, Marguerite Thery, Antoine Bonnet, Hugo Bordereaux\n",
    "\n",
    "**Dataset**: CMU Movie Summary Corpus\n",
    "\n",
    "# Part 3: Textual Analysis\n",
    "\n",
    "In this notebook, we analyze the pre-processed output of our custom CoreNLP pipeline. \n",
    "\n",
    "### Table of contents\n",
    "1. [Loading pre-processed coreNLP data](#section1)\n",
    "2. [Persona clusters](#section2)\n",
    "    - 2.1. [Embedding descriptions](#section2-1)\n",
    "    - 2.2. [Principal Component Analysis (PCA)](#section2-2)\n",
    "    - 2.3. [Clustering personas](#section2-3)\n",
    "    - 2.4. [Visualizing persona clusters](#section2-4)\n",
    "\n",
    "**Prerequisite**: \n",
    "\n",
    "Install [spaCy](https://spacy.io) using the following commands: \n",
    "\n",
    "        pip install spacy\n",
    "        \n",
    "        python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "from extraction import *\n",
    "from coreNLP_analysis import *\n",
    "from load_data import *\n",
    "from textual_analysis import *\n",
    "\n",
    "\n",
    "# NOTE: If you haven't loaded NLTK before, set this to True\n",
    "load_nltk = False\n",
    "\n",
    "if load_nltk: #Load the spaCy model for the semantic analysis\n",
    "    nlp_spacy = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load pre-processed coreNLP data <a class=\"anchor\" id=\"section1\"></a>\n",
    "\n",
    "We first load the pre-processed output from our custom CoreNLP pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_description_path = 'Data/CoreNLP/char_descriptions.csv'\n",
    "full_description_path = 'Data/CoreNLP/full_descriptions.csv'\n",
    "\n",
    "# Load character descriptions\n",
    "char_description_df = pd.read_csv(char_description_path, sep='\\t', index_col=None, low_memory=False)\n",
    "\n",
    "# Convert to lists\n",
    "char_description_df['agent_verbs'] = char_description_df.agent_verbs.apply(\n",
    "    lambda x: literal_eval(x) if type(x) == str else x)\n",
    "char_description_df['patient_verbs'] = char_description_df.patient_verbs.apply(\n",
    "    lambda x: literal_eval(x) if type(x) == str else x)\n",
    "char_description_df['attributes'] = char_description_df.attributes.apply(\n",
    "    lambda x: literal_eval(x) if type(x) == str else x)\n",
    "char_description_df['descriptions'] = char_description_df.descriptions.apply(\n",
    "    lambda x: literal_eval(x) if type(x) == str else x)\n",
    "char_description_df['title'] = char_description_df.title.apply(\n",
    "    lambda x: literal_eval(x) if type(x) == str else x)\n",
    "\n",
    "full_description_df = pd.read_csv(full_description_path, sep='\\t', index_col=None, low_memory=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Persona clusters <a class=\"anchor\" id=\"section2\"></a>\n",
    "\n",
    "### 2.1. Embedding descriptions <a class=\"anchor\" id=\"section2-1\"></a>\n",
    "\n",
    "We embed all descriptive words (actions, attributes, titles) of all characters into a high-dimensional vector space using spaCy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = 'Data/CoreNLP/char_description_embeddings.pickle'\n",
    "# If you haven't been here for a while, please delete the old pkl file you may still have\n",
    "# If we have already embedded the descriptions, load them from the pickle file, else execute code below\n",
    "if os.path.exists(embedding_file):\n",
    "    char_description_df = pd.read_pickle(embedding_file)\n",
    "\n",
    "else:\n",
    "    # Embed descriptions (Get a comfy chair, this takes a while) \n",
    "    char_description_df = construct_descriptions_embeddings(char_description_df, nlp_spacy)\n",
    "    # Split embeddings by category\n",
    "    char_description_df = embeddings_categorical(char_description_df)\n",
    "    # Save the embeddings to a pickle file\n",
    "    with open(embedding_file, 'wb') as f:\n",
    "        pickle.dump(char_description_df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Weighted average of word vectors <a class=\"anchor\" id=\"section2-2\"></a>\n",
    "\n",
    "We then weigh the word embedding of each word for each character by their cosine distance to the average semantic vector of words with the sam type used for all characters in the dataset. The *cosine distance* is defined as:\n",
    "\n",
    "$$\\text{cosine distance}(x_1, x_2) = 1-\\frac{x_1 \\cdot x_2}{||x_1||\\cdot||x_2||}$$\n",
    "\n",
    "where $x_1$ and $x_2$ are the vector representations of two words.\n",
    "\n",
    "Ideas for improvement: \n",
    "- [X] Add a threshold for the cosine difference to filter out words that are too close to the average vector. \n",
    "- [X] Compare each word to the average vector in their category rather than all words. \n",
    "- [X] Weigh each word using the word frequency as well.\n",
    "- [X] Add percentile to word weighing to keep top p% of words with highest cos-diff\n",
    "- [X] Give more importance to title than other words, (i.e. set title_weight to 0.5)\n",
    "- [X] Weight all descriptions by weighing each column separately then concatenating. See point 2. Give more weight to title at the end. \n",
    "- [Â ] Remove characters with < X mentions in plot summary using most_mentioned() function.\n",
    "\n",
    "Note: calling weight_embeddings() with column='descriptions' weights all other columns by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_df = weight_embeddings(char_description_df, column='descriptions', percentile=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with clustering methods\n",
    "\n",
    "Now that we have weighted each word's embedding, we will try to look for good clustering parameters.\n",
    "\n",
    "### 1. Clustering titles only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster1 = cluster_embeddings(weight_df, weighing=1, desc='title', sample=1, min_words=0, eps=7, min_samples=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster2 = cluster_embeddings(weight_df, weighing=1, desc='title', sample=1, min_words=0, eps=7, min_samples=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Clustering descriptions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster4 = cluster_embeddings(weight_df, weighing=1, desc='descriptions', sample=0.01, min_words=0, eps=7, min_samples=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Find the best combination of (desc, sample, min_words, eps, min_samples) parameters for cluster_embeddings() + best percentile parameter for weight_embeddings() to have good clusters without removing too many points, and having 20-50 clusters of characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = 'descriptions'\n",
    "embed_type = desc + '_embeddings'\n",
    "column = 'weighted_' + embed_type\n",
    "weighing = 1\n",
    "min_words = 0\n",
    "\n",
    "df = weight_df.copy(deep=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Dimensionality reduction <a class=\"anchor\" id=\"section2-3\"></a>\n",
    "\n",
    "#### 2.3.1. Principal Component Analysis (PCA) <a class=\"anchor\" id=\"section2-3-1\"></a>\n",
    "\n",
    "To visualize our clusters, we then map these high-dimensional descriptive vectors to 50-dimensional space using PCA to prepare the ground for a second dimensionality reduction technique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction: PCA to 50 dimensions -> t-SNE to 3 dimensions\n",
    "sample = 0.1\n",
    "\n",
    "if sample < 1: \n",
    "    df = df.sample(frac=sample, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total = df[column].apply(lambda x: 1 if type(x) == np.ndarray else 0).sum()\n",
    "pca_df = descriptions_PCA(df, column=column, n_components=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2. *t*-distributed Stochastic Neighbor Embedding (t-SNE) <a class=\"anchor\" id=\"section2-3-2\"></a>\n",
    "\n",
    "We now perform [t-SNE dimensionality reduction](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) on the pre-reduced weighted embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df = descriptions_tSNE(pca_df, column=column, n_components=3, learning_rate='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the embeddings to a pickle file\n",
    "#pickle_file = 'Data/CoreNLP/char_description_embeddings_tsne.pickle'\n",
    "#with open(pickle_file, 'wb') as f:\n",
    "#    pickle.dump(char_description_df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Clustering personas <a class=\"anchor\" id=\"section2-4\"></a>\n",
    "\n",
    "The persona point cloud is clustered into several categories using DBSCAN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 7\n",
    "min_samples = 50\n",
    "\n",
    "cluster_df, n_clusters, n_removed = DBSCAN_cluster(tsne_df, column, method='tsne', eps=eps, min_samples=min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why isn't the clustering working???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Visualizing persona clusters <a class=\"anchor\" id=\"section2-5\"></a>\n",
    "\n",
    "The clustered persona point cloud is shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 't-SNE + DBSCAN with {} clusters, \\nRemoved {}/{} noisy data points\\nDBSCAN: eps = {}, min_samples = {}\\nFilter: weighing = {}, desc = {}, sample={}, min_words={}'.format(n_clusters, n_removed, n_total, eps, min_samples, weighing, desc, sample, min_words)\n",
    "plot_clusters_3d(cluster_df, title, column=column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cda0ac541ab6c535dcb4ffe1de6394d0d0ba460ea4bcec2c3250fd08f595b9ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
