{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Applied Data Analysis Project\n",
    "Team: ToeStewBrr - Alexander Sternfeld, Marguerite Thery, Antoine Bonnet, Hugo Bordereaux\n",
    "\n",
    "Dataset: CMU Movie Summary Corpus\n",
    "\n",
    "### 1. Preprocessing data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import requests\n",
    "import tarfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as plt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Temporary configuration: Until we figure out how to extract the tar file directly from the url, please download the file and place it in the Data directory of this project.\n",
    "\n",
    "We first extract all files from the MoviesSummaries tar file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if not os.path.exists('Data/MovieSummaries'):\n",
    "    my_tar = tarfile.open('Data/MovieSummaries.tar.gz') # Can't upload voluminous data to Github\n",
    "    my_tar.extractall('./Data') # specify which folder to extract to\n",
    "    my_tar.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We then explore the structure of each of the data files."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. plot_summaries.txt [29 M]\n",
    "\n",
    "Plot summaries of 42,306 movies extracted from the November 2, 2012 dump of English-language Wikipedia.  Each line contains the Wikipedia movie ID (which indexes into movie.metadata.tsv) followed by the summary."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_path = 'Data/MovieSummaries/plot_summaries.txt'\n",
    "plot_cols = ['Wikipedia ID', 'Summary']\n",
    "plot_df = pd.read_csv(plot_path, sep='\\t', header=None, names=plot_cols, index_col=0)\n",
    "plot_df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. movie.metadata.tsv.gz [3.4 M]\n",
    "\n",
    "\n",
    "Metadata for 81,741 movies, extracted from the Noverber 4, 2012 dump of Freebase.  Tab-separated; columns:\n",
    "\n",
    "1. Wikipedia movie ID\n",
    "2. Freebase movie ID\n",
    "3. Movie name\n",
    "4. Movie release date\n",
    "5. Movie box office revenue\n",
    "6. Movie runtime\n",
    "7. Movie languages (Freebase ID:name tuples)\n",
    "8. Movie countries (Freebase ID:name tuples)\n",
    "9. Movie genres (Freebase ID:name tuples)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "movie_path = 'Data/MovieSummaries/movie.metadata.tsv'\n",
    "movie_cols = ['Wikipedia ID', 'Freebase ID', 'Name', 'Release date',\n",
    "              'Box office revenue', 'Runtime', 'Languages', 'Countries', 'Genres']\n",
    "movie_df = pd.read_csv(movie_path, sep='\\t', header=None, names=movie_cols, index_col=0)\n",
    "movie_df\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. character.metadata.tsv.gz [14 M]\n",
    "\n",
    "Metadata for 450,669 characters aligned to the movies above, extracted from the Noverber 4, 2012 dump of Freebase.  Tab-separated; columns:\n",
    "\n",
    "1. Wikipedia movie ID\n",
    "2. Freebase movie ID\n",
    "3. Movie release date\n",
    "4. Character name\n",
    "5. Actor date of birth\n",
    "6. Actor gender\n",
    "7. Actor height (in meters)\n",
    "8. Actor ethnicity (Freebase ID)\n",
    "9. Actor name\n",
    "10. Actor age at movie release\n",
    "11. Freebase character/actor map ID\n",
    "12. Freebase character ID\n",
    "13. Freebase actor ID\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "char_path = 'Data/MovieSummaries/character.metadata.tsv'\n",
    "char_cols = ['Wikipedia ID', 'Freebase ID', 'Release date', 'Character name', 'Date of birth',\n",
    "             'Gender', 'Height', 'Ethnicity', 'Actor name', 'Actor age at release',\n",
    "             'Freebase character/map ID', 'Freebase character ID', 'Freebase actor ID']\n",
    "char_df = pd.read_csv(char_path, sep='\\t', header=None, names=char_cols, index_col=0)\n",
    "char_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. corenlp_plot_summaries.tar.gz [628 M, separate download]\n",
    "\n",
    "The plot summaries from above, run through the Stanford CoreNLP pipeline (tagging, parsing, NER and coref). Each filename begins with the Wikipedia movie ID (which indexes into movie.metadata.tsv).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gzip\n",
    "import xml.etree.ElementTree as ET\n",
    "#I uploaded the corenlp_plot_summaries from local\n",
    "directory = './corenlp_plot_summaries'\n",
    "\n",
    "#TODO: Need to extract each file and convert them to xml\n",
    "#For loop to open every file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "  f = os.path.join(directory, filename)\n",
    "  if os.path.isfile(f):\n",
    "    #open and store file as xml\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Use file I already extracted on my computer to run some tests\n",
    "tree = ET.parse('3217.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "print(len(root.findall('.//*governor'))) #use parse or basic-dependencies to have more info\n",
    "#print(root.findall('.//*governor').text())\n",
    "for l in root.findall('.//*NER'):\n",
    "  if len(l.text) > 1:\n",
    "    print(l.text)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
