{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Data Analysis Project\n",
    "**Team**: ToeStewBrr - Alexander Sternfeld, Marguerite Thery, Antoine Bonnet, Hugo Bordereaux\n",
    "\n",
    "**Dataset**: CMU Movie Summary Corpus\n",
    "\n",
    "## Textual Analysis\n",
    "\n",
    "**Overview**:\n",
    "* 1. [Removing stopwords](#Section_1)\n",
    "* 2. [Semantic scoring](#Section_2)\n",
    "* 3. [Love words extraction](#Section_3)\n",
    "* 4. [Representing the data](#Section_4)\n",
    "* 5. [Next steps](#Section_5)\n",
    "    - 5.1 [Reference vector](#Section_5.1)\n",
    "    - 5.2 [Threshold](#Section_5.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/margueritethery/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/margueritethery/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/margueritethery/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/margueritethery/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from extraction import *\n",
    "from coreNLP_analysis import *\n",
    "from load_data import *\n",
    "from textual_analysis import *\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '5'\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To load spacy components, put to False when loaded before\n",
    "load_nltk = True\n",
    "\n",
    "if load_nltk == True:\n",
    "    #Load Natural Language Tool Kit (NLTK) word banks\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    #Load the spacy model for the semantic analysis\n",
    "    nlp_spacy = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "download_data(coreNLP=False)\n",
    "plot_df = load_plot_df()\n",
    "movie_df = load_movie_df()\n",
    "char_df = load_char_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load processed core-NLP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_path = 'Data/CoreNLP/descriptions.csv'\n",
    "relations_path = 'Data/CoreNLP/relations.csv'\n",
    "\n",
    "if not os.path.exists(description_path) and not os.path.exists(relations_path):\n",
    "\n",
    "    # Extract descriptions and relations from all xml files\n",
    "    output_dir = 'Data/CoreNLP/PlotsOutputs'\n",
    "    descriptions, relations = extract_descriptions_relations(output_dir)\n",
    "\n",
    "    # Save descriptions and relations into csv files\n",
    "    descriptions.to_csv(description_path, sep='\\t')\n",
    "    relations.to_csv(relations_path, sep='\\t')\n",
    "\n",
    "# If we've already run the extraction, we can load the dataframe from a file\n",
    "else:\n",
    "    descriptions = pd.read_csv(description_path, sep='\\t', index_col=0)\n",
    "    relations = pd.read_csv(relations_path, sep='\\t', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "romance_description_path = 'Data/CoreNLP/romance_descriptions.csv'\n",
    "romance_relations_path = 'Data/CoreNLP/romance_relations.csv'\n",
    "\n",
    "if not os.path.exists(romance_description_path) and not os.path.exists(romance_relations_path):\n",
    "\n",
    "    # Extract descriptions and relations from all romance xml files\n",
    "    romance_output_dir = 'Data/CoreNLP/RomancePlotsOutputs'\n",
    "\n",
    "    # Remove file '43849.xml' from the directory, as it is not a valid xml file\n",
    "    if os.path.exists(f'{romance_output_dir}/43849.xml'):\n",
    "        os.remove(f'{romance_output_dir}/43849.xml')\n",
    "    \n",
    "    romance_descriptions, romance_relations = extract_descriptions_relations(romance_output_dir, log_interval=1)\n",
    "\n",
    "    # Save descriptions and relations into csv files\n",
    "    romance_descriptions.to_csv(romance_description_path, sep='\\t')\n",
    "    romance_relations.to_csv(romance_relations_path, sep='\\t')\n",
    "\n",
    "# If we've already run the extraction, we can load the dataframe from a file\n",
    "else: \n",
    "    romance_descriptions = pd.read_csv(romance_description_path, sep='\\t', index_col=0)\n",
    "    romance_relations = pd.read_csv(romance_relations_path, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate descriptions and romance_descriptions, add boolean romance that is 1 if the row came from romance_descriptions\n",
    "descriptions['romance'] = False\n",
    "romance_descriptions['romance'] = True\n",
    "descriptions = pd.concat([descriptions, romance_descriptions])\n",
    "\n",
    "# Concatenate relations and romance_relations, add boolean romance that is 1 if the row came from romance_relations\n",
    "relations['romance'] = False\n",
    "romance_relations['romance'] = True\n",
    "relations = pd.concat([relations, romance_relations])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a list of everyone who is in a relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_characters(df):\n",
    "    # Get a dataframe with the unique characters from the relations dataframe and the wikipedia ID of the movie they appear in\n",
    "    only_subjects = df[['subject', 'movie_id']].drop_duplicates()\n",
    "    only_objects = df[['object', 'movie_id']].drop_duplicates()\n",
    "\n",
    "    # remove rows in only_objects where the object is in the column subject of only_subjects\n",
    "    only_objects = only_objects[~only_objects['object'].isin(\n",
    "        only_subjects['subject'])]\n",
    "\n",
    "    # Concatenate the two dataframes\n",
    "    characters_df = pd.concat([only_subjects, only_objects], ignore_index=True)\n",
    "\n",
    "    # Combine the subject and object columns into one column\n",
    "    characters_df['character'] = characters_df['subject'].combine_first(\n",
    "        characters_df['object'])\n",
    "    characters_df = characters_df.drop(columns=['subject', 'object'])\n",
    "    return characters_df\n",
    "\n",
    "\n",
    "characters = get_characters(relations)\n",
    "\n",
    "print('There are {} relationships in movies, consisting of {} unique characters.'.format(\n",
    "    len(relations), len(characters)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we merge the rows with the same character, and aggregate the titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each unique movie id and character combination, add an identifier for the character\n",
    "descriptions['character_id'] = descriptions.groupby(\n",
    "    ['movie_id', 'character']).ngroup()\n",
    "\n",
    "len_desc = len(descriptions)\n",
    "\n",
    "# Combine the rows with the same character_id into one row, with the title being the concatenation of all the titles. Preserve all other columns. Ignore NaN for titles\n",
    "\n",
    "descriptions = descriptions.groupby('character_id').agg(\n",
    "    {'movie_id': 'first', 'character': 'first', 'title': lambda x: ' '.join(x.dropna()), 'agent_verbs': 'first',\n",
    "     'patient_verbs': 'first', 'attributes': 'first', 'religion': 'first', 'age': 'first'})\n",
    "\n",
    "# print reduction in number of rows for each dataframe\n",
    "print('The number of rows in the descriptions dataframe was reduced from {} to {}.'.format(\n",
    "    len_desc, len(descriptions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we merge the characters with their attributes. `full_char` contains all characters in relationships, alongside their descriptions, which also includes the gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we merge the characters with their descriptions\n",
    "def map_char_attributes(char, descr):\n",
    "    # Join the two dataframes based on the character name and the movie ID\n",
    "    char_descr = char.merge(descr, on=['character', 'movie_id'], how='left')\n",
    "    return char_descr\n",
    "\n",
    "full_char = map_char_attributes(characters, descriptions)\n",
    "\n",
    "# Merge full_char with char_df, map movie_id to Wikipedia ID and map character to Character name. Keep only column Gender from char_df, keep all columns from full_char\n",
    "full_char = full_char.merge(char_df, left_on=['movie_id', 'character'],\n",
    "                            right_on=['Wikipedia ID', 'Character name'], how='left')\n",
    "# From the new dataframe, drop all columns from char_df except gender\n",
    "full_char = full_char.drop(['Character name', 'Wikipedia ID', 'Freebase ID',\n",
    "                            'Release date', 'Ethnicity', 'Date of birth', 'Height',\n",
    "                            'Actor name', 'Actor age at release', 'Freebase character/map ID',\n",
    "                            'Freebase character ID', 'Freebase actor ID'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count percentage of title in full_char that is not NaN\n",
    "print('The percentage of titles that are not NaN is {}%'.format(\n",
    "    round(100 * len(full_char[full_char['title'].notna()]) / len(full_char), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find most common titles in full_char\n",
    "full_char['title'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first embed the titles, to then be able to cluster them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_char' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m     df[\u001b[39m'\u001b[39m\u001b[39mtitle_embeddings\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(embeddings)\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m df\n\u001b[0;32m---> 16\u001b[0m title_indices \u001b[39m=\u001b[39m full_char[\u001b[39m~\u001b[39mfull_char[\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39misnull() \u001b[39m&\u001b[39m (full_char[\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)]\u001b[39m.\u001b[39mindex\n\u001b[1;32m     17\u001b[0m char_with_title \u001b[39m=\u001b[39m full_char\u001b[39m.\u001b[39mloc[title_indices]\n\u001b[1;32m     18\u001b[0m char_embeddings \u001b[39m=\u001b[39m embed_titles(char_with_title)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'full_char' is not defined"
     ]
    }
   ],
   "source": [
    "#embed the titles using spacy and nltk\n",
    "loading = True\n",
    "if loading:\n",
    "    nlp_spacy = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# get the embeddings for the titles\n",
    "\n",
    "def embed_titles(df):\n",
    "    titles = df['title'].values\n",
    "    embeddings = np.concatenate([nlp_spacy(title).vector.reshape(1, -1) for title in titles])\n",
    "    # add the embeddings to the dataframe\n",
    "    df['title_embeddings'] = list(embeddings)\n",
    "    return df\n",
    "\n",
    "\n",
    "title_indices = full_char[~full_char['title'].isnull() & (full_char['title'] != '')].index\n",
    "char_with_title = full_char.loc[title_indices]\n",
    "char_embeddings = embed_titles(char_with_title)\n",
    "\n",
    "\n",
    "print('There are {} characters with titles in movies.'.format(len(char_embeddings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_descriptions(char_description):\n",
    "    embeddings = np.zeros(300)\n",
    "    for word in char_description:\n",
    "        if word in nlp_spacy.vocab:\n",
    "            embeddings = embeddings + nlp_spacy(word).vector.reshape(1, -1)\n",
    "    embeddings = embeddings / len(char_description)\n",
    "    embeddings = embeddings.astype('float32')\n",
    "    return embeddings\n",
    "\n",
    "name_of_dataframe['descriptions_embeddings'] = name_of_dataframe['descriptions'].apply(embed_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.5281432 ,  4.82945   , -3.2066333 ,  0.5114233 ,  4.3305    ,\n",
       "        -1.6477    , -0.8028    ,  4.1424003 ,  1.0175333 ,  0.71446985,\n",
       "        10.505074  , -2.6028    , -1.0779667 , -0.88416666, -0.42611662,\n",
       "         7.273663  ,  2.7979667 ,  6.2700996 ,  0.5579133 ,  0.84056014,\n",
       "         2.4125366 , -0.20286667, -4.6755004 ,  3.2060335 , -4.1760335 ,\n",
       "        -3.8283665 ,  0.58723307, -3.477967  , -1.8548666 , -2.1498365 ,\n",
       "        -0.66377336,  3.0667667 , -1.7323333 , -0.40498003, -2.1022332 ,\n",
       "         5.0712495 ,  1.3844666 , -0.5012369 ,  4.7912    , -2.1499603 ,\n",
       "        -2.8671968 ,  0.20473333,  0.9876767 , -0.44306996, -0.95590013,\n",
       "         3.0837166 ,  5.1591096 , -4.6732097 , -1.31068   ,  0.07133333,\n",
       "        -1.0200567 , -1.05223   ,  3.9922001 , -2.7134    ,  1.5144    ,\n",
       "        -1.7126933 ,  4.0490966 , -2.5862334 , -2.0183334 , -0.54179984,\n",
       "         3.4980032 , -1.52428   , -0.4144067 , -1.5681    , -4.7678347 ,\n",
       "        -0.93435   , -2.897     , -6.4487534 ,  4.6833663 ,  2.9912999 ,\n",
       "        -1.2532073 ,  5.1814    , -7.2001    ,  0.26763332,  0.19905336,\n",
       "         3.46816   , -4.307463  ,  5.933777  , -3.7879333 , -1.6642332 ,\n",
       "        -6.42268   , -4.29505   ,  2.0577934 , -2.2078    ,  2.0142334 ,\n",
       "         1.1466167 , -0.05000003,  1.0744799 ,  8.909633  , -3.8596933 ,\n",
       "         0.19286664, -7.83366   ,  2.8790333 , -6.1240997 , -0.6581333 ,\n",
       "        -3.6411502 , -1.8636565 , -0.6463    , -1.6993734 , -6.2835164 ,\n",
       "        -0.5176967 ,  1.4888668 ,  5.2100663 ,  6.5972333 ,  1.1810566 ,\n",
       "         7.9364    ,  2.65404   ,  0.7682666 , -5.003943  ,  3.5945866 ,\n",
       "        -1.7913399 ,  5.5094333 , -1.7112666 ,  6.3044    , -0.4905033 ,\n",
       "        -2.6543    , -3.4793332 ,  1.6260067 ,  0.9286998 ,  0.63986665,\n",
       "        -0.8447855 , -1.4627047 ,  0.09150004,  8.2406    , -4.62081   ,\n",
       "        -8.439194  , -5.1918006 , -1.14946   ,  4.6439967 , -2.9258664 ,\n",
       "        -2.1321132 ,  0.09418663,  7.6434836 , -8.020864  ,  1.3538666 ,\n",
       "         0.9185567 ,  5.9610233 , -3.350033  ,  3.8870666 , -3.4044666 ,\n",
       "        -3.4386666 ,  1.7955332 ,  4.767713  ,  4.39345   ,  3.6886404 ,\n",
       "         1.9971666 , -4.704227  , -0.29357323,  0.1741999 ,  6.512633  ,\n",
       "         4.1031666 ,  5.6412396 ,  1.6944933 ,  3.1479332 ,  1.2505599 ,\n",
       "         1.7648333 ,  4.7598596 ,  4.4537663 ,  0.98272663, -0.8061333 ,\n",
       "        -4.3792267 , -3.424815  ,  4.4219666 ,  0.78788   , -4.144667  ,\n",
       "        -2.6300783 , -4.4540467 ,  8.94199   ,  2.4638336 , -0.5663166 ,\n",
       "         3.6995533 , -3.1463    ,  4.530733  ,  4.4785666 ,  0.5949099 ,\n",
       "        -4.1384335 ,  0.25429663, -0.69746685, -4.7507    , -0.15806668,\n",
       "        -1.8498999 ,  5.8169665 ,  2.1504734 , -2.4824333 , -3.0062332 ,\n",
       "        -0.08063332, -3.4922    , -0.79738665, -3.0274668 ,  4.262567  ,\n",
       "        -1.0387133 , -1.4731998 ,  0.5335166 , -0.31349993,  3.4496663 ,\n",
       "         1.541973  , -2.8709033 , -1.2086767 ,  4.0429335 , -4.0053334 ,\n",
       "         3.0051267 , -3.4641333 , -4.8343334 , -4.967954  , -0.10830005,\n",
       "        -0.32584992, -5.6343994 ,  0.95853764, -4.0190864 , -2.2275667 ,\n",
       "         4.4760337 ,  4.3835998 , -1.1053233 ,  5.007633  ,  4.9494133 ,\n",
       "        -1.9674934 ,  1.0625534 , -2.90251   , -1.0912334 ,  0.8172233 ,\n",
       "        -3.747553  , -3.4759033 ,  3.255038  , -1.0154333 ,  2.0230334 ,\n",
       "         0.31336656,  0.94176674, -0.87170005,  4.8433    , -1.9316667 ,\n",
       "         2.07921   , -8.428034  ,  3.2637866 ,  5.6193333 , -6.01976   ,\n",
       "         0.65569997,  1.4076    , -2.4542    ,  0.7758333 ,  1.9933599 ,\n",
       "         2.5383599 ,  1.5563134 ,  4.0968666 ,  5.64292   , -2.29554   ,\n",
       "        -2.1015666 , -6.495833  ,  2.0126665 ,  0.31910005,  4.4349465 ,\n",
       "        -0.47571334,  0.49086666, -5.4633    , -6.263967  , -1.2122332 ,\n",
       "        -1.9984668 ,  4.7021337 ,  5.8493004 ,  2.1033032 ,  0.16430332,\n",
       "        -1.0548668 ,  2.8564332 ,  3.5062    ,  8.238233  , -3.56952   ,\n",
       "         0.91502   , -2.3256    ,  0.18088335, -4.2366366 , -0.77225333,\n",
       "         5.9160233 , -1.0425767 , -1.6992999 , -4.73311   ,  2.47297   ,\n",
       "         1.8892001 ,  2.5210333 ,  4.3271637 , -0.04950333, -0.85283345,\n",
       "         0.21798666, -5.586867  , -5.1713066 ,  1.3742484 ,  8.420567  ,\n",
       "        -0.05436659,  4.8594337 , -0.05344009, -1.9513668 ,  2.1135    ,\n",
       "         0.8229335 ,  0.58833337,  2.4223135 ,  0.5229333 , -1.3023933 ,\n",
       "         4.3297    ,  0.76789004,  0.52742   , -2.08999   ,  4.5382733 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All code from here should be revised\n",
    "\n",
    "A stop word is a frequently used term that a search engine has been configured to ignore, both while indexing entries for searching and when retrieving them as the result of a search query. Examples of stop words include \"the,\" \"a,\" \"an,\" and \"in.\"\n",
    "We don't want these terms to take up any unnecessary storage space or processing time in our database. By keeping a record of the terms you believe to be stop words, we may easily eliminate them for this reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take random sample of 10% of the plot summaries\n",
    "plot_df_sample = plot_df.sample(frac=0.1, random_state=1)\n",
    "\n",
    "#copy the plot_df to a new dataframe\n",
    "plot_df_removed = plot_df_sample.copy()\n",
    "\n",
    "#Remove stopwords from the summaries\n",
    "plot_df_removed['Summary'] = plot_df_sample['Summary'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Semantic scoring\n",
    "\n",
    "The semantic scoring is done by using the [SpaCy](https://spacy.io/) library that has pretrained word vectors for our semantic scoring. We aim to assess whether a movie is romantic. We (somewhat arbitrarily) choose to find the similarity between a movie and the word \"love\". Spacy can calculate the cosine similarity between the vector representation of \"love\" and the vector representation of a plot summary. The cosine similarity is defined as:\n",
    "\n",
    "$\\text{cosine similarity}(x_1, x_2) = \\frac{x_1 \\cdot x_2}{||x_1||\\cdot||x_2||}$\n",
    "\n",
    "Where $x_1$ and $x_2$ are two vector representations of either a word or document. Spacy calculates the vector representation of a document as the average of the representations of its words.\n",
    "\n",
    "The idea behind this method is that if a plot summary is semantically close to the word \"love\", it is likely to be a romantic movie. One has to be aware that this method has severe downsides. First, the word \"love\" is a somewhat arbitrary choice. A second downside will become apparent later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The reference word is the word that we want to find the similarity with\n",
    "words = nlp_spacy(\"love\")\n",
    "\n",
    "#Create a column with the similarity score of the summaries to each word in words\n",
    "for word in words:\n",
    "        plot_df_removed[word.text] = np.nan\n",
    "        plot_df_removed[word.text] = plot_df_removed['Summary'].apply(lambda x: nlp_spacy(' '.join(x)).similarity(words))\n",
    "\n",
    "#sort the dataframe by the similarity score\n",
    "plot_df_removed.sort_values(by='love', ascending=False, inplace=True)\n",
    "plot_df_removed.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the similarity score of the summaries to the reference word\n",
    "plot_df_removed['love'].plot(kind='hist', bins=100)\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Similarity score to the word \"love\"')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Love words extraction\n",
    "\n",
    "In the previous part of the analysis, we have given similarity scores to each plot summary, which reflect how semantically close a summary is to the word \"love\". Now, we have to set a threshold to classify words as love-related or not. This presents a second challenge for this method. Picking the optimal threshold is not trivial. A threshold that is too low will result in many unrelated words, whereas a threshold that is too high will result in a low recall of romantic movies. We will show this effect here, by presenting the results of three thresholds. With a threshold of 0.9, only the word 'love' is classified as love-related. At a threshold of 0.6, we see more good words popping up, like 'feel' and 'like'. However, already at this threshold there are some questionable words, like 'think' and 'thought'. At a threshold of 0.3, there are too many words of which many unrelated to love. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textual_analysis import *\n",
    "#The threshold is the minimum similarity score to be considered a love-related word\n",
    "love_thresholds = [0.9, 0.6, 0.3]\n",
    "\n",
    "for love_threshold in love_thresholds:\n",
    "#Create a column with the love-related words in the summaries\n",
    "    plot_df_removed['love_words'] = np.nan\n",
    "    plot_df_removed['love_words'] = plot_df_removed['Summary'].apply(lambda x :extract_love_words(x, words=words, threshold=love_threshold)) \n",
    "    \n",
    "    #sort love-related words by similarity to love\n",
    "    for word in words:\n",
    "        plot_df_removed['love_words'] = plot_df_removed['love_words'].apply(lambda x: sorted(x, key=lambda y: nlp_spacy(y).similarity(words)))\n",
    "\n",
    "    #concatenate all the love-related words in a list\n",
    "    love_words = []\n",
    "    love_words = [love_words + word for word in plot_df_removed[\"love_words\"]]\n",
    "    love_words = np.unique(list(np.concatenate(love_words).flat))\n",
    "    print('For threshold ', love_threshold, ', the following words were classified as love-related: \\n', love_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the threshold goes down to 0.3, we see a large increase in the number of love-related words. This illustrates the importance of setting the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Threshold 0.3 gives', len(love_words), 'love-related words, of which the first 20 are: \\n', love_words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we see the top 5 movies, ranked by their similarity to the word \"love\". When opting for this approach, a further analysis can be done by considering metadata on the movie and characters through the wikipedia ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df_removed.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visualizing the semantic proximity\n",
    "\n",
    "Last, we visualize the semantic proximity of the words in two plots. Note that we use the love-related words obtained from the 0.3 threshold, as this plot is mostly for visualization purposes and the lower threshold gives a larger number of words. Therefore, the data is less sparse. However, one can see that te cloud of points is not so dense. This illustrates that the threshold is too low. When one sets a higher threshold, the words are more semantically related and the cloud of points becomes denser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of semantic vectors for each love-related word\n",
    "love_words_vectors = [nlp_spacy(str(word)).vector for word in love_words]\n",
    "\n",
    "#reduce the dimensionality of the word vectors to 3D\n",
    "pca = PCA(n_components=3)\n",
    "love_words_vectors_3D = pca.fit_transform(love_words_vectors)\n",
    "\n",
    "#plot the 3D word vectors\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(love_words_vectors_3D[:,0], love_words_vectors_3D[:,1], love_words_vectors_3D[:,2])\n",
    "\n",
    "#Label axis as 'dimension 1', 'dimension 2' and 'dimension 3'\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "ax.set_zlabel('Dimension 3')\n",
    "\n",
    "#Set title as 'Clustering of vector representations of love words mapped to a three dimentional space'\n",
    "ax.set_title('Vector representations of love words mapped to a three dimentional space')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final visualization, we use k-means to cluster the love-related words. There may be several uses for this, as there may be several categories of love-related words. To illustrate, one categorie of love-related words could be emotions (happy, elated, enthusiastic), whereas another category could be pronouns (wedding, ring, hug). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster the word vectors with kmeans\n",
    "kmeans = KMeans(n_clusters=5, algorithm = 'elkan', random_state=0).fit(love_words_vectors_3D)\n",
    "\n",
    "#plot the clusters\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(love_words_vectors_3D[:,0], love_words_vectors_3D[:,1], love_words_vectors_3D[:,2], c=kmeans.labels_)\n",
    "\n",
    "#Label axis as 'dimension 1', 'dimension 2' and 'dimension 3'\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "ax.set_zlabel('Dimension 3')\n",
    "\n",
    "#Set title as 'Clustering of vector representations of love words mapped to a three dimentional space'\n",
    "ax.set_title('Clustering of vector representations of love words mapped to a three dimentional space')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Next Steps\n",
    "\n",
    "We have 2 main problems with this approach:\n",
    "- How shall we define the reference vector ?\n",
    "- How is the threshold set ?\n",
    "\n",
    "#### 5.1. Reference vector\n",
    "\n",
    "The reference vector for now is just the semantic vector of the word \"love\". We have to implement a methodology that allows us to find the best reference vector. We have to find a way to define the reference vector in a way that it scores high all the summaries that depicts a relationship. To do this we are thinking of a cross-validation approach and use the movies labeled as romantic by their genres as the target true positives.\n",
    "\n",
    "#### 5.2. Threshold\n",
    "\n",
    "Once we have a correct reference vector that scores the summaries as wanted, we need to find the best way to set the threshold splitting the movies that depicts a relation from the ones that don't. We are thinking of using an F1-scoring method to find it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
