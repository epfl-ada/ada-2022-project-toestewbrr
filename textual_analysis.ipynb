{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied Data Analysis Project\n",
    "**Team**: ToeStewBrr - Alexander Sternfeld, Marguerite Thery, Antoine Bonnet, Hugo Bordereaux\n",
    "\n",
    "**Dataset**: CMU Movie Summary Corpus\n",
    "\n",
    "# Part 3: Textual Analysis\n",
    "\n",
    "In this notebook, we analyze the pre-processed output of our custom CoreNLP pipeline. \n",
    "\n",
    "### Table of contents\n",
    "1. [Loading pre-processed coreNLP data](#section1)\n",
    "2. [Persona clusters](#section2)\n",
    "    - 2.1. [Embedding descriptions](#section2-1)\n",
    "    - 2.2. [Principal Component Analysis (PCA)](#section2-2)\n",
    "    - 2.3. [Clustering personas](#section2-3)\n",
    "    - 2.4. [Visualizing persona clusters](#section2-4)\n",
    "\n",
    "**Prerequisite**: \n",
    "\n",
    "Install [spaCy](https://spacy.io) using the following commands: \n",
    "\n",
    "        pip install spacy\n",
    "        \n",
    "        python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "from extraction import *\n",
    "from coreNLP_analysis import *\n",
    "from load_data import *\n",
    "from textual_analysis import *\n",
    "\n",
    "# NOTE: If you haven't loaded NLTK before, set this to True\n",
    "load_nltk = True\n",
    "\n",
    "if load_nltk: #Load the spaCy model for the semantic analysis\n",
    "    nlp_spacy = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load pre-processed coreNLP data <a class=\"anchor\" id=\"section1\"></a>\n",
    "\n",
    "We first load the pre-processed output from our custom CoreNLP pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions, relations = load_descr_relations()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Persona clusters <a class=\"anchor\" id=\"section2\"></a>\n",
    "\n",
    "### 2.1. Embedding descriptions <a class=\"anchor\" id=\"section2-1\"></a>\n",
    "\n",
    "We embed all descriptive words (actions, attributes, titles) of all characters into a high-dimensional vector space using spaCy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = 'Data/CoreNLP/char_description_embeddings.pickle'\n",
    "\n",
    "# If we have already embedded the descriptions, load them from the pickle file\n",
    "if os.path.exists(pickle_file):\n",
    "    char_description_df = pd.read_pickle(pickle_file)\n",
    "\n",
    "else:\n",
    "    # Load character descriptions\n",
    "    char_description_path = 'Data/CoreNLP/char_descriptions.csv'\n",
    "    char_description_df = pd.read_csv(char_description_path, sep='\\t', index_col=None, low_memory=False)\n",
    "    full_description_path = 'Data/CoreNLP/full_descriptions.csv'\n",
    "    full_description_df = pd.read_csv(full_description_path, sep='\\t', index_col=None, low_memory=False)\n",
    "\n",
    "   # Convert to lists\n",
    "    char_description_df['agent_verbs'] = char_description_df.agent_verbs.apply(lambda x: literal_eval(x) if type(x) == str else x)\n",
    "    char_description_df['patient_verbs'] = char_description_df.patient_verbs.apply(lambda x: literal_eval(x) if type(x) == str else x)\n",
    "    char_description_df['attributes'] = char_description_df.attributes.apply(lambda x: literal_eval(x) if type(x) == str else x)\n",
    "    char_description_df['descriptions'] = char_description_df.descriptions.apply(lambda x: literal_eval(x) if type(x) == str else x)\n",
    "    char_description_df['title'] = char_description_df.title.apply(lambda x: literal_eval(x) if type(x) == str else x)\n",
    "\n",
    "    # Embed descriptions (Get a comfy chair, this takes a while)\n",
    "    char_description_df = construct_descriptions_embeddings(char_description_df, nlp_spacy)\n",
    "    \n",
    "    # Save the embeddings to a pickle file\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump(char_description_df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Weighted average of word vectors <a class=\"anchor\" id=\"section2-2\"></a>\n",
    "\n",
    "We then weigh the semantic vector of each word for each character by their cosine similarity with the average semantic vector of all descriptive words used for all characters in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_description_df = weigh_embeddings(char_description_df, nlp_spacy=nlp_spacy) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Dimensionality reduction <a class=\"anchor\" id=\"section2-3\"></a>\n",
    "\n",
    "#### 2.3.1. Principal Component Analysis (PCA) <a class=\"anchor\" id=\"section2-3-1\"></a>\n",
    "\n",
    "To visualize our clusters, we then map these high-dimensional descriptive vectors to 3-dimensional space so that we can visualize it as a point cloud using Principal Component Analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_description_PCA = descriptions_PCA(char_description_df, n_components=50)\n",
    "\n",
    "plot_clusters_3d(\n",
    "    char_description_PCA, 'Clusters of characters (PCA)', \n",
    "    x_axis='pca_1', y_axis='pca_2', z_axis='pca_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reach unsatisfactory clusters with PCA. Explain why. \n",
    "\n",
    "#### 2.3.2. *t*-distributed Stochastic Neighbor Embedding (t-SNE) <a class=\"anchor\" id=\"section2-3-2\"></a>\n",
    "\n",
    "We reach unsatisfactory clusters with PCA, and so we try out [t-SNE dimensionality reduction](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = 'Data/CoreNLP/char_description_embeddings_tsne.pickle'\n",
    "\n",
    "# If we have already reduced the embeddings with t-SNE, load them\n",
    "if os.path.exists(pickle_file):\n",
    "    char_description_df = pd.read_pickle\n",
    "\n",
    "# Otherwise, compute the t-SNE embeddings and save them\n",
    "else:\n",
    "    char_description_df = descriptions_tSNE(char_description_df, n_components=3)\n",
    "\n",
    "    # Save the embeddings to a pickle file\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump(char_description_df, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Clustering personas <a class=\"anchor\" id=\"section2-4\"></a>\n",
    "\n",
    "The persona point cloud is clustered into several categories using a Gaussian Mixture Model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the outlier on the pca_1 dimension\n",
    "char_description_df = char_description_df[char_description_df['tsne_1'] < 2]\n",
    "\n",
    "# Cluster the descriptions\n",
    "char_description_df = cluster_descriptions(char_description_df, n_components=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Visualizing persona clusters <a class=\"anchor\" id=\"section2-5\"></a>\n",
    "\n",
    "The clustered persona point cloud is shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a plot with more dense clusters, so it is easier to see the clusters\n",
    "char_description_df = cluster_descriptions(char_description_df, n_components=8, min_samples=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters_3d(char_description_df_short, 'Clusters of characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WE REACH A DEAD END HERE.**\n",
    "\n",
    "As you can see above, the clusters are not separated and form one big lump. We would like distinct, distanced clusters of personality types. \n",
    "\n",
    "There are two reasons for this: \n",
    "\n",
    "**Problem 1**: When averaging out the embedded vector of all descriptive words of a given character, we might drown out meaningful words in a list of semantically meaningless words (e.g. 'get' or 'tell'). Averaging assumes that all the words in a character description are equally important, which may not be the case. \n",
    "\n",
    "**Solution**: We first **remove stopwords** and **non-english words** (a few seeped in through our analysis) from the descriptions. We then use a pre-trained word embedding model, such as GloVe or Word2Vec, to **calculate the weights** for each word.\n",
    "\n",
    "We can use Word2Vec to give a weight to each descriptive word by calculating the cosine similarity between the word vectors for each word in a character description. The cosine similarity is a measure of the angle between two vectors, and it ranges from 0 to 1, where 0 indicates that the vectors are orthogonal (i.e., have no similarity) and 1 indicates that the vectors are identical. To calculate the weights for each word, we first calculate the cosine similarity between the word vector for each word and the average word vector for the entire character description. The weights can then be obtained by normalizing the cosine similarities so that they sum to 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Problem 2**: One issue with our approach is that we're using principal component analysis (PCA) to reduce the dimensionality of your character descriptions before clustering. While PCA can be a useful technique for visualizing high-dimensional data, it can also cause your data to lose important information, which can make it difficult to accurately cluster the data. \n",
    "\n",
    "**Solution**: One alternative approach is to use a different dimensionality reduction technique, such as **t-distributed stochastic neighbor embedding** (t-SNE). This technique is specifically designed for visualizing high-dimensional data, and it can often produce more interpretable results than PCA.\n",
    "\n",
    "t-distributed stochastic neighbor embedding (t-SNE) is a non-linear dimensionality reduction technique that is specifically designed for visualizing high-dimensional data. It works by creating a low-dimensional representation of the data that preserves the distances between the points in the high-dimensional space as well as possible.\n",
    "\n",
    "To understand how t-SNE works, it's helpful to consider how a similar technique called principal component analysis (PCA) works. PCA finds a set of axes in the high-dimensional space that capture the maximum amount of variance in the data. The axes are called principal components, and they are ordered from the most important (i.e., the one that captures the most variance) to the least important.\n",
    "\n",
    "In contrast, t-SNE tries to preserve the local structure of the data by minimizing the divergence between the probability distributions of the points in the high-dimensional space and their corresponding points in the low-dimensional space. This is achieved by defining a probability distribution over pairs of points in the high-dimensional space and then minimizing the Kullback-Leibler divergence between the distributions in the high-dimensional and low-dimensional spaces.\n",
    "\n",
    "The result of t-SNE is a low-dimensional representation of the data where points that were close together in the high-dimensional space are also close together in the low-dimensional space. This makes it easier to visually identify clusters in the data, and it can be especially useful for data that doesn't have a clear linear structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All code from here should be revised\n",
    "\n",
    "A stop word is a frequently used term that a search engine has been configured to ignore, both while indexing entries for searching and when retrieving them as the result of a search query. Examples of stop words include \"the,\" \"a,\" \"an,\" and \"in.\"\n",
    "We don't want these terms to take up any unnecessary storage space or processing time in our database. By keeping a record of the terms you believe to be stop words, we may easily eliminate them for this reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take random sample of 10% of the plot summaries\n",
    "plot_df_sample = plot_df.sample(frac=0.1, random_state=1)\n",
    "\n",
    "#copy the plot_df to a new dataframe\n",
    "plot_df_removed = plot_df_sample.copy()\n",
    "\n",
    "#Remove stopwords from the summaries\n",
    "plot_df_removed['Summary'] = plot_df_sample['Summary'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Semantic scoring\n",
    "\n",
    "The semantic scoring is done by using the [SpaCy](https://spacy.io/) library that has pretrained word vectors for our semantic scoring. We aim to assess whether a movie is romantic. We (somewhat arbitrarily) choose to find the similarity between a movie and the word \"love\". Spacy can calculate the cosine similarity between the vector representation of \"love\" and the vector representation of a plot summary. The cosine similarity is defined as:\n",
    "\n",
    "$\\text{cosine similarity}(x_1, x_2) = \\frac{x_1 \\cdot x_2}{||x_1||\\cdot||x_2||}$\n",
    "\n",
    "Where $x_1$ and $x_2$ are two vector representations of either a word or document. Spacy calculates the vector representation of a document as the average of the representations of its words.\n",
    "\n",
    "The idea behind this method is that if a plot summary is semantically close to the word \"love\", it is likely to be a romantic movie. One has to be aware that this method has severe downsides. First, the word \"love\" is a somewhat arbitrary choice. A second downside will become apparent later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The reference word is the word that we want to find the similarity with\n",
    "words = nlp_spacy(\"love\")\n",
    "\n",
    "#Create a column with the similarity score of the summaries to each word in words\n",
    "for word in words:\n",
    "        plot_df_removed[word.text] = np.nan\n",
    "        plot_df_removed[word.text] = plot_df_removed['Summary'].apply(lambda x: nlp_spacy(' '.join(x)).similarity(words))\n",
    "\n",
    "#sort the dataframe by the similarity score\n",
    "plot_df_removed.sort_values(by='love', ascending=False, inplace=True)\n",
    "plot_df_removed.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the similarity score of the summaries to the reference word\n",
    "plot_df_removed['love'].plot(kind='hist', bins=100)\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Similarity score to the word \"love\"')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Love words extraction\n",
    "\n",
    "In the previous part of the analysis, we have given similarity scores to each plot summary, which reflect how semantically close a summary is to the word \"love\". Now, we have to set a threshold to classify words as love-related or not. This presents a second challenge for this method. Picking the optimal threshold is not trivial. A threshold that is too low will result in many unrelated words, whereas a threshold that is too high will result in a low recall of romantic movies. We will show this effect here, by presenting the results of three thresholds. With a threshold of 0.9, only the word 'love' is classified as love-related. At a threshold of 0.6, we see more good words popping up, like 'feel' and 'like'. However, already at this threshold there are some questionable words, like 'think' and 'thought'. At a threshold of 0.3, there are too many words of which many unrelated to love. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textual_analysis import *\n",
    "#The threshold is the minimum similarity score to be considered a love-related word\n",
    "love_thresholds = [0.9, 0.6, 0.3]\n",
    "\n",
    "for love_threshold in love_thresholds:\n",
    "#Create a column with the love-related words in the summaries\n",
    "    plot_df_removed['love_words'] = np.nan\n",
    "    plot_df_removed['love_words'] = plot_df_removed['Summary'].apply(lambda x :extract_love_words(x, words=words, threshold=love_threshold)) \n",
    "    \n",
    "    #sort love-related words by similarity to love\n",
    "    for word in words:\n",
    "        plot_df_removed['love_words'] = plot_df_removed['love_words'].apply(lambda x: sorted(x, key=lambda y: nlp_spacy(y).similarity(words)))\n",
    "\n",
    "    #concatenate all the love-related words in a list\n",
    "    love_words = []\n",
    "    love_words = [love_words + word for word in plot_df_removed[\"love_words\"]]\n",
    "    love_words = np.unique(list(np.concatenate(love_words).flat))\n",
    "    print('For threshold ', love_threshold, ', the following words were classified as love-related: \\n', love_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the threshold goes down to 0.3, we see a large increase in the number of love-related words. This illustrates the importance of setting the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Threshold 0.3 gives', len(love_words), 'love-related words, of which the first 20 are: \\n', love_words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we see the top 5 movies, ranked by their similarity to the word \"love\". When opting for this approach, a further analysis can be done by considering metadata on the movie and characters through the wikipedia ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df_removed.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visualizing the semantic proximity\n",
    "\n",
    "Last, we visualize the semantic proximity of the words in two plots. Note that we use the love-related words obtained from the 0.3 threshold, as this plot is mostly for visualization purposes and the lower threshold gives a larger number of words. Therefore, the data is less sparse. However, one can see that te cloud of points is not so dense. This illustrates that the threshold is too low. When one sets a higher threshold, the words are more semantically related and the cloud of points becomes denser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of semantic vectors for each love-related word\n",
    "love_words_vectors = [nlp_spacy(str(word)).vector for word in love_words]\n",
    "\n",
    "#reduce the dimensionality of the word vectors to 3D\n",
    "pca = PCA(n_components=3)\n",
    "love_words_vectors_3D = pca.fit_transform(love_words_vectors)\n",
    "\n",
    "#plot the 3D word vectors\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(love_words_vectors_3D[:,0], love_words_vectors_3D[:,1], love_words_vectors_3D[:,2])\n",
    "\n",
    "#Label axis as 'dimension 1', 'dimension 2' and 'dimension 3'\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "ax.set_zlabel('Dimension 3')\n",
    "\n",
    "#Set title as 'Clustering of vector representations of love words mapped to a three dimentional space'\n",
    "ax.set_title('Vector representations of love words mapped to a three dimentional space')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final visualization, we use k-means to cluster the love-related words. There may be several uses for this, as there may be several categories of love-related words. To illustrate, one categorie of love-related words could be emotions (happy, elated, enthusiastic), whereas another category could be pronouns (wedding, ring, hug). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster the word vectors with kmeans\n",
    "kmeans = KMeans(n_clusters=5, algorithm = 'elkan', random_state=0).fit(love_words_vectors_3D)\n",
    "\n",
    "#plot the clusters\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(love_words_vectors_3D[:,0], love_words_vectors_3D[:,1], love_words_vectors_3D[:,2], c=kmeans.labels_)\n",
    "\n",
    "#Label axis as 'dimension 1', 'dimension 2' and 'dimension 3'\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "ax.set_zlabel('Dimension 3')\n",
    "\n",
    "#Set title as 'Clustering of vector representations of love words mapped to a three dimentional space'\n",
    "ax.set_title('Clustering of vector representations of love words mapped to a three dimentional space')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Next Steps\n",
    "\n",
    "We have 2 main problems with this approach:\n",
    "- How shall we define the reference vector ?\n",
    "- How is the threshold set ?\n",
    "\n",
    "#### 5.1. Reference vector\n",
    "\n",
    "The reference vector for now is just the semantic vector of the word \"love\". We have to implement a methodology that allows us to find the best reference vector. We have to find a way to define the reference vector in a way that it scores high all the summaries that depicts a relationship. To do this we are thinking of a cross-validation approach and use the movies labeled as romantic by their genres as the target true positives.\n",
    "\n",
    "#### 5.2. Threshold\n",
    "\n",
    "Once we have a correct reference vector that scores the summaries as wanted, we need to find the best way to set the threshold splitting the movies that depicts a relation from the ones that don't. We are thinking of using an F1-scoring method to find it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
